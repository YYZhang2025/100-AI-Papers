---
title: "01: Attention is All You Need"

---



## Preliminary

### Dot Product Similarity（点积相似度）

给定两个向量 $\mathbf{q}, \mathbf{k} \in \mathbb{R}^d$, 它们的点积为:
$$
\text{score} = \mathbf{q} \cdot \mathbf{k} = \sum_{i=1}^{d} q_i k_i
$$ {#eq-dot-product}

- 测量两个向量的**相似度**, 数值越大，说明两个向量越相似
- 在 Self-Attention 中，用来衡量“当前词对其他词的关注程度”

### Softmax 函数

给定向量 $\mathbf{x} = (x_1, x_2, \ldots, x_n)$，Softmax 函数定义为:
$$
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$ {#eq-softmax}


- 将注意力“得分（dot product）”转换为概率分布
- 每个词对其他词的注意力权重 $\alpha_{ij} \in [0, 1]$


## Attention is All You Need

> “The dominant sequence transduction models are based on complex recurrent or convolutional neural networks … We propose a new simple network architecture, the Transformer, based **solely on attention mechanisms**, dispensing with recurrence and convolutions entirely.”


本篇论文提出：序列建模（Sequence Modeling）长期依赖 RNN/CNN，其计算受制于<u>时间步骤的串行性</u>，难以并行化，且<u>长距离依赖捕获效率低</u>。
为了解决这个问题，作者提出了Transformer模型，完全基于<u>注意力机制</u>，去除了循环和卷积操作，从而实现了<u>全局信息交互</u>。利用多头注意力机制（Multi-Head Attention）+ 前馈网络 （Feed Forward Network）编码器-解码器均去除了循环/卷积，仅靠注意力实现全局信息交互。
除此之外，他们还设计了Scaled Dot-Product Attention 和 Multi Head Attention，来**降低数值尺度**的同时并行**学习多子空间表示**。

以下是Transformer的整体架构图：

![The Transformer Family Version 2.0 | Lil'Log](01-attention.assets/transformer.png){#fig-transformer-architecture}

如图 @fig-transformer-architecture 所示，Transformer是由：

- Input Embedding
- Position Embedding
- Multi-Head Attention
- Point-Wise Feed Forward Network
- Layer Normalization

这几个小模块来组成的，每个小模块组成一个Encoder Block，多个Encoder Block堆叠起来形成Encoder，Decoder Block也是类似的。接下来，我们会逐个介绍这些模块。


### Input Embedding 
Input Embedding是将输入的token转换为向量的过程。Transformer的输入是一个序列，序列中的每个词都被转换为一个向量。*这个向量可以是预训练的词向量，也可以是随机初始化的向量*。Transformer使用Word Embedding来将每个token转换为一个固定维度的向量。这在自然语言处理（NLP）中是一个常见的做法。

### Positional Encoding
Transformer 的另一个重要特点是它不使用循环神经网络（RNN）或卷积神经网络（CNN）来处理序列数据。这就导致了一个问题：Transformer无法捕捉到序列中词语的位置信息。为了解决这个问题，Transformer引入了位置编码（Position Encoding）。

> Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.

位置编码是一个向量，它为每个词提供了一个唯一的位置信息。**位置编码的维度与词向量的维度相同**。Transformer使用位置编码来为*每个词提供一个唯一的位置信息*，从而使得模型能够捕捉到词语在序列中的相对位置关系。

在Transformer中，位置编码是通过正弦和余弦函数来实现的：具体来说，对于序列中的第 $pos$ 个位置，位置编码的第 $i$ 维可以表示为：
$$
\begin{align}
\mathrm{PE}(pos,2i)=\sin\!\bigl(pos \times \tfrac{1}{10000^{2i/d_\mathrm{model}}}\bigr) \\ 
\mathrm{PE}(pos,2i{+}1)=\cos\!\bigl(pos \times \tfrac{1}{10000^{2i/d_\mathrm{model}}}\bigr)
\end{align}
$$ {#eq-position-encoding}

其中 $d_{\text{model}}$ 是位置编码的维度（与Word Embedding 的维度一样大）。位置编码的作用是为每个词提供一个唯一的位置信息，使得模型能够捕捉到词语在序列中的相对位置关系。


### Multi-Head Attention
Multi-Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。Attention 其实就是一个**加权求和**的过程，它可以看作是对输入向量的加权平均。其核心公式为：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$ {#eq-attention}

其中 $Q$、$K$、$V$ 分别是查询（Query）、键（Key）和值（Value）矩阵，其中 $\sqrt{d_k}$ 是一个缩放因子，用来防止点积的值过大导致梯度消失。这个公式的含义是：首先计算查询和键的点积，然后通过Softmax函数将其转换为概率分布，最后用这个概率分布对值进行加权求和。

::: {.callout-note}
@eq-attention 是Attention的核心公式，它是Transformer的核心模块。理解这个公式是理解Transformer的关键。后续很多的创新和变体都是在这个公式的基础上进行的。
:::

Multi-Head Attention 是Attention@eq-attention 的一个扩展，它将输入的向量分成多个子空间（head），然后在每个子空间上独立地计算注意力。最后，将所有子空间的输出拼接起来，得到最终的输出。Multi-Head Attention 的公式为：
$$
\begin{align*}
\text{MultiHead}(Q, K, V) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}    
$$ {#eq-multi-head-attention}

其中 $W_i^Q, W_i^K, W_i^V$ 是用于将输入向量投影到子空间的权重矩阵，$W^O$ 是用于将所有子空间的输出拼接起来的权重矩阵。Multi-Head Attention 的作用是通过多个子空间来捕捉不同的语义信息，从而提高模型的表达能力。

> Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively.


尽管增加了Heads会增加计算量，但由于每个Head的维度较小，因此可以并行计算，从而提高了模型的效率。Multi-Head Attention 使得模型能够同时关注输入序列中的不同部分，从而捕捉到更丰富的语义信息。


> Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.

### Causal Attention
Causal Attention（因果注意力）是Transformer中的一个重要概念，它的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。具体来说，Causal Attention会在计算注意力时，将未来的信息屏蔽掉，从而使得模型只能看到当前和过去的信息。这样可以保证模型在训练时不会看到未来的信息，从而保证模型的自回归特性。 Causal Attention通常作用在Decoder 模块中。
![Illustration of Causal Attention](01-attention.assets/causal-attention.png){#fig-causal-attention}
Causal Attention的公式为：
$$
\text{CausalAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
$$ {#eq-causal-attention}   

在这个公式中，$M$ 是一个掩码矩阵（mask matrix），它的作用是将未来的信息屏蔽掉。具体来说，$M$ 是一个上三角矩阵，它的对角线以下的元素为0，对角线以上的元素为负无穷大（-inf）。这样在计算Softmax时，对角线以上的元素会被屏蔽掉，从而保证模型只能看到当前和过去的信息。

### Cross Attention
Cross Attention（交叉注意力）是Transformer中的另一个重要概念，它的作用是将Encoder的输出与Decoder的输入进行交叉注意力计算，从而使得Decoder能够利用Encoder的输出信息。具体来说，Cross Attention会将Encoder的输出作为键（Key）和值（Value），将Decoder的输入作为查询（Query），然后计算注意力。
Cross Attention的公式与Attention @eq-attention 类似，只不过将Encoder的输出作为键和值，Decoder的输入作为查询



### Layer Normalization
Layer Normalization是Transformer中的一个重要模块，它可以帮助模型更快地收敛。Layer Normalization的作用是对每个位置的向量进行归一化处理，从而使得模型在训练时更加稳定。具体来说，Layer Normalization会对**每个位置的向量(feature)**进行均值和方差的归一化处理，使得每个位置的向量都具有相同的分布。这样可以使得模型在训练时更加稳定，从而加快模型的收敛速度。

![Illustration of Layer Normalization](01-attention.assets/layer-norm.png){#fig-layer-normalization}

Layer Normalization的公式为：
$$
\text{LayerNorm}(x) = \frac{x - \mu}{\sigma}
$$ {#eq-layer-normalization}

其中 $\mu$ 是向量 $x$ 的均值，$\sigma$ 是向量 $x$ 的标准差。Layer Normalization的作用是对每个位置的向量进行归一化处理，从而使得模型在训练时更加稳定。
对于一个batch的输入，Layer Normalization会对每个位置的向量进行归一化处理，而不是对整个batch进行归一化处理。这使得Layer Normalization可以更好地适应不同长度的序列，从而提高模型的性能。具体来说, $x \in \mathbb{R}^{B \times H \times S \times d_v}$我们对每个位置的向量也就是 $S$ 维度进行归一化处理，而不是对整个batch进行归一化处理。这样可以使得模型在训练时更加稳定，从而加快模型的收敛速度。


### Residual Connection 

$$
\text{Output} = \mathrm{LayerNorm}\bigl(\mathbf{x} + \mathrm{Sublayer}(\mathbf{x})\bigr)
$$ {#eq-residual-connection}

![Illustration of Residual Connection](01-attention.assets/residuakl-connection.png){#fig-residual-connection}

学习Residual Connection的目的是为了让模型更容易学习到Identity Mapping。具体来说，Residual Connection会将输入的向量与子层的输出相加，从而使得模型可以更容易地学习到Identity Mapping。这样可以使得模型在训练时更加稳定，从而加快模型的收敛速度。在优化的过程中，让Gradient Information更容易地传递到前面的层，从而使得模型更容易学习到Identity Mapping。Residual Connection的公式为：
$$
\frac{\partial \mathcal L}{\partial \mathbf{x}} = \underbrace{\frac{\partial \mathcal L}{\partial \mathbf{y}}}_{\text{straight path}} + \underbrace{\frac{\partial \mathcal L}{\partial \mathbf{y}}\frac{\partial\,\mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}}}_{\text{through the sub-layer}} 
$$ {#eq-residual-connection-gradient}

::: {.callout-note}
在后续的研究中表示，Normalization的位置可以放在Residual Connection的前面或者后面，效果差不多。Transformer论文中是放在后面的（Post-Normalization）@eq-residual-connection 。但在后续的研究中，很多模型（比如BERT）将Normalization放在Residual Connection的前面（Pre-Normalization）。
$$
\text{Output} = \mathrm{Sublayer}(\mathbf{x}) + \mathrm{LayerNorm}(\mathbf{x})
$$ {#eq-pre-normalization}
:::

### Point-Wise Feed Forward Network
Point-Wise Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Point-Wise Feed Forward Network由两个线性变换和一个ReLU激活函数组成。它的作用是对每个位置的向量进行非线性变换，从而使得模型可以更好地捕捉到输入序列中的不同语义信息。
Point-Wise Feed Forward Network的公式为：
$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$ {#eq-point-wise-ffn}

其中 $W_1$ 和 $W_2$ 是线性变换的权重矩阵，$b_1$ 和 $b_2$ 是偏置项。Point-Wise Feed Forward Network的作用是对每个位置的向量进行非线性变换，从而使得模型可以更好地捕捉到输入序列中的不同语义信息。


### Output Linear Projection & Softmax
在Transformer的Decoder中，最后一步是将Decoder的输出通过一个线性变换和Softmax函数转换为词汇表中的概率分布。这个过程可以看作是将Decoder的输出映射到词汇表中的每个词的概率。具体来说，
$$\text{Output} = \text{Softmax}(xW + b)
$$ {#eq-output-softmax}
其中 $W$ 是线性变换的权重矩阵，$b$ 是偏置项。Softmax函数将线性变换的输出转换为概率分布，从而使得模型可以生成下一个词的概率分布。

在这个过程中，Transformer 的作者采用了一种 **weight tying** 的方法，将输入的词嵌入矩阵和输出的线性变换矩阵共享同一个权重矩阵。这种方法可以减少模型的参数量，从而提高模型的效率，并且在实践中效果也很好。具体来说，Transformer 的作者将输入的词嵌入矩阵和输出的线性变换矩阵共享同一个权重矩阵，这样可以减少模型的参数量，从而提高模型的效率。


### Full Process
:::{.column-page}
::: {#fig-full-transformer layout-ncol=2}
![Transformer Encoding Process](./01-attention.assets/transformer-encoding.gif){#fig-transformer-encoding}

![Transformer Decoding Process](./01-attention.assets/transformer-decoding.gif){#fig-transformer-decoding}

Illustation of Transformer Encoding and Decoding Process (Image Source: [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/))
:::
:::

如图 @fig-full-transformer 所示，Transformer的编码过程和解码过程是相似的。编码过程将输入的token转换为向量，然后通过多个Encoder Block进行编码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。解码过程则是将Encoder的输出与Decoder的输入进行交叉注意力计算，然后通过多个Decoder Block进行解码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。


### Training Process
接下来，我们探讨一下Transformer的训练过程。Transformer的训练过程与其他神经网络模型类似，主要包括以下几个步骤：

1. **数据预处理**：将输入的文本数据转换为token，并进行分词和编码。通常使用Word Embedding将每个token转换为一个固定维度的向量。
2. **模型初始化**：初始化Transformer模型的参数，包括Word Embedding、Position Embedding、Multi-Head Attention、Point-Wise Feed Forward Network等模块的参数。
3. **前向传播**：将输入的token通过Word Embedding和Position Embedding转换为向量，然后通过多个Encoder Block进行编码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。
4. **计算损失**：使用交叉熵损失函数（Cross-Entropy Loss）计算模型的输出与真实标签之间的差异。交叉熵损失函数是一个常用的分类损失函数，它可以衡量模型的输出概率分布与真实标签之间的差异。
5. **反向传播**：通过计算损失函数对模型参数的梯度，使用梯度下降算法（如Adam优化器）更新模型的参数。梯度下降算法是一种常用的优化算法，它可以通过计算损失函数对模型参数的梯度来更新模型的参数，从而使得模型的输出更接近真实标签

具体的实现过程，我们将在接下来的PyTorch实现中详细介绍。

## PyTorch 实现
接下来，我们叫利用PyTorch来实现Transformer的模型架构。我们采用Bottom-Up的方法，先实现Word Embedding， 接着是Position Embedding，接着实现我们的重点，即Multi-Head-Attention，再次之后，我们会实现 Point-Wise Feed Forward Network。最后将这几个模块组合起来，实现Transformer的Encoder 和 Decoder。准备好了吗？让我们开始吧！


### Word Embedding
Word Embedding是将词语转换为向量的过程。在PyTorch 中的实现非常简单，我们可以使用`nn.Embedding`类来实现。这个类会将每个token映射到一个*固定维度*的向量空间中。

```python
class WordEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.embedding = nn.Embedding(config.vocab_size, config.d_model)
    
    def _init_weight(self):
        nn.init.trunc_normal_(self.embedding.weight, mean = 0, std=0.02, a=-3, b=3)
    
    def forward(self, x):
        return self.embedding(x)    
```



### Position Embedding 

接下来，我们来实现Position Embedding @eq-position-encoding。 
```python
class PositionEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.seq_len = config.seq_len
        self.d_model = config.d_model

```


有了Word Embedding 和 Position Embedding，我们就可以将输入的token转换为向量了。我们需要接下来需要做的就是，将这两个向量相加，得到最终的输入向量。
```python
class InputEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.word_embedding = WordEmbedding(config)
        self.position_embedding = PositionEmbedding(config)
    
    def forward(self, x):
        word_emb = self.word_embedding(x)
        pos_emb = self.position_embedding(x)
        
        return word_emb + pos_emb
```


### Feed Forward Network 
我们先跳过Multi Head Attention，先实现Feed Forward Network。Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Feed Forward Network由两个线性变换和一个ReLU激活函数组成。

```python
class FeedForwardNetwork(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.linear1 = nn.Linear(config.d_model, config.d_ff)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(config.d_ff, config.d_model)
    def forward(self, x):
        return self.linear2(self.relu(self.linear1(x)))
``` 

### Layer Normalization

还有一个重要的模块是Layer Normalization，它可以帮助模型更快地收敛。

```python
class LayerNormalization(nn.Module):
    def __init__(self, in_features, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(in_features))
        self.bias = nn.Parameter(torch.zeros(in_features))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        normalized_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.weight * normalized_x + self.bias
```






### Multi Head Attention 
Multi Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。

:::{.callout-important}
这部分是Transformer的核心模块，理解它是理解Transformer以及他变型的关键。记得多看几遍，直到你能理解为止。
:::


```python
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()

        assert config.d_model % config.num_heads == 0, "d_model must be divisible by num_heads"
        self.head_dim = config.d_model // config.num_heads

        self.qkv_proj = nn.Linear(config.d_model, config.d_model * 3)
        self.out_proj = nn.Linear(config.d_model, config.d_model)

```


### Encoder Block 
Encoder Block是Transformer的一个重要模块，它由Multi Head Attention和Feed Forward Network组成。它的作用是对输入的向量进行编码，从而捕捉到不同的语义信息。

```python
class EncoderBlock(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.attention = MultiHeadAttention(config)
        self.ffn = FeedForwardNetwork(config)
        self.norm1 = LayerNormalization(config.d_model)
        self.norm2 = LayerNormalization(config.d_model)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        # Multi Head Attention
        attn_output = self.attention(x)
        x = self.norm1(x + self.dropout(attn_output))   
        # Feed Forward Network
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        return x
``` 



### Decoder Block
Decoder Block是Transformer的另一个重要模块，它与Encoder Block类似，但它还需要处理
Masked Multi Head Attention。Masked Multi Head Attention的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。

```python
class DecoderBlock(nn.Module):
    def __init__(self, config):
        super().__init__()      

        self.attention1 = MultiHeadAttention(config)
        self.attention2 = MultiHeadAttention(config, is_causal=True)

```




恭喜你，以及成功的实现了Transformer，这个是当前最重要的AI模型框架。理解了它，你就理解可以理解大部分的AI模型了。现在大火的ChatGPT，DeepSeek等模型都是基于Transformer的变型（在接下来的文章中，我们会阅读到这些模型）。完整的代码可以在[GitHub]()上查看。




## 扩展
自从Transformer被提出以来，已经有了很多的变型和改进。具体的来说，Attention在Transformer中需要 $\mathcal{O}(n^2)$ 的计算复杂度，这在处理长文本时会变得非常慢。因此，很多研究者提出了各种各样的改进方法来降低计算复杂度。以下是一些常见的改进方法：

- **Sparse Attention**: 通过稀疏化注意力矩阵来降低计算复杂度。比如，Reformer模型使用了局部敏感哈希（LSH）来实现稀疏注意力。
- **Linear Attention**: 通过将注意力计算转换为线性时间复杂度






## Q&A

::: {.QA title="问题：为什么要用 $\sqrt{d_k}$ 缩放点积？"}
回答：如果不缩放，当 $d_k$ 很大时，QK 的方差也会变大，使 softmax 落入梯度非常小的区域。除以 $\sqrt{d_k}$ 有助于将激活值保持在适合训练的范围内。
:::

::: {.QA title="问题：多头注意力解决了什么问题？"}
回答：它让模型可以同时关注不同的表示子空间和位置的信息，从而克服单头自注意力容易“平均化”的问题。
:::

::: {.QA title="问题：Transformer 如何实现自回归式解码？"}
回答：解码器通过将未来位置的注意力得分设为 $-\infty$ 来屏蔽它们，确保每个位置只能关注到前面的输出。
:::

::: {.QA title="问题：为什么使用正弦位置编码而不是可学习的位置编码？"}
回答：正弦位置编码允许模型推广到更长的序列，参数量少，并且在 BLEU 分数上与可学习的位置编码效果相当。
:::

::: {.QA title="问题：与 RNN/CNN 相比，路径长度和计算复杂度有何不同？"}
回答：自注意力的路径长度是常数，每层的计算复杂度是 $\mathcal{O}(n²d)$；而 RNN 需要 $\mathcal{O}(n)$ 的串行步骤，CNN 需要堆叠多层才能覆盖长距离依赖。
:::