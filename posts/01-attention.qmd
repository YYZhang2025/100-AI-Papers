---
title: "01: Attention is All You Need"
---



## Preliminary

### Dot Product Similarity（点积相似度）

给定两个向量 $\mathbf{q}, \mathbf{k} \in \mathbb{R}^d$, 它们的点积为:
$$
\text{score} = \mathbf{q} \cdot \mathbf{k} = \sum_{i=1}^{d} q_i k_i
$$ {#eq-dot-product}

- 测量两个向量的**相似度**, 数值越大，说明两个向量越相似
- 在 Self-Attention 中，用来衡量“当前词对其他词的关注程度”

### Softmax 函数

给定向量 $\mathbf{x} = (x_1, x_2, \ldots, x_n)$，Softmax 函数定义为:
$$
\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$ {#eq-softmax}


- 将注意力“得分（dot product）”转换为概率分布
- 每个词对其他词的注意力权重 $\alpha_{ij} \in [0, 1]$


## Attention is All You Need
![The Transformer Family Version 2.0 | Lil'Log](01-attention.assets/transformer.png){#fig-transformer-architecture}

如图 @fig-transformer-architecture 所示，是Transformer的整体架构。从



### 位置编码
Transformer的输入是一个序列，序列中的每个词都被转换为一个向量。为了让模型能够理解词语在序列中的位置，我们需要添加位置编码（Position Encoding）。位置编码可以是固定的，也可以是可学习的。
在Transformer中，位置编码是通过正弦和余弦函数来实现的：具体来说，对于序列中的第 $i$ 个位置，位置编码的第 $j$ 维可以表示为：
$$
\text{PE}(i, j) =
\begin{cases}
\sin\left(\frac{i}{10000^{j/d}}\right) & \text{if } j \text{ is even} \\
\cos\left(\frac{i}{10000^{(j-1)/d}}\right) & \text{if } j \     \text{ is odd}
\end{cases}
$$ {#eq-position-encoding}
其中 $d$ 是位置编码的维度。位置编码的作用是为每个词提供一个唯一的位置信息，使得模型能够捕捉到词语在序列中的相对位置关系。





## PyTorch 实现
接下来，我们叫利用PyTorch来实现Transformer的模型架构。我们采用Bottom-Up的方法，先实现Word Embedding， 接着是Position Embedding，接着实现我们的重点，即Multi-Head-Attention，再次之后，我们会实现 Point-Wise Feed Forward Network。最后将这几个模块组合起来，实现Transformer的Encoder 和 Decoder。准备好了吗？让我们开始吧！


### Word Embedding
Word Embedding是将词语转换为向量的过程。在PyTorch 中的实现非常简单，我们可以使用`nn.Embedding`类来实现。这个类会将每个token映射到一个*固定维度*的向量空间中。

```python
class WordEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.embedding = nn.Embedding(config.vocab_size, config.d_model)
    
    def _init_weight(self):
        nn.init.trunc_normal_(self.embedding.weight, mean = 0, std=0.02, a=-3, b=3)
    
    def forward(self, x):
        return self.embedding(x)    
```



### Position Embedding 

接下来，我们来实现Position Embedding @eq-position-encoding。 
```python
class PositionEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.seq_len = config.seq_len
        self.d_model = config.d_model

```


有了Word Embedding 和 Position Embedding，我们就可以将输入的token转换为向量了。我们需要接下来需要做的就是，将这两个向量相加，得到最终的输入向量。
```python
class InputEmbedding(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.word_embedding = WordEmbedding(config)
        self.position_embedding = PositionEmbedding(config)
    
    def forward(self, x):
        word_emb = self.word_embedding(x)
        pos_emb = self.position_embedding(x)
        
        return word_emb + pos_emb
```


### Feed Forward Network 
我们先跳过Multi Head Attention，先实现Feed Forward Network。Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Feed Forward Network由两个线性变换和一个ReLU激活函数组成。

```python
class FeedForwardNetwork(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.linear1 = nn.Linear(config.d_model, config.d_ff)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(config.d_ff, config.d_model)
    def forward(self, x):
        return self.linear2(self.relu(self.linear1(x)))
``` 

### Layer Normalization

还有一个重要的模块是Layer Normalization，它可以帮助模型更快地收敛。

```python
class LayerNormalization(nn.Module):
    def __init__(self, in_features, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(in_features))
        self.bias = nn.Parameter(torch.zeros(in_features))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        normalized_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.weight * normalized_x + self.bias
```





### Multi Head Attention 
Multi Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。

:::{.callout-important}
这部分是Transformer的核心模块，理解它是理解Transformer以及他变型的关键。记得多看几遍，直到你能理解为止。
:::


```python
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()

        assert config.d_model % config.num_heads == 0, "d_model must be divisible by num_heads"
        self.head_dim = config.d_model // config.num_heads

        self.qkv_proj = nn.Linear(config.d_model, config.d_model * 3)
        self.out_proj = nn.Linear(config.d_model, config.d_model)

```



### Encoder Block 
Encoder Block是Transformer的一个重要模块，它由Multi Head Attention和Feed Forward Network组成。它的作用是对输入的向量进行编码，从而捕捉到不同的语义信息。

```python
class EncoderBlock(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.attention = MultiHeadAttention(config)
        self.ffn = FeedForwardNetwork(config)
        self.norm1 = LayerNormalization(config.d_model)
        self.norm2 = LayerNormalization(config.d_model)
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        # Multi Head Attention
        attn_output = self.attention(x)
        x = self.norm1(x + self.dropout(attn_output))   
        # Feed Forward Network
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))
        return x
``` 



### Decoder Block
Decoder Block是Transformer的另一个重要模块，它与Encoder Block类似，但它还需要处理
Masked Multi Head Attention。Masked Multi Head Attention的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。

```python
class DecoderBlock(nn.Module):
    def __init__(self, config):
        super().__init__()      

        self.attention1 = MultiHeadAttention(config)
        self.attention2 = MultiHeadAttention(config, is_causal=True)

```




恭喜你，以及成功的实现了Transformer，这个是当前最重要的AI模型框架。理解了它，你就理解可以理解大部分的AI模型了。现在大火的ChatGPT，DeepSeek等模型都是基于Transformer的变型（在接下来的文章中，我们会阅读到这些模型）。完整的代码可以在[GitHub]()上查看。




## 扩展
自从Transformer被提出以来，已经有了很多的变型和改进。具体的来说，Attention在Transformer中需要 $\mathcal{O}(n^2)$ 的计算复杂度，这在处理长文本时会变得非常慢。因此，很多研究者提出了各种各样的改进方法来降低计算复杂度。以下是一些常见的改进方法：

- **Sparse Attention**: 通过稀疏化注意力矩阵来降低计算复杂度。比如，Reformer模型使用了局部敏感哈希（LSH）来实现稀疏注意力。
- **Linear Attention**: 通过将注意力计算转换为线性时间复杂度





