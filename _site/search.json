[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nPaper Link\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need\nTransformer-based model for sequence modeling\nğŸ”—\n\nNLP / Transformer\n\n\n02\nBERT\nBidirectional encoder for pre-training NLP tasks\nğŸ”—\n\nNLP / Pretraining\n\n\n03\nGPT-3\nGenerative transformer for autoregressive tasks\nğŸ”—\n\nNLP / Generation\n\n\n04\nCLIP\nContrastive Language-Image Pretraining\nğŸ”—\n\nVision / Multimodal\n\n\n05\nViT\nVision Transformer for image classification\nğŸ”—\n\nVision / Transformer",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/01-attention.html",
    "href": "posts/01-attention.html",
    "title": "01: Attention is all you need",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis",
    "crumbs": [
      "100 Papers",
      "01: Attention is all you need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#preliminary",
    "href": "posts/01-attention.html#preliminary",
    "title": "01: Attention is all you need",
    "section": "Preliminary",
    "text": "Preliminary\n\nDot Product Similarityï¼ˆç‚¹ç§¯ç›¸ä¼¼åº¦ï¼‰\nç»™å®šä¸¤ä¸ªå‘é‡ \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\)\nå®ƒä»¬çš„ç‚¹ç§¯ä¸º:\n\\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\næµ‹é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦, æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼\nåœ¨ Self-Attention ä¸­ï¼Œç”¨æ¥è¡¡é‡â€œå½“å‰è¯å¯¹å…¶ä»–è¯çš„å…³æ³¨ç¨‹åº¦â€\n\n\n\nSoftmax å‡½æ•°\nç»™å®šå‘é‡ \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)ï¼ŒSoftmax å‡½æ•°å®šä¹‰ä¸º: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\nå°†æ³¨æ„åŠ›â€œå¾—åˆ†ï¼ˆdot productï¼‰â€è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ\næ¯ä¸ªè¯å¯¹å…¶ä»–è¯çš„æ³¨æ„åŠ›æƒé‡ \\(\\alpha_{ij} \\in [0, 1]\\)\n\n\n\n\nThe Transformer Family Version 2.0 | Lilâ€™Log\n\n\nThis is the figure see, figure EquationÂ 2",
    "crumbs": [
      "100 Papers",
      "01: Attention is all you need"
    ]
  },
  {
    "objectID": "posts/02-bert.html",
    "href": "posts/02-bert.html",
    "title": "02: BERT",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see EquationÂ 1\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\n\nprint(\"Hello,m World!\")\n\\[\n\\mathcal{H} = \\mathcal{W} \\cdot \\mathcal{O}\n\\tag{1}\\]\nThis is the figure see, figure ?@fig-polar $$",
    "crumbs": [
      "100 Papers",
      "02: BERT"
    ]
  }
]