<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuyang Zhang">

<title>01: Attention is All You Need – 100 Papers with Codes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../00-how-to-read-paper.html" rel="prev">
<link href=".././images/icon.avif" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7ddd24ee98d70ca00bb532ce6196e657.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-62960f262fec3dcc18a72ed9802b3bf0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-7ddd24ee98d70ca00bb532ce6196e657.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<style>
div.callout-QA.callout {
  border-left-color: #e7f3ff;
}
div.callout-QA.callout-style-default > .callout-header {
  background-color: rgba(231, 243, 255, 0.13);
}
div.callout-QA .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-QA.callout-style-default .callout-icon::before, div.callout-QA.callout-titled .callout-icon::before {
  content: '🗨️';
  background-image: none;
}
div.callout-thumbs-up.callout {
  border-left-color: #008000;
}
div.callout-thumbs-up.callout-style-default > .callout-header {
  background-color: rgba(0, 128, 0, 0.13);
}
div.callout-thumbs-up .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-thumbs-up.callout-style-default .callout-icon::before, div.callout-thumbs-up.callout-titled .callout-icon::before {
  font-family: 'Font Awesome 6 Free', FontAwesome;
  font-style: normal;
  content: '\f164' !important;
  background-image: none;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../style/style.css">
<meta property="og:title" content="01: Attention is All You Need – 100 Papers with Codes">
<meta property="og:description" content="">
<meta property="og:image" content="01-attention.assets/transformer.png">
<meta property="og:site_name" content="100 Papers with Codes">
</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../posts/01-attention.html">100 Papers</a></li><li class="breadcrumb-item"><a href="../posts/01-attention.html">01: Attention is All You Need</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src=".././images/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main tools-wide">
    <a href="https://yyzhang2000.github.io/Blog/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-globe"></i></a>
    <a href="https://github.com/YYZhang2025" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.linkedin.com/in/zhang-yuyang/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
    <a href="mailto:zhangyuyang1211@gmail.com" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-envelope"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-how-to-read-paper.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">00 Preparation for following</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">100 Papers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../posts/01-attention.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">01: Attention is All You Need</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary"><span class="header-section-number">1</span> Preliminary</a>
  <ul>
  <li><a href="#dot-product-similarity点积相似度" id="toc-dot-product-similarity点积相似度" class="nav-link" data-scroll-target="#dot-product-similarity点积相似度"><span class="header-section-number">1.1</span> Dot Product Similarity（点积相似度）</a></li>
  <li><a href="#softmax-函数" id="toc-softmax-函数" class="nav-link" data-scroll-target="#softmax-函数"><span class="header-section-number">1.2</span> Softmax 函数</a></li>
  <li><a href="#matrix-multiplication" id="toc-matrix-multiplication" class="nav-link" data-scroll-target="#matrix-multiplication"><span class="header-section-number">1.3</span> Matrix Multiplication</a></li>
  </ul></li>
  <li><a href="#attention-is-all-you-need" id="toc-attention-is-all-you-need" class="nav-link" data-scroll-target="#attention-is-all-you-need"><span class="header-section-number">2</span> Attention is All You Need</a>
  <ul>
  <li><a href="#input-embedding" id="toc-input-embedding" class="nav-link" data-scroll-target="#input-embedding"><span class="header-section-number">2.1</span> Input Embedding</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding"><span class="header-section-number">2.2</span> Positional Encoding</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention"><span class="header-section-number">2.3</span> Multi-Head Attention</a></li>
  <li><a href="#time-complexity-of-multi-head-attention" id="toc-time-complexity-of-multi-head-attention" class="nav-link" data-scroll-target="#time-complexity-of-multi-head-attention"><span class="header-section-number">2.4</span> Time Complexity of Multi-Head Attention</a></li>
  <li><a href="#causal-attention" id="toc-causal-attention" class="nav-link" data-scroll-target="#causal-attention"><span class="header-section-number">2.5</span> Causal Attention</a></li>
  <li><a href="#cross-attention" id="toc-cross-attention" class="nav-link" data-scroll-target="#cross-attention"><span class="header-section-number">2.6</span> Cross Attention</a></li>
  <li><a href="#layer-normalization" id="toc-layer-normalization" class="nav-link" data-scroll-target="#layer-normalization"><span class="header-section-number">2.7</span> Layer Normalization</a></li>
  <li><a href="#residual-connection" id="toc-residual-connection" class="nav-link" data-scroll-target="#residual-connection"><span class="header-section-number">2.8</span> Residual Connection</a></li>
  <li><a href="#point-wise-feed-forward-network" id="toc-point-wise-feed-forward-network" class="nav-link" data-scroll-target="#point-wise-feed-forward-network"><span class="header-section-number">2.9</span> Point-Wise Feed Forward Network</a></li>
  <li><a href="#output-linear-projection-softmax" id="toc-output-linear-projection-softmax" class="nav-link" data-scroll-target="#output-linear-projection-softmax"><span class="header-section-number">2.10</span> Output Linear Projection &amp; Softmax</a></li>
  <li><a href="#full-process" id="toc-full-process" class="nav-link" data-scroll-target="#full-process"><span class="header-section-number">2.11</span> Full Process</a></li>
  <li><a href="#training-process" id="toc-training-process" class="nav-link" data-scroll-target="#training-process"><span class="header-section-number">2.12</span> Training Process</a></li>
  </ul></li>
  <li><a href="#pytorch-实现" id="toc-pytorch-实现" class="nav-link" data-scroll-target="#pytorch-实现"><span class="header-section-number">3</span> PyTorch 实现</a>
  <ul>
  <li><a href="#word-embedding" id="toc-word-embedding" class="nav-link" data-scroll-target="#word-embedding"><span class="header-section-number">3.1</span> Word Embedding</a></li>
  <li><a href="#position-embedding" id="toc-position-embedding" class="nav-link" data-scroll-target="#position-embedding"><span class="header-section-number">3.2</span> Position Embedding</a></li>
  <li><a href="#feed-forward-network" id="toc-feed-forward-network" class="nav-link" data-scroll-target="#feed-forward-network"><span class="header-section-number">3.3</span> Feed Forward Network</a></li>
  <li><a href="#layer-normalization-1" id="toc-layer-normalization-1" class="nav-link" data-scroll-target="#layer-normalization-1"><span class="header-section-number">3.4</span> Layer Normalization</a></li>
  <li><a href="#multi-head-attention-1" id="toc-multi-head-attention-1" class="nav-link" data-scroll-target="#multi-head-attention-1"><span class="header-section-number">3.5</span> Multi Head Attention</a></li>
  <li><a href="#encoder-block" id="toc-encoder-block" class="nav-link" data-scroll-target="#encoder-block"><span class="header-section-number">3.6</span> Encoder Block</a></li>
  <li><a href="#decoder-block" id="toc-decoder-block" class="nav-link" data-scroll-target="#decoder-block"><span class="header-section-number">3.7</span> Decoder Block</a></li>
  </ul></li>
  <li><a href="#扩展" id="toc-扩展" class="nav-link" data-scroll-target="#扩展"><span class="header-section-number">4</span> 扩展</a></li>
  <li><a href="#qa" id="toc-qa" class="nav-link" data-scroll-target="#qa"><span class="header-section-number">5</span> Q&amp;A</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../posts/01-attention.html">100 Papers</a></li><li class="breadcrumb-item"><a href="../posts/01-attention.html">01: Attention is All You Need</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">01: Attention is All You Need</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yuyang Zhang </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="preliminary" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="preliminary"><span class="header-section-number">1</span> Preliminary</h2>
<p>在理解Transformer之前，我们需要先了解一些基本的概念。有基础的同学可以跳过这一节，直接看下一节的Transformer模型架构。</p>
<section id="dot-product-similarity点积相似度" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="dot-product-similarity点积相似度"><span class="header-section-number">1.1</span> Dot Product Similarity（点积相似度）</h3>
<p>给定两个向量 <span class="math inline">\(\mathbf{q}, \mathbf{k} \in \mathbb{R}^d\)</span>, 它们的点积为: <span id="eq-dot-product"><span class="math display">\[
\text{score}(\mathbf{q}, \mathbf{k}) = \mathbf{q} \cdot \mathbf{k} = \sum_{i=1}^{d} q_i k_i
\tag{1}\]</span></span></p>
<p>这个score用于测量两个向量的<em>相似度</em>, 数值越大，说明两个向量越相似</p>
</section>
<section id="softmax-函数" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="softmax-函数"><span class="header-section-number">1.2</span> Softmax 函数</h3>
<p>给定向量 <span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots, x_n)\)</span>，softmax 函数定义为: <span id="eq-softmax"><span class="math display">\[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
\tag{2}\]</span></span></p>
<p>softmax 函数将向量 <span class="math inline">\(\mathbf{x}\)</span> 转换为一个<em>概率分布</em>，所有元素的和为1。它常用于将模型的输出转换为概率分布。</p>
<p>计算softmax 的时间复杂度为 <span class="math inline">\(\mathcal{O}(n)\)</span>，其中 <span class="math inline">\(n\)</span> 是向量 <span class="math inline">\(\mathbf{x}\)</span> 的维度。</p>
</section>
<section id="matrix-multiplication" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">1.3</span> Matrix Multiplication</h3>
<p>给定矩阵 <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> 和 <span class="math inline">\(\mathbf{B} \in \mathbb{R}^{n \times p}\)</span>，它们的矩阵乘法定义为: <span id="eq-matrix-multiplication"><span class="math display">\[
\mathbf{C} = \mathbf{A} \cdot \mathbf{B} \in \mathbb{R}^{m \times p}
\tag{3}\]</span></span></p>
<p>矩阵乘法是线性代数中的基本运算，用于将两个矩阵相乘，得到一个新的矩阵。它在神经网络中被广泛使用，尤其是在计算注意力分数时。其中，Matrix Multiplication的时间复杂度为 <span class="math inline">\(\mathcal{O}(mnp)\)</span>，其中 <span class="math inline">\(m\)</span> 是 <span class="math inline">\(\mathbf{A}\)</span> 的行数，<span class="math inline">\(n\)</span> 是 <span class="math inline">\(\mathbf{A}\)</span> 的列数，也是 <span class="math inline">\(\mathbf{B}\)</span> 的行数，<span class="math inline">\(p\)</span> 是 <span class="math inline">\(\mathbf{B}\)</span> 的列数。</p>
<p>Wikipedia 上有一个关于 <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication">Matrix Multiplication</a> 的详细介绍。</p>
</section>
</section>
<section id="attention-is-all-you-need" class="level2 page-columns page-full" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="attention-is-all-you-need"><span class="header-section-number">2</span> Attention is All You Need</h2>
<blockquote class="blockquote">
<p>“The dominant sequence transduction models are based on complex recurrent or convolutional neural networks … We propose a new simple network architecture, the Transformer, based <strong>solely on attention mechanisms</strong>, dispensing with recurrence and convolutions entirely.”</p>
</blockquote>
<p>本篇论文提出：序列建模（Sequence Modeling）长期依赖 RNN/CNN，其计算受制于<u>时间步骤的串行性</u>，难以并行化，且<u>长距离依赖捕获效率低</u>。 为了解决这个问题，作者提出了Transformer模型，完全基于<u>注意力机制</u>，去除了循环和卷积操作，从而实现了<u>全局信息交互</u>。利用多头注意力机制（Multi-Head Attention）+ 前馈网络 （Feed Forward Network）编码器-解码器均去除了循环/卷积，仅靠注意力实现全局信息交互。 除此之外，他们还设计了Scaled Dot-Product Attention 和 Multi Head Attention，来<strong>降低数值尺度</strong>的同时并行<strong>学习多子空间表示</strong>。</p>
<p>以下是Transformer的整体架构图：</p>
<div id="fig-transformer-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/transformer.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The Transformer Family Version 2.0 | Lil’Log
</figcaption>
</figure>
</div>
<p>如图 <a href="#fig-transformer-architecture" class="quarto-xref">Figure&nbsp;1</a> 所示，Transformer是由：</p>
<ul>
<li>Input Embedding</li>
<li>Position Embedding</li>
<li>Multi-Head Attention</li>
<li>Point-Wise Feed Forward Network</li>
<li>Layer Normalization</li>
</ul>
<p>这几个小模块来组成的，每个小模块组成一个Encoder Block，多个Encoder Block堆叠起来形成Encoder，Decoder Block也是类似的。接下来，我们会逐个介绍这些模块。</p>
<section id="input-embedding" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="input-embedding"><span class="header-section-number">2.1</span> Input Embedding</h3>
<p>Input Embedding是将输入的token转换为向量的过程。Transformer的输入是一个序列，序列中的每个词都被转换为一个向量。<em>这个向量可以是预训练的词向量，也可以是随机初始化的向量</em>。Transformer使用Word Embedding来将每个token转换为一个固定维度的向量。这在自然语言处理（NLP）中是一个常见的做法。</p>
</section>
<section id="positional-encoding" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="positional-encoding"><span class="header-section-number">2.2</span> Positional Encoding</h3>
<p>Transformer 的另一个重要特点是它不使用循环神经网络（RNN）或卷积神经网络（CNN）来处理序列数据。这就导致了一个问题：Transformer无法捕捉到序列中词语的位置信息。为了解决这个问题，Transformer引入了位置编码（Position Encoding）。</p>
<blockquote class="blockquote">
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.</p>
</blockquote>
<p>位置编码是一个向量，它为每个词提供了一个唯一的位置信息。<strong>位置编码的维度与词向量的维度相同</strong>。Transformer使用位置编码来为<em>每个词提供一个唯一的位置信息</em>，从而使得模型能够捕捉到词语在序列中的相对位置关系。</p>
<p>在Transformer中，位置编码是通过正弦和余弦函数来实现的：具体来说，对于序列中的第 <span class="math inline">\(pos\)</span> 个位置，位置编码的第 <span class="math inline">\(i\)</span> 维可以表示为： <span id="eq-position-encoding"><span class="math display">\[
\begin{align}
\mathrm{PE}(pos,2i)=\sin\!\bigl(pos \times \tfrac{1}{10000^{2i/d_\mathrm{model}}}\bigr) \\
\mathrm{PE}(pos,2i{+}1)=\cos\!\bigl(pos \times \tfrac{1}{10000^{2i/d_\mathrm{model}}}\bigr)
\end{align}
\tag{4}\]</span></span></p>
<p>其中 <span class="math inline">\(d_{\text{model}}\)</span> 是位置编码的维度（与Word Embedding 的维度一样大）。位置编码的作用是为每个词提供一个唯一的位置信息，使得模型能够捕捉到词语在序列中的相对位置关系。</p>
</section>
<section id="multi-head-attention" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">2.3</span> Multi-Head Attention</h3>
<p>Multi-Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。Attention 其实就是一个<strong>加权求和</strong>的过程，它可以看作是对输入向量的加权平均。其核心公式为： <span id="eq-attention"><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\tag{5}\]</span></span></p>
<p>其中 <span class="math inline">\(Q\)</span>、<span class="math inline">\(K\)</span>、<span class="math inline">\(V\)</span> 分别是查询（Query）、键（Key）和值（Value）矩阵，其中 <span class="math inline">\(\sqrt{d_k}\)</span> 是一个缩放因子，用来防止点积的值过大导致梯度消失。这个公式的含义是：首先计算查询和键的点积，然后通过Softmax函数将其转换为概率分布，最后用这个概率分布对值进行加权求和。</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="#eq-attention" class="quarto-xref">Equation&nbsp;5</a> 是Attention的核心公式，它是Transformer的核心模块。理解这个公式是理解Transformer的关键。后续很多的创新和变体都是在这个公式的基础上进行的。</p>
</div>
</div>
<p>Multi-Head Attention 是Attention@eq-attention 的一个扩展，它将输入的向量分成多个子空间（head），然后在每个子空间上独立地计算注意力。最后，将所有子空间的输出拼接起来，得到最终的输出。Multi-Head Attention 的公式为： <span id="eq-multi-head-attention"><span class="math display">\[
\begin{align*}
\text{MultiHead}(Q, K, V) &amp;= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{head}_i &amp;= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}    
\tag{6}\]</span></span></p>
<p>其中 <span class="math inline">\(W_i^Q, W_i^K, W_i^V\)</span> 是用于将输入向量投影到子空间的权重矩阵，<span class="math inline">\(W^O\)</span> 是用于将所有子空间的输出拼接起来的权重矩阵。Multi-Head Attention 的作用是通过多个子空间来捕捉不同的语义信息，从而提高模型的表达能力。</p>
<blockquote class="blockquote">
<p>Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively.</p>
</blockquote>
<p>尽管增加了Heads会增加计算量，但由于每个Head的维度较小，因此可以并行计算，从而提高了模型的效率。Multi-Head Attention 使得模型能够同时关注输入序列中的不同部分，从而捕捉到更丰富的语义信息。</p>
<blockquote class="blockquote">
<p>Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p>
</blockquote>
</section>
<section id="time-complexity-of-multi-head-attention" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="time-complexity-of-multi-head-attention"><span class="header-section-number">2.4</span> Time Complexity of Multi-Head Attention</h3>
<p>我们假设input的长度为 <span class="math inline">\(n\)</span>，每个head的维度为 <span class="math inline">\(d_k\)</span>，则Multi-Head Attention的时间复杂度为 <span class="math inline">\(\mathcal{O}(n^2 d_k)\)</span> <a href="#eq-matrix-multiplication" class="quarto-xref">Equation&nbsp;3</a>。</p>
<p>接下来是Softmax的计算，Softmax的时间复杂度为 <span class="math inline">\(\mathcal{O}(n)\)</span>。对于每个score matrix <span class="math inline">\(QK^T \in \mathbf{R}^{n \times n}\)</span>，的每一行，我们需要计算 softmax，这需要 <span class="math inline">\(n\)</span> 次计算。因此，Softmax的时间复杂度为 <span class="math inline">\(\mathcal{O}(n^2)\)</span>。</p>
<p>之后是对于value的加权，计算复杂度也是 <span class="math inline">\(\mathcal{O}(n^2d)\)</span>. 所以总的时间复杂度为：<span class="math inline">\(\mathcal{O}(n^2)\)</span>, 随着 <span class="math inline">\(n\)</span> 的增加，计算复杂度会Polynomial 增长。具体的计算复杂度如下表所示： <span id="eq-multi-head-attention-complexity"><span class="math display">\[
\begin{array}{|l|l|}
\hline
\textbf{Step} &amp; \textbf{Time Complexity} \\
\hline
QK^\top &amp; \mathcal{O}(n^2 d) \\
\text{softmax}(QK^\top) &amp; \mathcal{O}(n^2) \\
\text{attention} \times V &amp; \mathcal{O}(n^2 d) \\
\hline
\textbf{Total} &amp; \mathcal{O}(n^2 d) \\
\hline
\end{array}
\tag{7}\]</span></span></p>
</section>
<section id="causal-attention" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="causal-attention"><span class="header-section-number">2.5</span> Causal Attention</h3>
<p>Causal Attention（因果注意力）是Transformer中的一个重要概念，它的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。具体来说，Causal Attention会在计算注意力时，将未来的信息屏蔽掉，从而使得模型只能看到当前和过去的信息。这样可以保证模型在训练时不会看到未来的信息，从而保证模型的自回归特性。 Causal Attention通常作用在Decoder 模块中。 <img src="01-attention.assets/causal-attention.png" id="fig-causal-attention" class="img-fluid" alt="Illustration of Causal Attention"> Causal Attention的公式为： <span id="eq-causal-attention"><span class="math display">\[
\text{CausalAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
\tag{8}\]</span></span></p>
<p>在这个公式中，<span class="math inline">\(M\)</span> 是一个掩码矩阵（mask matrix），它的作用是将未来的信息屏蔽掉。具体来说，<span class="math inline">\(M\)</span> 是一个上三角矩阵，它的对角线以下的元素为0，对角线以上的元素为负无穷大（-inf）。这样在计算Softmax时，对角线以上的元素会被屏蔽掉，从而保证模型只能看到当前和过去的信息。</p>
</section>
<section id="cross-attention" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="cross-attention"><span class="header-section-number">2.6</span> Cross Attention</h3>
<p>Cross Attention（交叉注意力）是Transformer中的另一个重要概念，它的作用是将Encoder的输出与Decoder的输入进行交叉注意力计算，从而使得Decoder能够利用Encoder的输出信息。具体来说，Cross Attention会将Encoder的输出作为键（Key）和值（Value），将Decoder的输入作为查询（Query），然后计算注意力。 Cross Attention的公式与Attention <a href="#eq-attention" class="quarto-xref">Equation&nbsp;5</a> 类似，只不过将Encoder的输出作为键和值，Decoder的输入作为查询</p>
</section>
<section id="layer-normalization" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="layer-normalization"><span class="header-section-number">2.7</span> Layer Normalization</h3>
<p>Layer Normalization是Transformer中的一个重要模块，它可以帮助模型更快地收敛。Layer Normalization的作用是对每个位置的向量进行归一化处理，从而使得模型在训练时更加稳定。具体来说，Layer Normalization会对<strong>每个位置的向量(feature)</strong>进行均值和方差的归一化处理，使得每个位置的向量都具有相同的分布。这样可以使得模型在训练时更加稳定，从而加快模型的收敛速度。</p>
<div id="fig-layer-normalization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-layer-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/layer-norm.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-layer-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustration of Layer Normalization
</figcaption>
</figure>
</div>
<p>Layer Normalization的公式为： <span id="eq-layer-normalization"><span class="math display">\[
\text{LayerNorm}(x) = \frac{x - \mu}{\sigma}
\tag{9}\]</span></span></p>
<p>其中 <span class="math inline">\(\mu\)</span> 是向量 <span class="math inline">\(x\)</span> 的均值，<span class="math inline">\(\sigma\)</span> 是向量 <span class="math inline">\(x\)</span> 的标准差。Layer Normalization的作用是对每个位置的向量进行归一化处理，从而使得模型在训练时更加稳定。 对于一个batch的输入，Layer Normalization会对每个位置的向量进行归一化处理，而不是对整个batch进行归一化处理。这使得Layer Normalization可以更好地适应不同长度的序列，从而提高模型的性能。具体来说, <span class="math inline">\(x \in \mathbb{R}^{B \times H \times S \times d_v}\)</span>我们对每个位置的向量也就是 <span class="math inline">\(S\)</span> 维度进行归一化处理，而不是对整个batch进行归一化处理。这样可以使得模型在训练时更加稳定，从而加快模型的收敛速度。</p>
</section>
<section id="residual-connection" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="residual-connection"><span class="header-section-number">2.8</span> Residual Connection</h3>
<p><span id="eq-residual-connection"><span class="math display">\[
\text{Output} = \mathrm{LayerNorm}\bigl(\mathbf{x} + \mathrm{Sublayer}(\mathbf{x})\bigr)
\tag{10}\]</span></span></p>
<div id="fig-residual-connection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/residuakl-connection.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Illustration of Residual Connection
</figcaption>
</figure>
</div>
<p>学习Residual Connection的目的是为了让模型更容易学习到Identity Mapping。具体来说，Residual Connection会将输入的向量与子层的输出相加，从而使得模型可以更容易地学习到Identity Mapping。这样可以使得模型在训练时更加稳定，从而加快模型的收敛速度。在优化的过程中，让Gradient Information更容易地传递到前面的层，从而使得模型更容易学习到Identity Mapping。Residual Connection的公式为： <span id="eq-residual-connection-gradient"><span class="math display">\[
\frac{\partial \mathcal L}{\partial \mathbf{x}} = \underbrace{\frac{\partial \mathcal L}{\partial \mathbf{y}}}_{\text{straight path}} + \underbrace{\frac{\partial \mathcal L}{\partial \mathbf{y}}\frac{\partial\,\mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}}}_{\text{through the sub-layer}}
\tag{11}\]</span></span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>在后续的研究中表示，Normalization的位置可以放在Residual Connection的前面或者后面，效果差不多。Transformer论文中是放在后面的（Post-Normalization）<a href="#eq-residual-connection" class="quarto-xref">Equation&nbsp;10</a> 。但在后续的研究中，很多模型（比如BERT）将Normalization放在Residual Connection的前面（Pre-Normalization）。 <span id="eq-pre-normalization"><span class="math display">\[
\text{Output} = \mathrm{Sublayer}(\mathbf{x}) + \mathrm{LayerNorm}(\mathbf{x})
\tag{12}\]</span></span></p>
</div>
</div>
</section>
<section id="point-wise-feed-forward-network" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="point-wise-feed-forward-network"><span class="header-section-number">2.9</span> Point-Wise Feed Forward Network</h3>
<p>Point-Wise Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Point-Wise Feed Forward Network由两个线性变换和一个ReLU激活函数组成。它的作用是对每个位置的向量进行非线性变换，从而使得模型可以更好地捕捉到输入序列中的不同语义信息。 Point-Wise Feed Forward Network的公式为： <span id="eq-point-wise-ffn"><span class="math display">\[
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\tag{13}\]</span></span></p>
<p>其中 <span class="math inline">\(W_1\)</span> 和 <span class="math inline">\(W_2\)</span> 是线性变换的权重矩阵，<span class="math inline">\(b_1\)</span> 和 <span class="math inline">\(b_2\)</span> 是偏置项。Point-Wise Feed Forward Network的作用是对每个位置的向量进行非线性变换，从而使得模型可以更好地捕捉到输入序列中的不同语义信息。</p>
</section>
<section id="output-linear-projection-softmax" class="level3" data-number="2.10">
<h3 data-number="2.10" class="anchored" data-anchor-id="output-linear-projection-softmax"><span class="header-section-number">2.10</span> Output Linear Projection &amp; Softmax</h3>
<p>在Transformer的Decoder中，最后一步是将Decoder的输出通过一个线性变换和Softmax函数转换为词汇表中的概率分布。这个过程可以看作是将Decoder的输出映射到词汇表中的每个词的概率。具体来说， <span id="eq-output-softmax"><span class="math display">\[\text{Output} = \text{Softmax}(xW + b)
\tag{14}\]</span></span> 其中 <span class="math inline">\(W\)</span> 是线性变换的权重矩阵，<span class="math inline">\(b\)</span> 是偏置项。Softmax函数将线性变换的输出转换为概率分布，从而使得模型可以生成下一个词的概率分布。</p>
<p>在这个过程中，Transformer 的作者采用了一种 <strong>weight tying</strong> 的方法，将输入的词嵌入矩阵和输出的线性变换矩阵共享同一个权重矩阵。这种方法可以减少模型的参数量，从而提高模型的效率，并且在实践中效果也很好。具体来说，Transformer 的作者将输入的词嵌入矩阵和输出的线性变换矩阵共享同一个权重矩阵，这样可以减少模型的参数量，从而提高模型的效率。</p>
</section>
<section id="full-process" class="level3 page-columns page-full" data-number="2.11">
<h3 data-number="2.11" class="anchored" data-anchor-id="full-process"><span class="header-section-number">2.11</span> Full Process</h3>
<div class="page-columns page-full">
<div id="fig-full-transformer" class="quarto-layout-panel page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-full-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<div class="quarto-layout-row column-page">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-full-transformer" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-transformer-encoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-transformer-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/transformer-encoding.gif" class="img-fluid figure-img" data-ref-parent="fig-full-transformer">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-transformer-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Transformer Encoding Process
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-full-transformer" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-transformer-decoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-transformer-decoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/transformer-decoding.gif" class="img-fluid figure-img" data-ref-parent="fig-full-transformer">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-transformer-decoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Transformer Decoding Process
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-full-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Illustation of Transformer Encoding and Decoding Process (Image Source: <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>)
</figcaption>
</figure>
</div>
</div>
<p>如图 <a href="#fig-full-transformer" class="quarto-xref">Figure&nbsp;4</a> 所示，Transformer的编码过程和解码过程是相似的。编码过程将输入的token转换为向量，然后通过多个Encoder Block进行编码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。解码过程则是将Encoder的输出与Decoder的输入进行交叉注意力计算，然后通过多个Decoder Block进行解码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。</p>
</section>
<section id="training-process" class="level3" data-number="2.12">
<h3 data-number="2.12" class="anchored" data-anchor-id="training-process"><span class="header-section-number">2.12</span> Training Process</h3>
<p>接下来，我们探讨一下Transformer的训练过程。Transformer的训练过程与其他神经网络模型类似，主要包括以下几个步骤：</p>
<ol type="1">
<li><strong>数据预处理</strong>：将输入的文本数据转换为token，并进行分词和编码。通常使用Word Embedding将每个token转换为一个固定维度的向量。</li>
<li><strong>模型初始化</strong>：初始化Transformer模型的参数，包括Word Embedding、Position Embedding、Multi-Head Attention、Point-Wise Feed Forward Network等模块的参数。</li>
<li><strong>前向传播</strong>：将输入的token通过Word Embedding和Position Embedding转换为向量，然后通过多个Encoder Block进行编码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。</li>
<li><strong>计算损失</strong>：使用交叉熵损失函数（Cross-Entropy Loss）计算模型的输出与真实标签之间的差异。交叉熵损失函数是一个常用的分类损失函数，它可以衡量模型的输出概率分布与真实标签之间的差异。</li>
<li><strong>反向传播</strong>：通过计算损失函数对模型参数的梯度，使用梯度下降算法（如Adam优化器）更新模型的参数。梯度下降算法是一种常用的优化算法，它可以通过计算损失函数对模型参数的梯度来更新模型的参数，从而使得模型的输出更接近真实标签</li>
</ol>
<p>具体的实现过程，我们将在接下来的PyTorch实现中详细介绍。</p>
</section>
</section>
<section id="pytorch-实现" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="pytorch-实现"><span class="header-section-number">3</span> PyTorch 实现</h2>
<p>接下来，我们叫利用PyTorch来实现Transformer的模型架构。我们采用Bottom-Up的方法，先实现Word Embedding， 接着是Position Embedding，接着实现我们的重点，即Multi-Head-Attention，再次之后，我们会实现 Point-Wise Feed Forward Network。最后将这几个模块组合起来，实现Transformer的Encoder 和 Decoder。准备好了吗？让我们开始吧！</p>
<section id="word-embedding" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="word-embedding"><span class="header-section-number">3.1</span> Word Embedding</h3>
<p>Word Embedding是将词语转换为向量的过程。在PyTorch 中的实现非常简单，我们可以使用<code>nn.Embedding</code>类来实现。这个类会将每个token映射到一个<em>固定维度</em>的向量空间中。</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">class</span> WordEmbedding(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb1-3"><a href="#cb1-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(config.vocab_size, config.d_model)</span>
<span id="cb1-6"><a href="#cb1-6"></a>    </span>
<span id="cb1-7"><a href="#cb1-7"></a>    <span class="kw">def</span> _init_weight(<span class="va">self</span>):</span>
<span id="cb1-8"><a href="#cb1-8"></a>        nn.init.trunc_normal_(<span class="va">self</span>.embedding.weight, mean <span class="op">=</span> <span class="dv">0</span>, std<span class="op">=</span><span class="fl">0.02</span>, a<span class="op">=-</span><span class="dv">3</span>, b<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-9"><a href="#cb1-9"></a>    </span>
<span id="cb1-10"><a href="#cb1-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-11"><a href="#cb1-11"></a>        <span class="cf">return</span> <span class="va">self</span>.embedding(x)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="position-embedding" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="position-embedding"><span class="header-section-number">3.2</span> Position Embedding</h3>
<p>接下来，我们来实现Position Embedding <a href="#eq-position-encoding" class="quarto-xref">Equation&nbsp;4</a>。</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">class</span> PositionEmbedding(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb2-3"><a href="#cb2-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-4"><a href="#cb2-4"></a></span>
<span id="cb2-5"><a href="#cb2-5"></a>        <span class="va">self</span>.seq_len <span class="op">=</span> config.seq_len</span>
<span id="cb2-6"><a href="#cb2-6"></a>        <span class="va">self</span>.d_model <span class="op">=</span> config.d_model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>有了Word Embedding 和 Position Embedding，我们就可以将输入的token转换为向量了。我们需要接下来需要做的就是，将这两个向量相加，得到最终的输入向量。</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> InputEmbedding(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb3-3"><a href="#cb3-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="va">self</span>.word_embedding <span class="op">=</span> WordEmbedding(config)</span>
<span id="cb3-6"><a href="#cb3-6"></a>        <span class="va">self</span>.position_embedding <span class="op">=</span> PositionEmbedding(config)</span>
<span id="cb3-7"><a href="#cb3-7"></a>    </span>
<span id="cb3-8"><a href="#cb3-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-9"><a href="#cb3-9"></a>        word_emb <span class="op">=</span> <span class="va">self</span>.word_embedding(x)</span>
<span id="cb3-10"><a href="#cb3-10"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding(x)</span>
<span id="cb3-11"><a href="#cb3-11"></a>        </span>
<span id="cb3-12"><a href="#cb3-12"></a>        <span class="cf">return</span> word_emb <span class="op">+</span> pos_emb</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="feed-forward-network" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="feed-forward-network"><span class="header-section-number">3.3</span> Feed Forward Network</h3>
<p>我们先跳过Multi Head Attention，先实现Feed Forward Network。Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Feed Forward Network由两个线性变换和一个ReLU激活函数组成。</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">class</span> FeedForwardNetwork(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb4-3"><a href="#cb4-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(config.d_model, config.d_ff)</span>
<span id="cb4-6"><a href="#cb4-6"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb4-7"><a href="#cb4-7"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(config.d_ff, config.d_model)</span>
<span id="cb4-8"><a href="#cb4-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-9"><a href="#cb4-9"></a>        <span class="cf">return</span> <span class="va">self</span>.linear2(<span class="va">self</span>.relu(<span class="va">self</span>.linear1(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="layer-normalization-1" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="layer-normalization-1"><span class="header-section-number">3.4</span> Layer Normalization</h3>
<p>还有一个重要的模块是Layer Normalization，它可以帮助模型更快地收敛。</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">class</span> LayerNormalization(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_features, eps<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb5-3"><a href="#cb5-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb5-5"><a href="#cb5-5"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.ones(in_features))</span>
<span id="cb5-6"><a href="#cb5-6"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(in_features))</span>
<span id="cb5-7"><a href="#cb5-7"></a></span>
<span id="cb5-8"><a href="#cb5-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-9"><a href="#cb5-9"></a>        mean <span class="op">=</span> x.mean(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-10"><a href="#cb5-10"></a>        var <span class="op">=</span> x.var(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>, unbiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-11"><a href="#cb5-11"></a>        normalized_x <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> torch.sqrt(var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb5-12"><a href="#cb5-12"></a>        <span class="cf">return</span> <span class="va">self</span>.weight <span class="op">*</span> normalized_x <span class="op">+</span> <span class="va">self</span>.bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="multi-head-attention-1" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="multi-head-attention-1"><span class="header-section-number">3.5</span> Multi Head Attention</h3>
<p>Multi Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>这部分是Transformer的核心模块，理解它是理解Transformer以及他变型的关键。记得多看几遍，直到你能理解为止。</p>
</div>
</div>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb6-3"><a href="#cb6-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>        <span class="cf">assert</span> config.d_model <span class="op">%</span> config.num_heads <span class="op">==</span> <span class="dv">0</span>, <span class="st">"d_model must be divisible by num_heads"</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>        <span class="va">self</span>.head_dim <span class="op">=</span> config.d_model <span class="op">//</span> config.num_heads</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a>        <span class="va">self</span>.qkv_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model <span class="op">*</span> <span class="dv">3</span>)</span>
<span id="cb6-9"><a href="#cb6-9"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="encoder-block" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="encoder-block"><span class="header-section-number">3.6</span> Encoder Block</h3>
<p>Encoder Block是Transformer的一个重要模块，它由Multi Head Attention和Feed Forward Network组成。它的作用是对输入的向量进行编码，从而捕捉到不同的语义信息。</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">class</span> EncoderBlock(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb7-3"><a href="#cb7-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb7-6"><a href="#cb7-6"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FeedForwardNetwork(config)</span>
<span id="cb7-7"><a href="#cb7-7"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNormalization(config.d_model)</span>
<span id="cb7-8"><a href="#cb7-8"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNormalization(config.d_model)</span>
<span id="cb7-9"><a href="#cb7-9"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-12"><a href="#cb7-12"></a>        <span class="co"># Multi Head Attention</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>        attn_output <span class="op">=</span> <span class="va">self</span>.attention(x)</span>
<span id="cb7-14"><a href="#cb7-14"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> <span class="va">self</span>.dropout(attn_output))   </span>
<span id="cb7-15"><a href="#cb7-15"></a>        <span class="co"># Feed Forward Network</span></span>
<span id="cb7-16"><a href="#cb7-16"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb7-17"><a href="#cb7-17"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> <span class="va">self</span>.dropout(ffn_output))</span>
<span id="cb7-18"><a href="#cb7-18"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="decoder-block" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="decoder-block"><span class="header-section-number">3.7</span> Decoder Block</h3>
<p>Decoder Block是Transformer的另一个重要模块，它与Encoder Block类似，但它还需要处理 Masked Multi Head Attention。Masked Multi Head Attention的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">class</span> DecoderBlock(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb8-3"><a href="#cb8-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()      </span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a>        <span class="va">self</span>.attention1 <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb8-6"><a href="#cb8-6"></a>        <span class="va">self</span>.attention2 <span class="op">=</span> MultiHeadAttention(config, is_causal<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>恭喜你，以及成功的实现了Transformer，这个是当前最重要的AI模型框架。理解了它，你就理解可以理解大部分的AI模型了。现在大火的ChatGPT，DeepSeek等模型都是基于Transformer的变型（在接下来的文章中，我们会阅读到这些模型）。完整的代码可以在<a href="">GitHub</a>上查看。</p>
</section>
</section>
<section id="扩展" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="扩展"><span class="header-section-number">4</span> 扩展</h2>
<p>自从Transformer被提出以来，已经有了很多的变型和改进。具体的来说，Attention在Transformer中需要 <span class="math inline">\(\mathcal{O}(n^2)\)</span> 的计算复杂度，这在处理长文本时会变得非常慢。因此，很多研究者提出了各种各样的改进方法来降低计算复杂度。以下是一些常见的改进方法：</p>
<ul>
<li><strong>Sparse Attention</strong>: 通过稀疏化注意力矩阵来降低计算复杂度。比如，Reformer模型使用了局部敏感哈希（LSH）来实现稀疏注意力。</li>
<li><strong>Linear Attention</strong>: 通过将注意力计算转换为线性时间复杂度</li>
</ul>
</section>
<section id="qa" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="qa"><span class="header-section-number">5</span> Q&amp;A</h2>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
问题：为什么要用 $\sqrt{d_k}$ 缩放点积？
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>回答：如果不缩放，当 <span class="math inline">\(d_k\)</span> 很大时，QK 的方差也会变大，使 softmax 落入梯度非常小的区域。除以 <span class="math inline">\(\sqrt{d_k}\)</span> 有助于将激活值保持在适合训练的范围内。</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
问题：多头注意力解决了什么问题？
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>回答：它让模型可以同时关注不同的表示子空间和位置的信息，从而克服单头自注意力容易“平均化”的问题。</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
问题：Transformer 如何实现自回归式解码？
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>回答：解码器通过将未来位置的注意力得分设为 <span class="math inline">\(-\infty\)</span> 来屏蔽它们，确保每个位置只能关注到前面的输出。</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
问题：为什么使用正弦位置编码而不是可学习的位置编码？
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>回答：正弦位置编码允许模型推广到更长的序列，参数量少，并且在 BLEU 分数上与可学习的位置编码效果相当。</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
问题：与 RNN/CNN 相比，路径长度和计算复杂度有何不同？
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>回答：自注意力的路径长度是常数，每层的计算复杂度是 <span class="math inline">\(\mathcal{O}(n²d)\)</span>；而 RNN 需要 <span class="math inline">\(\mathcal{O}(n)\)</span> 的串行步骤，CNN 需要堆叠多层才能覆盖长距离依赖。</p>
</div>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../00-how-to-read-paper.html" class="pagination-link" aria-label="00 Preparation for following">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">00 Preparation for following</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with ❤️ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>