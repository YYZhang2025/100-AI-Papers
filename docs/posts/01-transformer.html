<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yuyang Zhang">

<title>01: Attention is All You Need â€“ 100 Papers with Codes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../posts/02-vision-transformer.html" rel="next">
<link href="../00-how-to-read-paper.html" rel="prev">
<link href=".././images/icon.avif" rel="icon">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-0348920b7671f696dc9078d39bff215e.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-234273d1456647dabc34a594ac50e507.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7ddd24ee98d70ca00bb532ce6196e657.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-62960f262fec3dcc18a72ed9802b3bf0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-7ddd24ee98d70ca00bb532ce6196e657.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link href="https://fonts.cdnfonts.com/css/cmu-sans-serif" rel="stylesheet">
<style>
div.callout-QA.callout {
  border-left-color: #e7f3ff;
}
div.callout-QA.callout-style-default > .callout-header {
  background-color: rgba(231, 243, 255, 0.13);
}
div.callout-QA .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-QA.callout-style-default .callout-icon::before, div.callout-QA.callout-titled .callout-icon::before {
  content: 'ğŸ—¨ï¸';
  background-image: none;
}
div.callout-thumbs-up.callout {
  border-left-color: #008000;
}
div.callout-thumbs-up.callout-style-default > .callout-header {
  background-color: rgba(0, 128, 0, 0.13);
}
div.callout-thumbs-up .callout-toggle::before {  background-image: url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(33, 37, 41)" class="bi bi-chevron-down" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M1.646 4.646a.5.5 0 0 1 .708 0L8 10.293l5.646-5.647a.5.5 0 0 1 .708.708l-6 6a.5.5 0 0 1-.708 0l-6-6a.5.5 0 0 1 0-.708z"/></svg>');}
div.callout-thumbs-up.callout-style-default .callout-icon::before, div.callout-thumbs-up.callout-titled .callout-icon::before {
  font-family: 'Font Awesome 6 Free', FontAwesome;
  font-style: normal;
  content: '\f164' !important;
  background-image: none;
}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../style/style.css">
<meta property="og:title" content="01: Attention is All You Need â€“ 100 Papers with Codes">
<meta property="og:description" content="">
<meta property="og:image" content="01-attention.assets/transformer.png">
<meta property="og:site_name" content="100 Papers with Codes">
</head>

<body class="nav-sidebar docked quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../posts/01-transformer.html">100 Papers</a></li><li class="breadcrumb-item"><a href="../posts/01-transformer.html">01: Attention is All You Need</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src=".././images/logo.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main tools-wide">
    <a href="https://yyzhang2000.github.io/Blog/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-globe"></i></a>
    <a href="https://github.com/YYZhang2025" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.linkedin.com/in/zhang-yuyang/" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
    <a href="mailto:zhangyuyang1211@gmail.com" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-envelope"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../00-how-to-read-paper.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">00 Preparation for following</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">100 Papers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../posts/01-transformer.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">01: Attention is All You Need</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../posts/02-vision-transformer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">02: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="1">
    <h2 id="toc-title">Table of Contents</h2>
   
  <ul>
  <li><a href="#preliminary" id="toc-preliminary" class="nav-link active" data-scroll-target="#preliminary"><span class="header-section-number">1</span> Preliminary</a>
  <ul>
  <li><a href="#sec-dot-product" id="toc-sec-dot-product" class="nav-link" data-scroll-target="#sec-dot-product"><span class="header-section-number">1.1</span> Dot Product Similarityï¼ˆç‚¹ç§¯ç›¸ä¼¼åº¦ï¼‰</a></li>
  <li><a href="#sec-softmax" id="toc-sec-softmax" class="nav-link" data-scroll-target="#sec-softmax"><span class="header-section-number">1.2</span> Softmax å‡½æ•°</a></li>
  <li><a href="#matrix-multiplication" id="toc-matrix-multiplication" class="nav-link" data-scroll-target="#matrix-multiplication"><span class="header-section-number">1.3</span> Matrix Multiplication</a></li>
  </ul></li>
  <li><a href="#attention-is-all-you-need" id="toc-attention-is-all-you-need" class="nav-link" data-scroll-target="#attention-is-all-you-need"><span class="header-section-number">2</span> Attention is All You Need</a>
  <ul>
  <li><a href="#sec-input-embedding" id="toc-sec-input-embedding" class="nav-link" data-scroll-target="#sec-input-embedding"><span class="header-section-number">2.1</span> Input Embedding</a></li>
  <li><a href="#sec-positional-encoding" id="toc-sec-positional-encoding" class="nav-link" data-scroll-target="#sec-positional-encoding"><span class="header-section-number">2.2</span> Positional Encoding</a></li>
  <li><a href="#sec-multi-head-attention" id="toc-sec-multi-head-attention" class="nav-link" data-scroll-target="#sec-multi-head-attention"><span class="header-section-number">2.3</span> Multi-Head Attention</a>
  <ul class="collapse">
  <li><a href="#time-complexity-of-multi-head-attention" id="toc-time-complexity-of-multi-head-attention" class="nav-link" data-scroll-target="#time-complexity-of-multi-head-attention"><span class="header-section-number">2.3.1</span> Time Complexity of Multi-Head Attention</a></li>
  <li><a href="#causal-attention" id="toc-causal-attention" class="nav-link" data-scroll-target="#causal-attention"><span class="header-section-number">2.3.2</span> Causal Attention</a></li>
  <li><a href="#cross-attention" id="toc-cross-attention" class="nav-link" data-scroll-target="#cross-attention"><span class="header-section-number">2.3.3</span> Cross Attention</a></li>
  </ul></li>
  <li><a href="#sec-layer-normalization" id="toc-sec-layer-normalization" class="nav-link" data-scroll-target="#sec-layer-normalization"><span class="header-section-number">2.4</span> Layer Normalization</a></li>
  <li><a href="#residual-connection" id="toc-residual-connection" class="nav-link" data-scroll-target="#residual-connection"><span class="header-section-number">2.5</span> Residual Connection</a></li>
  <li><a href="#sec-point-wise-ffn" id="toc-sec-point-wise-ffn" class="nav-link" data-scroll-target="#sec-point-wise-ffn"><span class="header-section-number">2.6</span> Point-Wise Feed Forward Network</a></li>
  <li><a href="#output-linear-projection-softmax" id="toc-output-linear-projection-softmax" class="nav-link" data-scroll-target="#output-linear-projection-softmax"><span class="header-section-number">2.7</span> Output Linear Projection &amp; Softmax</a></li>
  <li><a href="#full-model" id="toc-full-model" class="nav-link" data-scroll-target="#full-model"><span class="header-section-number">2.8</span> Full Model</a></li>
  <li><a href="#training-process" id="toc-training-process" class="nav-link" data-scroll-target="#training-process"><span class="header-section-number">2.9</span> Training Process</a></li>
  </ul></li>
  <li><a href="#pytorch-å®ç°" id="toc-pytorch-å®ç°" class="nav-link" data-scroll-target="#pytorch-å®ç°"><span class="header-section-number">3</span> PyTorch å®ç°</a>
  <ul>
  <li><a href="#word-embedding" id="toc-word-embedding" class="nav-link" data-scroll-target="#word-embedding"><span class="header-section-number">3.1</span> Word Embedding</a></li>
  <li><a href="#position-embedding" id="toc-position-embedding" class="nav-link" data-scroll-target="#position-embedding"><span class="header-section-number">3.2</span> Position Embedding</a></li>
  <li><a href="#feed-forward-network" id="toc-feed-forward-network" class="nav-link" data-scroll-target="#feed-forward-network"><span class="header-section-number">3.3</span> Feed Forward Network</a></li>
  <li><a href="#layer-normalization" id="toc-layer-normalization" class="nav-link" data-scroll-target="#layer-normalization"><span class="header-section-number">3.4</span> Layer Normalization</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention"><span class="header-section-number">3.5</span> Multi Head Attention</a></li>
  <li><a href="#encoder-block" id="toc-encoder-block" class="nav-link" data-scroll-target="#encoder-block"><span class="header-section-number">3.6</span> Encoder Block</a></li>
  <li><a href="#decoder-block" id="toc-decoder-block" class="nav-link" data-scroll-target="#decoder-block"><span class="header-section-number">3.7</span> Decoder Block</a></li>
  </ul></li>
  <li><a href="#æ‰©å±•" id="toc-æ‰©å±•" class="nav-link" data-scroll-target="#æ‰©å±•"><span class="header-section-number">4</span> æ‰©å±•</a></li>
  <li><a href="#qa" id="toc-qa" class="nav-link" data-scroll-target="#qa"><span class="header-section-number">5</span> Q&amp;A</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../posts/01-transformer.html">100 Papers</a></li><li class="breadcrumb-item"><a href="../posts/01-transformer.html">01: Attention is All You Need</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">01: Attention is All You Need</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yuyang Zhang </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="preliminary" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="preliminary"><span class="header-section-number">1</span> Preliminary</h2>
<p>åœ¨ç†è§£Transformerä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ä¸€äº›åŸºæœ¬çš„æ¦‚å¿µã€‚æœ‰åŸºç¡€çš„åŒå­¦å¯ä»¥è·³è¿‡è¿™ä¸€èŠ‚ï¼Œç›´æ¥çœ‹ä¸‹ä¸€èŠ‚çš„Transformeræ¨¡å‹æ¶æ„ã€‚</p>
<section id="sec-dot-product" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="sec-dot-product"><span class="header-section-number">1.1</span> Dot Product Similarityï¼ˆç‚¹ç§¯ç›¸ä¼¼åº¦ï¼‰</h3>
<p>ç»™å®šä¸¤ä¸ªå‘é‡ <span class="math inline">\(\mathbf{q}, \mathbf{k} \in \mathbb{R}^d\)</span>, å®ƒä»¬çš„ç‚¹ç§¯ä¸º: <span id="eq-dot-product"><span class="math display">\[
\text{score}(\mathbf{q}, \mathbf{k}) = \mathbf{q} \cdot \mathbf{k} = \sum_{i=1}^{d} q_i k_i
\tag{1}\]</span></span> è¿™ä¸ªscoreç”¨äºæµ‹é‡ä¸¤ä¸ªå‘é‡çš„<em>ç›¸ä¼¼åº¦</em>, æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼</p>
</section>
<section id="sec-softmax" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="sec-softmax"><span class="header-section-number">1.2</span> Softmax å‡½æ•°</h3>
<p>ç»™å®šå‘é‡ <span class="math inline">\(\mathbf{x} = (x_1, x_2, \ldots, x_n)\)</span>ï¼Œsoftmax å‡½æ•°å®šä¹‰ä¸º: <span id="eq-softmax"><span class="math display">\[
\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
\tag{2}\]</span></span> softmax å‡½æ•°å°†å‘é‡ <span class="math inline">\(\mathbf{x}\)</span> è½¬æ¢ä¸ºä¸€ä¸ª<em>æ¦‚ç‡åˆ†å¸ƒ</em>ï¼Œæ‰€æœ‰å…ƒç´ çš„å’Œä¸º1ã€‚å®ƒå¸¸ç”¨äºå°†æ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚å…¶ä¸­ï¼Œè®¡ç®—softmax çš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n)\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(n\)</span> æ˜¯å‘é‡ <span class="math inline">\(\mathbf{x}\)</span> çš„ç»´åº¦ã€‚</p>
</section>
<section id="matrix-multiplication" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="matrix-multiplication"><span class="header-section-number">1.3</span> Matrix Multiplication</h3>
<p>ç»™å®šçŸ©é˜µ <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> å’Œ <span class="math inline">\(\mathbf{B} \in \mathbb{R}^{n \times p}\)</span>ï¼Œå®ƒä»¬çš„çŸ©é˜µä¹˜æ³•å®šä¹‰ä¸º: <span id="eq-matrix-multiplication"><span class="math display">\[
\mathbf{C} = \mathbf{A} \cdot \mathbf{B} \in \mathbb{R}^{m \times p}
\tag{3}\]</span></span> çŸ©é˜µä¹˜æ³•æ˜¯çº¿æ€§ä»£æ•°ä¸­çš„åŸºæœ¬è¿ç®—ï¼Œç”¨äºå°†ä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„çŸ©é˜µã€‚å®ƒåœ¨ç¥ç»ç½‘ç»œä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶ã€‚å…¶ä¸­ï¼ŒMatrix Multiplicationçš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(mnp)\)</span>ï¼Œå…¶ä¸­ <span class="math inline">\(m\)</span> æ˜¯ <span class="math inline">\(\mathbf{A}\)</span> çš„è¡Œæ•°ï¼Œ<span class="math inline">\(n\)</span> æ˜¯ <span class="math inline">\(\mathbf{A}\)</span> çš„åˆ—æ•°ï¼Œä¹Ÿæ˜¯ <span class="math inline">\(\mathbf{B}\)</span> çš„è¡Œæ•°ï¼Œ<span class="math inline">\(p\)</span> æ˜¯ <span class="math inline">\(\mathbf{B}\)</span> çš„åˆ—æ•°ã€‚ Wikipedia ä¸Šæœ‰ä¸€ä¸ªå…³äº <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_matrix_multiplication">Matrix Multiplication Time Complexity</a> çš„è¯¦ç»†ä»‹ç»ã€‚</p>
</section>
</section>
<section id="attention-is-all-you-need" class="level2 page-columns page-full" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="attention-is-all-you-need"><span class="header-section-number">2</span> Attention is All You Need</h2>
<p>åœ¨è¿™ä¸ªç¯‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥é˜…è¯»è¿™ç¯‡è®ºæ–‡ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹è¿™ç¯‡è®ºæ–‡çš„èƒŒæ™¯å’Œä¸»è¦è´¡çŒ®</p>
<blockquote class="blockquote">
<p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks â€¦ We propose a new simple network architecture, the Transformer, based <strong>solely on attention mechanisms</strong>, dispensing with recurrence and convolutions entirely.</p>
</blockquote>
<p>æœ¬ç¯‡è®ºæ–‡æå‡ºï¼šåºåˆ—å»ºæ¨¡ï¼ˆSequence Modelingï¼‰é•¿æœŸä¾èµ– RNN/CNNï¼Œå…¶è®¡ç®—å—åˆ¶äº<u>æ—¶é—´æ­¥éª¤çš„ä¸²è¡Œæ€§</u>ï¼Œéš¾ä»¥å¹¶è¡ŒåŒ–ï¼Œä¸”<u>é•¿è·ç¦»ä¾èµ–æ•è·æ•ˆç‡ä½</u>ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…æå‡ºäº†Transformeræ¨¡å‹ï¼Œå®Œå…¨åŸºäº<u>æ³¨æ„åŠ›æœºåˆ¶</u>ï¼Œå»é™¤äº†å¾ªç¯å’Œå·ç§¯æ“ä½œï¼Œä»è€Œå®ç°äº†<u>å…¨å±€ä¿¡æ¯äº¤äº’</u>ã€‚åˆ©ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰+ å‰é¦ˆç½‘ç»œ ï¼ˆFeed Forward Networkï¼‰çš„ç¼–ç å™¨-è§£ç å™¨ï¼Œå»é™¤äº†å¾ªç¯/å·ç§¯ï¼Œä»…é æ³¨æ„åŠ›å®ç°å…¨å±€ä¿¡æ¯äº¤äº’ã€‚ é™¤æ­¤ä¹‹å¤–ï¼Œä»–ä»¬è¿˜è®¾è®¡äº†Scaled Dot-Product Attention å’Œ Multi Head Attentionï¼Œæ¥<strong>é™ä½æ•°å€¼å°ºåº¦</strong>çš„åŒæ—¶å¹¶è¡Œ<strong>å­¦ä¹ å¤šå­ç©ºé—´è¡¨ç¤º</strong>ã€‚</p>
<p>ä»¥ä¸‹æ˜¯Transformerçš„æ•´ä½“æ¶æ„å›¾ï¼š</p>
<div id="fig-transformer-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-transformer-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/transformer.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-transformer-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The Transformer Family Version 2.0 | Lilâ€™Log
</figcaption>
</figure>
</div>
<p>å¦‚å›¾ <a href="#fig-transformer-architecture" class="quarto-xref">Figure&nbsp;1</a> æ‰€ç¤ºï¼ŒTransformeræ˜¯ç”±ï¼š</p>
<ul>
<li>Input Embedding: <a href="#sec-input-embedding" class="quarto-xref">Section&nbsp;2.1</a></li>
<li>Position Embedding: <a href="#sec-positional-encoding" class="quarto-xref">Section&nbsp;2.2</a></li>
<li>Multi-Head Attention: <a href="#sec-multi-head-attention" class="quarto-xref">Section&nbsp;2.3</a></li>
<li>Layer Normalization: <a href="#sec-layer-normalization" class="quarto-xref">Section&nbsp;2.4</a></li>
<li>Point-Wise Feed Forward Network: <a href="#sec-point-wise-ffn" class="quarto-xref">Section&nbsp;2.6</a></li>
</ul>
<p>è¿™å‡ ä¸ªå°æ¨¡å—æ¥ç»„æˆçš„ï¼Œæ¯ä¸ªå°æ¨¡å—ç»„æˆä¸€ä¸ªEncoder Blockï¼Œå¤šä¸ªEncoder Blockå †å èµ·æ¥å½¢æˆEncoderï¼ŒDecoder Blockä¹Ÿæ˜¯ç±»ä¼¼çš„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼šé€ä¸ªä»‹ç»è¿™äº›æ¨¡å—ã€‚</p>
<section id="sec-input-embedding" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="sec-input-embedding"><span class="header-section-number">2.1</span> Input Embedding</h3>
<p>Input Embeddingæ˜¯å°†è¾“å…¥çš„tokenè½¬æ¢ä¸ºå‘é‡çš„è¿‡ç¨‹ã€‚Transformerçš„è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œåºåˆ—ä¸­çš„æ¯ä¸ªè¯éƒ½è¢«è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡ã€‚<em>è¿™ä¸ªå‘é‡å¯ä»¥æ˜¯é¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œä¹Ÿå¯ä»¥æ˜¯éšæœºåˆå§‹åŒ–çš„å‘é‡</em>ã€‚Transformerä½¿ç”¨Word Embeddingæ¥å°†æ¯ä¸ªtokenè½¬æ¢ä¸ºä¸€ä¸ªå›ºå®šç»´åº¦çš„å‘é‡ã€‚è¿™åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­æ˜¯ä¸€ä¸ªå¸¸è§çš„åšæ³•ã€‚</p>
<hr>
</section>
<section id="sec-positional-encoding" class="level3 page-columns page-full" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="sec-positional-encoding"><span class="header-section-number">2.2</span> Positional Encoding</h3>
<p>Transformer çš„ä¸€ä¸ªé‡è¦ç‰¹ç‚¹æ˜¯å®ƒä¸ä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æˆ–å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥å¤„ç†åºåˆ—æ•°æ®ã€‚è¿™å°±å¯¼è‡´äº†ä¸€ä¸ªé—®é¢˜ï¼šTransformeræ— æ³•æ•æ‰åˆ°åºåˆ—ä¸­è¯è¯­çš„ä½ç½®ä¿¡æ¯ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒTransformerå¼•å…¥äº†ä½ç½®ç¼–ç ï¼ˆPosition Encodingï¼‰ã€‚</p>
<blockquote class="blockquote">
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.</p>
</blockquote>
<p>ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå®ƒä¸ºæ¯ä¸ªè¯æä¾›äº†ä¸€ä¸ªå”¯ä¸€çš„ä½ç½®ä¿¡æ¯ã€‚<strong>ä½ç½®ç¼–ç çš„ç»´åº¦ä¸è¯å‘é‡çš„ç»´åº¦ç›¸åŒ</strong>ã€‚Transformerä½¿ç”¨ä½ç½®ç¼–ç æ¥ä¸º<em>æ¯ä¸ªè¯æä¾›ä¸€ä¸ªå”¯ä¸€çš„ä½ç½®ä¿¡æ¯</em>ï¼Œä»è€Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°è¯è¯­åœ¨åºåˆ—ä¸­çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚</p>
<p>åœ¨Transformerä¸­ï¼Œä½ç½®ç¼–ç æ˜¯é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°æ¥å®ç°çš„ï¼šå…·ä½“æ¥è¯´ï¼Œå¯¹äºåºåˆ—ä¸­çš„ç¬¬ <span class="math inline">\(pos\)</span> ä¸ªä½ç½®ï¼Œä½ç½®ç¼–ç çš„ç¬¬ <span class="math inline">\(i\)</span> ç»´å¯ä»¥è¡¨ç¤ºä¸ºï¼š <span id="eq-position-encoding"><span class="math display">\[
\begin{align}
\mathrm{PE}(pos,2i)=\sin\!\bigl(pos \times \tfrac{1}{10000^{2i/d_\mathrm{model}}}\bigr) \\
\mathrm{PE}(pos,2i{+}1)=\cos\!\bigl(pos \times \tfrac{1}{10000^{2i/d_\mathrm{model}}}\bigr)
\end{align}
\tag{4}\]</span></span></p>
<p>å…¶ä¸­ <span class="math inline">\(d_{\text{model}}\)</span> æ˜¯ä½ç½®ç¼–ç çš„ç»´åº¦ï¼ˆä¸Word Embedding çš„ç»´åº¦ä¸€æ ·å¤§ï¼‰ã€‚ä½ç½®ç¼–ç çš„ä½œç”¨æ˜¯ä¸ºæ¯ä¸ªè¯æä¾›ä¸€ä¸ªå”¯ä¸€çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°è¯è¯­åœ¨åºåˆ—ä¸­çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚</p>
<div class="column-page">
<div id="fig-position-encoding" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-position-encoding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-position-100" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-position-100-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/position-100.png" class="img-fluid figure-img" data-ref-parent="fig-position-encoding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-position-100-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Position Encoding with max sequence length 100
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-position-encoding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-position-200" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-position-200-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/position-200.png" class="img-fluid figure-img" data-ref-parent="fig-position-encoding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-position-200-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Position Encoding with max sequence length 100
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-position-encoding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-position-max-len-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-position-max-len-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/positional_encoding.gif" class="img-fluid figure-img" data-ref-parent="fig-position-encoding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-position-max-len-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Position Encoding with max sequence length from 100 to 200
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-position-encoding" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-position-log-base-gif" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-position-log-base-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/position-logbase_encoding.gif" class="img-fluid figure-img" data-ref-parent="fig-position-encoding">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-position-log-base-gif-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d) Position Encoding with log base from 10,000 to 1,000
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustration of Position Encoding with different max sequence lengths, the horizontal red line represent the encoding at position 50. The encoding is consistent across different max sequence lengths, which allows the model to generalize to longer sequences.
</figcaption>
</figure>
</div>
</div>
<p>ä» <a href="#fig-position-encoding" class="quarto-xref">Figure&nbsp;2</a> ä¸­å¯ä»¥çœ‹åˆ°ï¼Œéšç€max sequence lengthçš„å¢åŠ ï¼Œä½ç½®ç¼–ç çš„å˜åŒ–æ˜¯è¿ç»­çš„ <a href="#fig-position-max-len-gif" class="quarto-xref">Figure&nbsp;2 (c)</a> ã€‚å¹¶ä¸”åœ¨ä¸åŒçš„max sequence lengthä¸‹ï¼Œä½ç½®ç¼–ç çš„å˜åŒ–æ˜¯ç›¸åŒçš„ï¼Œ å›¾ <a href="#fig-position-100" class="quarto-xref">Figure&nbsp;2 (a)</a>, <a href="#fig-position-200" class="quarto-xref">Figure&nbsp;2 (b)</a>ï¼Œ æ˜¾ç¤ºçš„æ˜¯ä½ç½®50åœ¨ä¸åŒçš„max sequence lengthä¸‹çš„ä½ç½®ç¼–ç å˜åŒ–ã€‚å¯ä»¥çœ‹åˆ°ï¼Œä½ç½®ç¼–ç åœ¨ä¸åŒçš„max sequence lengthä¸‹æ˜¯ç›¸åŒçš„ï¼Œè¿™ä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—ä¸Šã€‚</p>
<div id="fig-position-detail" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-position-detail-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/position-detail.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-position-detail-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Details of Position Encoding
</figcaption>
</figure>
</div>
<p>å›¾ <a href="#fig-position-detail" class="quarto-xref">Figure&nbsp;3</a> å±•ç¤ºäº†ä½ç½®ç¼–ç ä¸åŒdimensionä¹‹é—´çš„ç»†èŠ‚ã€‚ç›¸å¯¹äºdiminution(4, 5), dimension(6,7) çš„éšç€ä½ç½®å˜åŒ–æ›´å¤§ã€‚ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼š</p>
<ul>
<li>ä½ <span class="math inline">\(i\)</span>ï¼ˆé å‰çš„ç»´åº¦ï¼‰â€”â€” æ³¢é•¿çŸ­ï¼Œéšä½ç½® pos å˜åŒ–å¾—å¿« â†’ æ˜“åŒºåˆ«ç›¸é‚» tokenï¼›</li>
<li>é«˜ <span class="math inline">\(i\)</span>ï¼ˆé åçš„ç»´åº¦ï¼‰â€”â€” æ³¢é•¿é•¿ï¼Œéšä½ç½® pos å˜åŒ–å¾—æ…¢ â†’ æ•è·å…¨å±€ä½ç½®ä¿¡æ¯ã€‚</li>
</ul>
<p>é™¤æ­¤ä¹‹å¤–ï¼Œä½ç½®ç¼–ç è¿˜å¯ä»¥é€šè¿‡æ”¹å˜æ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„åŸºæ•°æ¥å®ç°ä¸åŒçš„æ•ˆæœã€‚æ¯”å¦‚ï¼Œå¯ä»¥å°†åŸºæ•°ä»10000æ”¹ä¸º1000ï¼Œè¿™æ ·å¯ä»¥ä½¿å¾—ä½ç½®ç¼–ç çš„æ³¢é•¿å˜çŸ­ï¼Œä»è€Œä½¿å¾—æ¨¡å‹æ›´å®¹æ˜“æ•æ‰åˆ°ç›¸é‚»ä½ç½®çš„ä¿¡æ¯ã€‚å›¾ <a href="#fig-position-log-base-gif" class="quarto-xref">Figure&nbsp;2 (d)</a> å±•ç¤ºäº†ä½ç½®ç¼–ç åœ¨ä¸åŒçš„åŸºæ•°ä¸‹çš„å˜åŒ–ã€‚</p>
<hr>
</section>
<section id="sec-multi-head-attention" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="sec-multi-head-attention"><span class="header-section-number">2.3</span> Multi-Head Attention</h3>
<p>Multi-Head Attentionæ˜¯Transformerçš„æ ¸å¿ƒæ¨¡å—ã€‚å®ƒçš„ä½œç”¨æ˜¯å°†è¾“å…¥çš„å‘é‡è¿›è¡Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚Attention å…¶å®å°±æ˜¯ä¸€ä¸ª<strong>åŠ æƒæ±‚å’Œ</strong>çš„è¿‡ç¨‹ï¼Œå®ƒå¯ä»¥çœ‹ä½œæ˜¯å¯¹è¾“å…¥å‘é‡çš„åŠ æƒå¹³å‡ã€‚å…¶æ ¸å¿ƒå…¬å¼ä¸ºï¼š <span id="eq-attention"><span class="math display">\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\tag{5}\]</span></span></p>
<p>å…¶ä¸­ <span class="math inline">\(Q\)</span>ã€<span class="math inline">\(K\)</span>ã€<span class="math inline">\(V\)</span> åˆ†åˆ«æ˜¯æŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰çŸ©é˜µï¼Œå…¶ä¸­ <span class="math inline">\(\sqrt{d_k}\)</span> æ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œ<u>ç”¨æ¥é˜²æ­¢ç‚¹ç§¯çš„å€¼è¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±</u>ã€‚è¿™ä¸ªå…¬å¼çš„å«ä¹‰æ˜¯ï¼šé¦–å…ˆè®¡ç®—æŸ¥è¯¢å’Œé”®çš„ç‚¹ç§¯ (<a href="#sec-dot-product" class="quarto-xref">Section&nbsp;1.1</a>) ï¼Œç„¶åé€šè¿‡Softmaxå‡½æ•° (<a href="#sec-softmax" class="quarto-xref">Section&nbsp;1.2</a>) å°†å…¶è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæœ€åç”¨è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒå¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="#eq-attention" class="quarto-xref">Equation&nbsp;5</a> æ˜¯Attentionçš„æ ¸å¿ƒå…¬å¼ï¼Œè€ŒAttentionæ˜¯Transformerçš„æ ¸å¿ƒæ¨¡å—ã€‚ç†è§£è¿™ä¸ªå…¬å¼æ˜¯ç†è§£Transformerçš„å…³é”®ã€‚åç»­å¾ˆå¤šçš„åˆ›æ–°æ¯”å¦‚ï¼ŒLinear Attention <span class="citation" data-cites="LinformerSelfAttentionLinear2020wang">(<a href="#ref-LinformerSelfAttentionLinear2020wang" role="doc-biblioref">Wang et al. 2020</a>)</span>ï¼Œ Multi-head Latent Attention<span class="citation" data-cites="DeepSeekV2StrongEconomical2024deepseek-ai">(<a href="#ref-DeepSeekV2StrongEconomical2024deepseek-ai" role="doc-biblioref">DeepSeek-AI et al. 2024</a>)</span> éƒ½æ˜¯åœ¨è¿™ä¸ªå…¬å¼çš„åŸºç¡€ä¸Šè¿›è¡Œæ”¹è¿›ã€‚</p>
</div>
</div>
<blockquote class="blockquote">
<p>Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to <span class="math inline">\(d_k\)</span>, <span class="math inline">\(d_k\)</span> and <span class="math inline">\(d_v\)</span> dimensions, respectively.</p>
</blockquote>
<p>Multi-Head Attention æ˜¯Attention <a href="#eq-attention" class="quarto-xref">Equation&nbsp;5</a> çš„ä¸€ä¸ªæ‰©å±•ï¼Œå®ƒå°†è¾“å…¥çš„å‘é‡åˆ†æˆå¤šä¸ªå­ç©ºé—´ï¼ˆheadï¼‰ï¼Œç„¶ååœ¨æ¯ä¸ªå­ç©ºé—´ä¸Šç‹¬ç«‹åœ°è®¡ç®—æ³¨æ„åŠ›ã€‚æœ€åï¼Œå°†æ‰€æœ‰å­ç©ºé—´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚Multi-Head Attention çš„å…¬å¼ä¸ºï¼š <span id="eq-multi-head-attention"><span class="math display">\[
\begin{align*}
\text{MultiHead}(Q, K, V) &amp;= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \\
\text{where}\ \text{head}_i &amp;= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{align*}    
\tag{6}\]</span></span></p>
<p>å…¶ä¸­ <span class="math inline">\(W_i^Q, W_i^K \in \mathbb{R}^{d_\text{model} \times d_k}, W_i^V \in \mathbb{R}^{d_\text{model} \times d_v}\)</span> æ˜¯ç”¨äºå°†è¾“å…¥å‘é‡æŠ•å½±åˆ°å­ç©ºé—´çš„æƒé‡çŸ©é˜µï¼Œ<span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_\text{model} }\)</span> æ˜¯ç”¨äºå°†æ‰€æœ‰å­ç©ºé—´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥çš„æƒé‡çŸ©é˜µã€‚Multi-Head Attention çš„ä½œç”¨æ˜¯<u>é€šè¿‡å¤šä¸ªå­ç©ºé—´æ¥æ•æ‰ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›</u>ã€‚</p>
<blockquote class="blockquote">
<p>Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.</p>
</blockquote>
<p>æ–‡ç« ä¸­æåˆ°ï¼Œå°½ç®¡å¢åŠ äº†Headsä¼šå¢åŠ è®¡ç®—é‡ï¼Œä½†ç”±äºæ¯ä¸ªHeadçš„ç»´åº¦è¾ƒå°ï¼Œå¹¶ä¸”å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„æ•ˆç‡ã€‚</p>
<section id="time-complexity-of-multi-head-attention" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="time-complexity-of-multi-head-attention"><span class="header-section-number">2.3.1</span> Time Complexity of Multi-Head Attention</h4>
<p>åœ¨ç»§ç»­äº†è§£åˆ«çš„componentä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæ¥åˆ†æä¸€ä¸‹Multi-Head Attentionçš„æ—¶é—´å¤æ‚åº¦ã€‚å‡è®¾inputçš„é•¿åº¦ä¸º <span class="math inline">\(n\)</span>ï¼Œæ¯ä¸ªheadçš„ç»´åº¦ä¸º <span class="math inline">\(d_k\)</span>ï¼Œé‚£ä¹ˆè®¡ç®— <span class="math inline">\(QK^T\)</span> çš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n^2 d_k)\)</span> (<a href="#eq-matrix-multiplication" class="quarto-xref">Equation&nbsp;3</a>)ã€‚</p>
<p>æ¥ä¸‹æ¥æ˜¯Softmaxçš„è®¡ç®—ï¼ŒSoftmaxçš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n)\)</span>(<a href="#sec-softmax" class="quarto-xref">Section&nbsp;1.2</a>)ã€‚å¯¹äºæ¯ä¸ªscore matrix <span class="math inline">\(QK^T \in \mathbb{R}^{n \times n}\)</span> çš„æ¯ä¸€è¡Œï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®— softmaxï¼Œè¿™éœ€è¦ <span class="math inline">\(n\)</span> æ¬¡è®¡ç®—ã€‚å› æ­¤ï¼ŒSoftmaxçš„æ—¶é—´å¤æ‚åº¦ä¸º <span class="math inline">\(\mathcal{O}(n^2)\)</span>ã€‚</p>
<p>ä¹‹åæ˜¯å¯¹äºvalueçš„åŠ æƒï¼Œè®¡ç®—å¤æ‚åº¦ä¹Ÿæ˜¯ <span class="math inline">\(\mathcal{O}(n^2d)\)</span>. æ‰€ä»¥æ€»çš„æ—¶é—´å¤æ‚åº¦ä¸ºï¼š<span class="math inline">\(\mathcal{O}(n^2d)\)</span>, éšç€ <span class="math inline">\(n\)</span> çš„å¢åŠ ï¼Œè®¡ç®—å¤æ‚åº¦ä¼šPolynomial å¢é•¿ã€‚å…·ä½“çš„è®¡ç®—å¤æ‚åº¦å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š <span id="eq-multi-head-attention-complexity"><span class="math display">\[
\begin{array}{|l|l|}
\hline
\textbf{Step} &amp; \textbf{Time Complexity} \\
\hline
QK^\top &amp; \mathcal{O}(n^2 d) \\
\text{softmax}(QK^\top) &amp; \mathcal{O}(n^2) \\
\text{attention} \times V &amp; \mathcal{O}(n^2 d) \\
\hline
\textbf{Total} &amp; \mathcal{O}(n^2 d) \\
\hline
\end{array}
\tag{7}\]</span></span></p>
</section>
<section id="causal-attention" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="causal-attention"><span class="header-section-number">2.3.2</span> Causal Attention</h4>
<p>Causal Attentionï¼ˆå› æœæ³¨æ„åŠ›ï¼‰æ˜¯Transformerä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼Œå®ƒçš„ä½œç”¨æ˜¯<u>é˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒæ—¶çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ï¼Œä»è€Œä¿è¯æ¨¡å‹çš„è‡ªå›å½’ç‰¹æ€§</u>ã€‚å…·ä½“æ¥è¯´ï¼ŒCausal Attentionä¼šåœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ï¼Œå°†æœªæ¥çš„ä¿¡æ¯å±è”½æ‰ï¼Œä»è€Œä½¿å¾—æ¨¡å‹åªèƒ½çœ‹åˆ°å½“å‰å’Œè¿‡å»çš„ä¿¡æ¯ã€‚è¿™æ ·å¯ä»¥ä¿è¯æ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¸ä¼šçœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ï¼Œä»è€Œä¿è¯æ¨¡å‹çš„è‡ªå›å½’ç‰¹æ€§ã€‚ Causal Attentioné€šå¸¸ä½œç”¨åœ¨Decoder æ¨¡å—ä¸­ã€‚</p>
<div id="fig-causal-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-causal-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/causal-attention.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-causal-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Example of Causal Attention
</figcaption>
</figure>
</div>
<p>Causal Attentionçš„å…¬å¼ä¸ºï¼š <span id="eq-causal-attention"><span class="math display">\[
\text{CausalAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
\tag{8}\]</span></span></p>
<p>åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œ<span class="math inline">\(M\)</span> æ˜¯ä¸€ä¸ªæ©ç çŸ©é˜µï¼ˆmask matrixï¼‰ï¼Œå®ƒçš„ä½œç”¨æ˜¯å°†æœªæ¥çš„ä¿¡æ¯å±è”½æ‰ã€‚<span class="math inline">\(M\)</span> æ˜¯ä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œå®ƒçš„å¯¹è§’çº¿ä»¥ä¸‹çš„å…ƒç´ ä¸º0ï¼Œå¯¹è§’çº¿ä»¥ä¸Šçš„å…ƒç´ ä¸º<span class="math inline">\(-\infty\)</span>ã€‚è¿™æ ·åœ¨è®¡ç®—Softmaxæ—¶ï¼Œå¯¹è§’çº¿ä»¥ä¸Šçš„å…ƒç´ ä¼šè¢«å±è”½æ‰ï¼Œä»è€Œä¿è¯æ¨¡å‹åªèƒ½çœ‹åˆ°å½“å‰å’Œè¿‡å»çš„ä¿¡æ¯ã€‚</p>
</section>
<section id="cross-attention" class="level4" data-number="2.3.3">
<h4 data-number="2.3.3" class="anchored" data-anchor-id="cross-attention"><span class="header-section-number">2.3.3</span> Cross Attention</h4>
<p>Cross Attentionï¼ˆäº¤å‰æ³¨æ„åŠ›ï¼‰ä½œç”¨æ˜¯å°†Encoderçš„è¾“å‡ºä¸Decoderçš„è¾“å…¥è¿›è¡Œäº¤å‰æ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œä½¿å¾—Decoderèƒ½å¤Ÿåˆ©ç”¨Encoderçš„è¾“å‡ºä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼ŒCross Attentionä¼šå°†Encoderçš„è¾“å‡ºä½œä¸ºé”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰ï¼Œå°†Decoderçš„è¾“å…¥ä½œä¸ºæŸ¥è¯¢ï¼ˆQueryï¼‰ï¼Œç„¶åè®¡ç®—æ³¨æ„åŠ›ã€‚ Cross Attentionçš„å…¬å¼ä¸Attention <a href="#eq-attention" class="quarto-xref">Equation&nbsp;5</a> ç±»ä¼¼ï¼Œåªä¸è¿‡å°†Encoderçš„è¾“å‡ºä½œä¸ºé”®å’Œå€¼ï¼ŒDecoderçš„è¾“å…¥ä½œä¸ºæŸ¥è¯¢</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Cross Attention é™¤äº†å¯ä»¥åº”ç”¨åœ¨textä¸­ï¼Œå¯ä»¥åº”ç”¨åœ¨ä¸åŒçš„modalityä¹‹é—´ï¼Œæ¯”å¦‚å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„äº¤å‰æ³¨æ„åŠ›ã€‚æ¯”å¦‚åœ¨è§†è§‰é—®ç­”ï¼ˆVisual Question Answeringï¼‰ä»»åŠ¡ä¸­ï¼ŒCross Attentionå¯ä»¥å°†å›¾åƒç‰¹å¾ä¸é—®é¢˜æ–‡æœ¬è¿›è¡Œäº¤å‰æ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å›¾åƒä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚</p>
</div>
</div>
</section>
</section>
<section id="sec-layer-normalization" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="sec-layer-normalization"><span class="header-section-number">2.4</span> Layer Normalization</h3>
<p>Layer Normalizationæ˜¯Transformerä¸­çš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›ã€‚Layer Normalizationçš„ä½œç”¨æ˜¯å¯¹æ¯ä¸ªä½ç½®çš„å‘é‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä»è€Œä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ›´åŠ ç¨³å®šã€‚å…·ä½“æ¥è¯´ï¼ŒLayer Normalizationä¼šå¯¹<strong>æ¯ä¸ªä½ç½®çš„å‘é‡(feature)</strong>è¿›è¡Œå‡å€¼å’Œæ–¹å·®çš„å½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—æ¯ä¸ªä½ç½®çš„å‘é‡éƒ½å…·æœ‰ç›¸åŒçš„åˆ†å¸ƒã€‚è¿™æ ·å¯ä»¥ä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ›´åŠ ç¨³å®šï¼Œä»è€ŒåŠ å¿«æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦ã€‚</p>
<div id="fig-layer-normalization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-layer-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/layer-norm.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-layer-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Illustration of Layer Normalization
</figcaption>
</figure>
</div>
<p>Layer Normalizationçš„å…¬å¼ä¸ºï¼š <span id="eq-layer-normalization"><span class="math display">\[
\text{LayerNorm}(\mathrm{x}) = \frac{\mathrm{x} - \mu}{\sigma + \epsilon} \cdot \gamma + \beta
\tag{9}\]</span></span> å…¶ä¸­ <span class="math inline">\(\mu\)</span> æ˜¯å‘é‡ <span class="math inline">\(\mathrm{x}\)</span> çš„å‡å€¼ï¼Œ<span class="math inline">\(\sigma\)</span> æ˜¯å‘é‡ <span class="math inline">\(\mathrm{x}\)</span> çš„æ ‡å‡†å·®ã€‚</p>
<p>å¯¹äºä¸€ä¸ªbatchçš„è¾“å…¥ï¼ŒLayer Normalizationä¼šå¯¹æ¯ä¸ªä½ç½®çš„å‘é‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªbatchè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚è¿™ä½¿å¾—Layer Normalizationå¯ä»¥æ›´å¥½åœ°é€‚åº”ä¸åŒé•¿åº¦çš„åºåˆ—ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚å…·ä½“æ¥è¯´, <span class="math inline">\(x \in \mathbb{R}^{B \times H \times S \times d_v}\)</span>æˆ‘ä»¬å¯¹æ¯ä¸ªä½ç½®çš„å‘é‡ä¹Ÿå°±æ˜¯ <span class="math inline">\(d_v\)</span> ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªbatchè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚</p>
<hr>
</section>
<section id="residual-connection" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="residual-connection"><span class="header-section-number">2.5</span> Residual Connection</h3>
<p>Residual Connectionï¼ˆæ®‹å·®è¿æ¥ï¼‰ä½œç”¨æ˜¯å°†è¾“å…¥çš„å‘é‡ä¸å­å±‚çš„è¾“å‡ºç›¸åŠ ï¼Œä»è€Œä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å®¹æ˜“åœ°å­¦ä¹ åˆ°Identity Mappingã€‚Residual Connectionçš„å¦‚å›¾æ‰€ç¤ºï¼š</p>
<div id="fig-residual-connection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="01-attention.assets/residuakl-connection.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Illustration of Residual Connection
</figcaption>
</figure>
</div>
<p>Residual Connectionåœ¨Transformerçš„åº”ç”¨å…¬å¼ä¸ºï¼š <span id="eq-residual-connection"><span class="math display">\[
\mathbf{y} = \text{LayerNorm}(\mathbf{x} + \mathrm{Sublayer}(\mathbf{x}))
\tag{10}\]</span></span></p>
<p>åœ¨è®­ç»ƒæ—¶ï¼Œæ®‹å·®è¿æ¥å¯ä»¥ä¸ºæ¢¯åº¦æä¾›ä¸€æ¡â€œæ·å¾„â€ï¼Œå³æ¢¯åº¦å¯ä»¥ä¸ç»è¿‡å­å±‚å¤æ‚çš„éçº¿æ€§å˜æ¢ï¼Œç›´æ¥ä¼ å›å‰é¢å±‚çš„è¾“å…¥ã€‚è¿™èƒ½æœ‰æ•ˆç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚</p>
<p><span id="eq-residual-connection-gradient"><span class="math display">\[
\begin{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{x}}
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \\
&amp;= \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot \left( \mathbf{I} + \frac{\partial \mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}} \right) \\
&amp;= \underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}}}_{\text{straight path}} +
\underbrace{\frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot
\frac{\partial\,\mathrm{Sublayer}(\mathbf{x})}{\partial \mathbf{x}}}_{\text{through the sub-layer}}
\end{align}
\tag{11}\]</span></span> å…¶ä¸­ï¼Œ <span class="math inline">\(\mathcal{L}\)</span> æ˜¯æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç”±äºå­˜åœ¨ç¬¬ä¸€é¡¹ï¼Œå“ªæ€•å­å±‚æ¢¯åº¦æ¥è¿‘ 0ï¼ŒGradient çš„ä¿¡æ¯ä¹Ÿä¸ä¼šå®Œå…¨ä¸¢å¤±ã€‚</p>
<div class="callout callout-style-default callout-note callout-titled" title="Pre-Normalization vs Post-Normalization">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pre-Normalization vs Post-Normalization
</div>
</div>
<div class="callout-body-container callout-body">
<p>Transformerè®ºæ–‡ä¸­, Normalizationçš„ä½ç½®æ˜¯æ”¾åœ¨Residual Connectionçš„åé¢ï¼ˆPost-Normalizationï¼‰<a href="#eq-residual-connection" class="quarto-xref">Equation&nbsp;10</a> ã€‚ä½†åœ¨åç»­çš„ç ”ç©¶ä¸­ï¼Œå¾ˆå¤šæ¨¡å‹ï¼ˆæ¯”å¦‚BERTï¼‰å°†Normalizationæ”¾åœ¨Residual Connectionçš„å‰é¢ï¼ˆPre-Normalizationï¼‰ã€‚ <span id="eq-pre-normalization"><span class="math display">\[
\text{Output} = \mathrm{Sublayer}(\mathbf{x}) + \mathrm{LayerNorm}(\mathbf{x})
\tag{12}\]</span></span></p>
</div>
</div>
</section>
<section id="sec-point-wise-ffn" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="sec-point-wise-ffn"><span class="header-section-number">2.6</span> Point-Wise Feed Forward Network</h3>
<blockquote class="blockquote">
<p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.</p>
</blockquote>
<p>Point-Wise Feed Forward Networkä½œç”¨æ˜¯å¯¹æ¯ä¸ªä½ç½®çš„å‘é‡è¿›è¡Œéçº¿æ€§å˜æ¢, å…¶å…¬å¼ä¸ºï¼š <span id="eq-point-wise-ffn"><span class="math display">\[
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
\tag{13}\]</span></span></p>
<p>å…¶ä¸­ <span class="math inline">\(W_1\)</span> å’Œ <span class="math inline">\(W_2\)</span> æ˜¯çº¿æ€§å˜æ¢çš„æƒé‡çŸ©é˜µï¼Œ<span class="math inline">\(b_1\)</span> å’Œ <span class="math inline">\(b_2\)</span> æ˜¯åç½®é¡¹ã€‚Point-Wise Feed Forward Networkçš„ä½œç”¨æ˜¯å¯¹æ¯ä¸ªä½ç½®çš„å‘é‡è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œä»è€Œä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ•æ‰åˆ°è¾“å…¥åºåˆ—ä¸­çš„ä¸åŒè¯­ä¹‰ä¿¡æ¯ã€‚</p>
</section>
<section id="output-linear-projection-softmax" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="output-linear-projection-softmax"><span class="header-section-number">2.7</span> Output Linear Projection &amp; Softmax</h3>
<p>åœ¨Transformerçš„Decoderä¸­ï¼Œæœ€åä¸€æ­¥æ˜¯å°†Decoderçš„è¾“å‡ºé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å’ŒSoftmaxå‡½æ•°è½¬æ¢ä¸ºè¯æ±‡è¡¨ä¸­çš„æ¦‚ç‡åˆ†å¸ƒã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥çœ‹ä½œæ˜¯å°†Decoderçš„è¾“å‡ºæ˜ å°„åˆ°è¯æ±‡è¡¨ä¸­çš„æ¯ä¸ªè¯çš„æ¦‚ç‡ã€‚å…·ä½“æ¥è¯´ï¼Œ <span id="eq-output-softmax"><span class="math display">\[\text{Output} = \text{Softmax}(xW + b)
\tag{14}\]</span></span> å…¶ä¸­ <span class="math inline">\(W \in \mathbb{R} ^ {d_\text{model} \times vocab}\)</span> æ˜¯çº¿æ€§å˜æ¢çš„æƒé‡çŸ©é˜µï¼Œ<span class="math inline">\(b\)</span> æ˜¯åç½®é¡¹ã€‚Softmaxå‡½æ•°å°†çº¿æ€§å˜æ¢çš„è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œä½¿å¾—æ¨¡å‹å¯ä»¥ç”Ÿæˆä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p>
<p>åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼ŒTransformer çš„ä½œè€…é‡‡ç”¨äº†ä¸€ç§ <strong>weight tying</strong><span class="citation" data-cites="UsingOutputEmbedding2017press">(<a href="#ref-UsingOutputEmbedding2017press" role="doc-biblioref">Press and Wolf 2017</a>)</span> çš„æ–¹æ³•ï¼Œå°†è¾“å…¥çš„è¯åµŒå…¥çŸ©é˜µå’Œè¾“å‡ºçš„çº¿æ€§å˜æ¢çŸ©é˜µå…±äº«åŒä¸€ä¸ªæƒé‡çŸ©é˜µã€‚è¿™ç§æ–¹æ³•å¯ä»¥å‡å°‘æ¨¡å‹çš„å‚æ•°é‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­æ•ˆæœä¹Ÿå¾ˆå¥½ã€‚å…·ä½“æ¥è¯´ï¼ŒTransformer çš„ä½œè€…å°†è¾“å…¥çš„è¯åµŒå…¥çŸ©é˜µå’Œè¾“å‡ºçš„çº¿æ€§å˜æ¢çŸ©é˜µå…±äº«åŒä¸€ä¸ªæƒé‡çŸ©é˜µï¼Œè¿™æ ·å¯ä»¥å‡å°‘æ¨¡å‹çš„å‚æ•°é‡ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚</p>
</section>
<section id="full-model" class="level3 page-columns page-full" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="full-model"><span class="header-section-number">2.8</span> Full Model</h3>
<p>Transformerçš„å®Œæ•´æ¨¡å‹æ¶æ„å¦‚å›¾ <a href="#fig-transformer-architecture" class="quarto-xref">Figure&nbsp;1</a> æ‰€ç¤ºã€‚Transformerç”±å¤šä¸ªEncoder Blockå’ŒDecoder Blockç»„æˆï¼Œæ¯ä¸ªEncoder Blockå’ŒDecoder Blockéƒ½åŒ…å«äº†å‰é¢ä»‹ç»çš„æ¨¡å—ã€‚Encoder Blockå’ŒDecoder Blockçš„ç»“æ„æ˜¯ç›¸ä¼¼çš„ï¼Œéƒ½æ˜¯ç”±Multi-Head Attentionã€Point-Wise Feed Forward Networkã€Layer Normalizationå’ŒResidual Connectionç»„æˆçš„ã€‚</p>
<p>ä¸‹å›¾æ˜¯æ•´ä¸ªTransformerçš„ç¼–ç å’Œè§£ç è¿‡ç¨‹çš„ç¤ºæ„å›¾ï¼š</p>
<div class="page-columns page-full">
<div id="fig-full-transformer" class="quarto-layout-panel page-columns page-full">
<figure class="quarto-float quarto-float-fig figure page-columns page-full">
<div aria-describedby="fig-full-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca" class="page-columns page-full">
<div class="quarto-layout-row column-page">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-full-transformer" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-transformer-encoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-transformer-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/transformer-encoding.gif" class="img-fluid figure-img" data-ref-parent="fig-full-transformer">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-transformer-encoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Transformer Encoding Process
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-full-transformer" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-transformer-decoding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-transformer-decoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./01-attention.assets/transformer-decoding.gif" class="img-fluid figure-img" data-ref-parent="fig-full-transformer">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-transformer-decoding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Transformer Decoding Process
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-full-transformer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Illustrate of Transformer Encoding and Decoding Process (Image Source: <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>)
</figcaption>
</figure>
</div>
</div>
<p>å¦‚å›¾ <a href="#fig-full-transformer" class="quarto-xref">Figure&nbsp;7</a> æ‰€ç¤ºï¼ŒTransformerçš„ç¼–ç è¿‡ç¨‹å’Œè§£ç è¿‡ç¨‹æ˜¯ç›¸ä¼¼çš„ã€‚ç¼–ç è¿‡ç¨‹å°†è¾“å…¥çš„tokenè½¬æ¢ä¸ºå‘é‡ï¼Œç„¶åé€šè¿‡å¤šä¸ªEncoder Blockè¿›è¡Œç¼–ç ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å’ŒSoftmaxå‡½æ•°è½¬æ¢ä¸ºè¯æ±‡è¡¨ä¸­çš„æ¦‚ç‡åˆ†å¸ƒã€‚è§£ç è¿‡ç¨‹åˆ™æ˜¯å°†Encoderçš„è¾“å‡ºä¸Decoderçš„è¾“å…¥è¿›è¡Œäº¤å‰æ³¨æ„åŠ›è®¡ç®—ï¼Œç„¶åé€šè¿‡å¤šä¸ªDecoder Blockè¿›è¡Œè§£ç ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å’ŒSoftmaxå‡½æ•°è½¬æ¢ä¸ºè¯æ±‡è¡¨ä¸­çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p>
</section>
<section id="training-process" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="training-process"><span class="header-section-number">2.9</span> Training Process</h3>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¢è®¨ä¸€ä¸‹Transformerçš„è®­ç»ƒè¿‡ç¨‹ã€‚Transformerçš„è®­ç»ƒè¿‡ç¨‹ä¸å…¶ä»–ç¥ç»ç½‘ç»œæ¨¡å‹ç±»ä¼¼ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š</p>
<ol type="1">
<li><strong>æ•°æ®é¢„å¤„ç†</strong>ï¼šå°†è¾“å…¥çš„æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºtokenï¼Œå¹¶è¿›è¡Œåˆ†è¯å’Œç¼–ç ã€‚é€šå¸¸ä½¿ç”¨Word Embeddingå°†æ¯ä¸ªtokenè½¬æ¢ä¸ºä¸€ä¸ªå›ºå®šç»´åº¦çš„å‘é‡ã€‚</li>
<li><strong>æ¨¡å‹åˆå§‹åŒ–</strong>ï¼šåˆå§‹åŒ–Transformeræ¨¡å‹çš„å‚æ•°ï¼ŒåŒ…æ‹¬Word Embeddingã€Position Embeddingã€Multi-Head Attentionã€Point-Wise Feed Forward Networkç­‰æ¨¡å—çš„å‚æ•°ã€‚</li>
<li><strong>å‰å‘ä¼ æ’­</strong>ï¼šå°†è¾“å…¥çš„tokené€šè¿‡Word Embeddingå’ŒPosition Embeddingè½¬æ¢ä¸ºå‘é‡ï¼Œç„¶åé€šè¿‡å¤šä¸ªEncoder Blockè¿›è¡Œç¼–ç ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢å’ŒSoftmaxå‡½æ•°è½¬æ¢ä¸ºè¯æ±‡è¡¨ä¸­çš„æ¦‚ç‡åˆ†å¸ƒã€‚</li>
<li><strong>è®¡ç®—æŸå¤±</strong>ï¼šä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆCross-Entropy Lossï¼‰è®¡ç®—æ¨¡å‹çš„è¾“å‡ºä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚äº¤å‰ç†µæŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„åˆ†ç±»æŸå¤±å‡½æ•°ï¼Œå®ƒå¯ä»¥è¡¡é‡æ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚</li>
<li><strong>åå‘ä¼ æ’­</strong>ï¼šé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆå¦‚Adamä¼˜åŒ–å™¨ï¼‰æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚æ¢¯åº¦ä¸‹é™ç®—æ³•æ˜¯ä¸€ç§å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼Œå®ƒå¯ä»¥é€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹çš„å‚æ•°ï¼Œä»è€Œä½¿å¾—æ¨¡å‹çš„è¾“å‡ºæ›´æ¥è¿‘çœŸå®æ ‡ç­¾</li>
</ol>
<p>å…·ä½“çš„å®ç°è¿‡ç¨‹ï¼Œæˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„PyTorchå®ç°ä¸­è¯¦ç»†ä»‹ç»ã€‚åŒ…æ‹¬Label Smoothingã€Adamã€Maskingç­‰ç»†èŠ‚å¤„ç†ã€‚</p>
</section>
</section>
<section id="pytorch-å®ç°" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="pytorch-å®ç°"><span class="header-section-number">3</span> PyTorch å®ç°</h2>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ©ç”¨PyTorchæ¥å®ç°Transformerçš„æ¨¡å‹æ¶æ„ã€‚æˆ‘ä»¬é‡‡ç”¨Bottom-Upçš„æ–¹æ³•ï¼Œå…ˆå®ç°Word Embeddingï¼Œ æ˜¯Position Embeddingï¼Œç„¶åå®ç°æˆ‘ä»¬çš„é‡ç‚¹ï¼Œå³Multi-Head-Attentionï¼Œå†æ¬¡ä¹‹åï¼Œæˆ‘ä»¬ä¼šå®ç° Point-Wise Feed Forward Networkã€‚æœ€åå°†è¿™å‡ ä¸ªæ¨¡å—ç»„åˆèµ·æ¥ï¼Œå®ç°Transformerçš„Encoder å’Œ Decoderã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å§ï¼</p>
<section id="word-embedding" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="word-embedding"><span class="header-section-number">3.1</span> Word Embedding</h3>
<p>Word Embeddingæ˜¯å°†è¯è¯­è½¬æ¢ä¸ºå‘é‡çš„è¿‡ç¨‹ã€‚åœ¨PyTorch ä¸­çš„å®ç°éå¸¸ç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨<code>nn.Embedding</code>ç±»æ¥å®ç°ã€‚è¿™ä¸ªç±»ä¼šå°†æ¯ä¸ªtokenæ˜ å°„åˆ°ä¸€ä¸ª<em>å›ºå®šç»´åº¦</em>çš„å‘é‡ç©ºé—´ä¸­ã€‚</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">class</span> WordEmbedding(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig, is_tgt: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb1-3"><a href="#cb1-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a>        <span class="cf">if</span> is_tgt:</span>
<span id="cb1-6"><a href="#cb1-6"></a>            <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(config.tgt_vocab_size, config.d_model)</span>
<span id="cb1-7"><a href="#cb1-7"></a>        <span class="cf">else</span>:</span>
<span id="cb1-8"><a href="#cb1-8"></a>            <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(config.src_vocab_size, config.d_model)</span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-11"><a href="#cb1-11"></a>        <span class="co">"""</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">        x: (batch_size, seq_len)</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="co">        """</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>        <span class="cf">return</span> <span class="va">self</span>.embedding(x)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="position-embedding" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="position-embedding"><span class="header-section-number">3.2</span> Position Embedding</h3>
<p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥å®ç°Position Embedding <a href="#eq-position-encoding" class="quarto-xref">Equation&nbsp;4</a>ã€‚</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">class</span> PositionalEmbedding(nn.Module):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb2-3"><a href="#cb2-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-4"><a href="#cb2-4"></a></span>
<span id="cb2-5"><a href="#cb2-5"></a>        pos_index <span class="op">=</span> torch.arange(config.max_seq).unsqueeze(<span class="dv">1</span>)  <span class="co"># (max_seq, 1)</span></span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a>        div_term <span class="op">=</span> torch.exp(</span>
<span id="cb2-8"><a href="#cb2-8"></a>            torch.arange(<span class="dv">0</span>, config.d_model, <span class="dv">2</span>) <span class="op">*</span> <span class="op">-</span>(math.log(<span class="fl">10000.0</span>) <span class="op">/</span> config.d_model)</span>
<span id="cb2-9"><a href="#cb2-9"></a>        )</span>
<span id="cb2-10"><a href="#cb2-10"></a></span>
<span id="cb2-11"><a href="#cb2-11"></a>        pe <span class="op">=</span> torch.zeros(config.max_seq, config.d_model)  <span class="co"># (max_seq, d_model)</span></span>
<span id="cb2-12"><a href="#cb2-12"></a>        pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(pos_index <span class="op">*</span> div_term)</span>
<span id="cb2-13"><a href="#cb2-13"></a>        pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(pos_index <span class="op">*</span> div_term)</span>
<span id="cb2-14"><a href="#cb2-14"></a></span>
<span id="cb2-15"><a href="#cb2-15"></a>        pe <span class="op">=</span> pe.unsqueeze(<span class="dv">0</span>)  <span class="co"># (1, max_seq, d_model)</span></span>
<span id="cb2-16"><a href="#cb2-16"></a></span>
<span id="cb2-17"><a href="#cb2-17"></a>        pe.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-18"><a href="#cb2-18"></a>        <span class="va">self</span>.register_buffer(<span class="st">"pe"</span>, pe)</span>
<span id="cb2-19"><a href="#cb2-19"></a></span>
<span id="cb2-20"><a href="#cb2-20"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-21"><a href="#cb2-21"></a>        <span class="co">"""</span></span>
<span id="cb2-22"><a href="#cb2-22"></a><span class="co">        x: (batch_size, seq_len, d_model)</span></span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="co">        """</span></span>
<span id="cb2-24"><a href="#cb2-24"></a>        seq_len <span class="op">=</span> x.size(<span class="dv">1</span>)</span>
<span id="cb2-25"><a href="#cb2-25"></a>        <span class="cf">return</span> <span class="va">self</span>.pe[:, :seq_len, :]  <span class="co"># (1, seq_len, d_model)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>æœ‰äº†Word Embedding å’Œ Position Embeddingï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†è¾“å…¥çš„tokenè½¬æ¢ä¸ºå‘é‡äº†ã€‚æˆ‘ä»¬éœ€è¦æ¥ä¸‹æ¥éœ€è¦åšçš„å°±æ˜¯ï¼Œå°†è¿™ä¸¤ä¸ªå‘é‡ç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å…¥å‘é‡ã€‚</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">class</span> Embedding(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig, is_tgt: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb3-3"><a href="#cb3-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4"></a>        <span class="va">self</span>.word_embedding <span class="op">=</span> WordEmbedding(config, is_tgt)</span>
<span id="cb3-5"><a href="#cb3-5"></a>        <span class="va">self</span>.positional_embedding <span class="op">=</span> PositionalEmbedding(config)</span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-8"><a href="#cb3-8"></a>        <span class="co">"""</span></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">        x: (batch_size, seq_len)</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">        """</span></span>
<span id="cb3-11"><a href="#cb3-11"></a>        word_emb <span class="op">=</span> <span class="va">self</span>.word_embedding(x)</span>
<span id="cb3-12"><a href="#cb3-12"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.positional_embedding(word_emb)</span>
<span id="cb3-13"><a href="#cb3-13"></a>        <span class="cf">return</span> word_emb <span class="op">+</span> pos_emb  <span class="co"># (batch_size, seq_len, d_model)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="feed-forward-network" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="feed-forward-network"><span class="header-section-number">3.3</span> Feed Forward Network</h3>
<p>æˆ‘ä»¬å…ˆè·³è¿‡Multi Head Attentionï¼Œå…ˆå®ç°Feed Forward Networkã€‚Feed Forward Networkæ˜¯Transformerä¸­çš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒçš„ä½œç”¨æ˜¯å¯¹æ¯ä¸ªä½ç½®çš„å‘é‡è¿›è¡Œéçº¿æ€§å˜æ¢ã€‚å…·ä½“æ¥è¯´ï¼ŒFeed Forward Networkç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°ç»„æˆã€‚</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">class</span> FFN(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb4-3"><a href="#cb4-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.Linear(config.d_model, config.d_ff, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.Linear(config.d_ff, config.d_model, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-7"><a href="#cb4-7"></a></span>
<span id="cb4-8"><a href="#cb4-8"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-9"><a href="#cb4-9"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.ln1(x))  <span class="co"># Apply ReLU activation</span></span>
<span id="cb4-10"><a href="#cb4-10"></a>        x <span class="op">=</span> <span class="va">self</span>.ln2(x)  <span class="co"># Linear transformation</span></span>
<span id="cb4-11"><a href="#cb4-11"></a>        <span class="cf">return</span> x  <span class="co"># (batch_size, seq_len, d_model)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="layer-normalization" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="layer-normalization"><span class="header-section-number">3.4</span> Layer Normalization</h3>
<p>è¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ¨¡å—æ˜¯Layer Normalizationï¼Œå®ƒå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›ã€‚</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">class</span> LayerNormalization(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb5-3"><a href="#cb5-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a>        <span class="va">self</span>.eps <span class="op">=</span> config.eps</span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.ones(config.d_model))  <span class="co"># (d_model,)</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(config.d_model))  <span class="co"># (d_model,)</span></span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a>    <span class="kw">def</span> _compute_mean_std(<span class="va">self</span>, x):</span>
<span id="cb5-11"><a href="#cb5-11"></a>        <span class="co">"""</span></span>
<span id="cb5-12"><a href="#cb5-12"></a><span class="co">        Compute mean and standard deviation for the input tensor x</span></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="co">        On the last dimension (features)</span></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="co">        x: (batch_size, seq_len, d_model)</span></span>
<span id="cb5-15"><a href="#cb5-15"></a><span class="co">        Output:</span></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co">            mean: (batch_size, seq_len, 1)</span></span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="co">            std: (batch_size, seq_len, 1)</span></span>
<span id="cb5-18"><a href="#cb5-18"></a><span class="co">        """</span></span>
<span id="cb5-19"><a href="#cb5-19"></a>        mean <span class="op">=</span> x.mean(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-20"><a href="#cb5-20"></a>        std <span class="op">=</span> x.std(dim<span class="op">=-</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-21"><a href="#cb5-21"></a>        <span class="cf">return</span> mean, std</span>
<span id="cb5-22"><a href="#cb5-22"></a></span>
<span id="cb5-23"><a href="#cb5-23"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-24"><a href="#cb5-24"></a>        mean, std <span class="op">=</span> <span class="va">self</span>._compute_mean_std(x)</span>
<span id="cb5-25"><a href="#cb5-25"></a></span>
<span id="cb5-26"><a href="#cb5-26"></a>        <span class="co"># normalize x: (batch_size, seq_len, d_model)</span></span>
<span id="cb5-27"><a href="#cb5-27"></a>        normalized_x <span class="op">=</span> (x <span class="op">-</span> mean) <span class="op">/</span> (std <span class="op">+</span> <span class="va">self</span>.eps)  <span class="co"># Avoid division by zero</span></span>
<span id="cb5-28"><a href="#cb5-28"></a></span>
<span id="cb5-29"><a href="#cb5-29"></a>        <span class="cf">return</span> normalized_x <span class="op">*</span> <span class="va">self</span>.gamma <span class="op">+</span> <span class="va">self</span>.beta  <span class="co"># (batch_size, seq_len, d_model)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="multi-head-attention" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="multi-head-attention"><span class="header-section-number">3.5</span> Multi Head Attention</h3>
<p>Multi Head Attentionæ˜¯Transformerçš„æ ¸å¿ƒæ¨¡å—ã€‚å®ƒçš„ä½œç”¨æ˜¯å°†è¾“å…¥çš„å‘é‡è¿›è¡Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>è¿™éƒ¨åˆ†æ˜¯Transformerçš„æ ¸å¿ƒæ¨¡å—ï¼Œç†è§£å®ƒæ˜¯ç†è§£Transformerä»¥åŠä»–å˜å‹çš„å…³é”®ã€‚è®°å¾—å¤šçœ‹å‡ éï¼Œç›´åˆ°ä½ èƒ½ç†è§£ä¸ºæ­¢ã€‚</p>
</div>
</div>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">def</span> scaled_dot_product_attention(q, k, v, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    <span class="co">"""</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co">    Scaled Dot-Product Attention</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="co">    q: (batch_size, num_heads, seq_len_q, d_k)</span></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="co">    k: (batch_size, num_heads, seq_len_k, d_k)</span></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="co">    v: (batch_size, num_heads, seq_len_v, d_v)</span></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co">    mask: (batch_size, 1, seq_len_q, seq_len_k) or None</span></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="co">    """</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>    d_k <span class="op">=</span> k.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a>    scores <span class="op">=</span> einops.einsum(</span>
<span id="cb6-12"><a href="#cb6-12"></a>        q,</span>
<span id="cb6-13"><a href="#cb6-13"></a>        k,</span>
<span id="cb6-14"><a href="#cb6-14"></a>        <span class="st">"batch heads seq_len_q d_k, batch heads seq_len_k d_k -&gt; batch heads seq_len_q seq_len_k"</span>,</span>
<span id="cb6-15"><a href="#cb6-15"></a>    )</span>
<span id="cb6-16"><a href="#cb6-16"></a></span>
<span id="cb6-17"><a href="#cb6-17"></a>    scores <span class="op">=</span> scores <span class="op">/</span> math.sqrt(d_k)  <span class="co"># Scale the scores</span></span>
<span id="cb6-18"><a href="#cb6-18"></a>    scores <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># Apply softmax to get attention weights</span></span>
<span id="cb6-19"><a href="#cb6-19"></a></span>
<span id="cb6-20"><a href="#cb6-20"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb6-21"><a href="#cb6-21"></a>        scores <span class="op">=</span> scores.masked_fill(mask, <span class="bu">float</span>(<span class="st">"-inf"</span>))  <span class="co"># Apply mask if provided</span></span>
<span id="cb6-22"><a href="#cb6-22"></a></span>
<span id="cb6-23"><a href="#cb6-23"></a>    output <span class="op">=</span> einops.einsum(</span>
<span id="cb6-24"><a href="#cb6-24"></a>        scores,</span>
<span id="cb6-25"><a href="#cb6-25"></a>        v,</span>
<span id="cb6-26"><a href="#cb6-26"></a>        <span class="st">"batch heads seq_len_q seq_len_k, batch heads seq_len_k d_v -&gt; batch heads seq_len_q d_v"</span>,</span>
<span id="cb6-27"><a href="#cb6-27"></a>    )</span>
<span id="cb6-28"><a href="#cb6-28"></a></span>
<span id="cb6-29"><a href="#cb6-29"></a>    <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb7-3"><a href="#cb7-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>        <span class="cf">assert</span> (</span>
<span id="cb7-6"><a href="#cb7-6"></a>            config.d_model <span class="op">%</span> config.num_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>        ), <span class="st">"d_model must be divisible by num_heads"</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>        <span class="va">self</span>.d_k <span class="op">=</span> config.d_model <span class="op">//</span> config.num_heads  <span class="co"># Dimension of each head</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> config.num_heads</span>
<span id="cb7-10"><a href="#cb7-10"></a></span>
<span id="cb7-11"><a href="#cb7-11"></a>        <span class="va">self</span>.qkv_proj <span class="op">=</span> nn.Linear(</span>
<span id="cb7-12"><a href="#cb7-12"></a>            config.d_model, config.d_model <span class="op">*</span> <span class="dv">3</span>, bias<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>        )  <span class="co"># (d_model, d_model * 3)</span></span>
<span id="cb7-14"><a href="#cb7-14"></a></span>
<span id="cb7-15"><a href="#cb7-15"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(config.d_model, config.d_model, bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-16"><a href="#cb7-16"></a></span>
<span id="cb7-17"><a href="#cb7-17"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-18"><a href="#cb7-18"></a>        <span class="co">"""</span></span>
<span id="cb7-19"><a href="#cb7-19"></a><span class="co">        x: (batch_size, seq_len, d_model)</span></span>
<span id="cb7-20"><a href="#cb7-20"></a><span class="co">        mask: (batch_size, 1, seq_len_q, seq_len_k) or None</span></span>
<span id="cb7-21"><a href="#cb7-21"></a><span class="co">        """</span></span>
<span id="cb7-22"><a href="#cb7-22"></a>        batch_size, seq_len, _ <span class="op">=</span> x.size()</span>
<span id="cb7-23"><a href="#cb7-23"></a></span>
<span id="cb7-24"><a href="#cb7-24"></a>        q, k, v <span class="op">=</span> <span class="bu">map</span>(</span>
<span id="cb7-25"><a href="#cb7-25"></a>            <span class="kw">lambda</span> t: einops.rearrange(</span>
<span id="cb7-26"><a href="#cb7-26"></a>                t,</span>
<span id="cb7-27"><a href="#cb7-27"></a>                <span class="st">"batch seq_len (heads d_k) -&gt; batch heads seq_len d_k"</span>,</span>
<span id="cb7-28"><a href="#cb7-28"></a>                heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb7-29"><a href="#cb7-29"></a>            ),</span>
<span id="cb7-30"><a href="#cb7-30"></a>            <span class="va">self</span>.qkv_proj(x).chunk(<span class="dv">3</span>, dim<span class="op">=-</span><span class="dv">1</span>),</span>
<span id="cb7-31"><a href="#cb7-31"></a>        )  <span class="co"># (batch, num_heads, seq_len, d_k)</span></span>
<span id="cb7-32"><a href="#cb7-32"></a></span>
<span id="cb7-33"><a href="#cb7-33"></a>        <span class="co"># Compute attention</span></span>
<span id="cb7-34"><a href="#cb7-34"></a>        attn_output <span class="op">=</span> scaled_dot_product_attention(q, k, v, mask)</span>
<span id="cb7-35"><a href="#cb7-35"></a></span>
<span id="cb7-36"><a href="#cb7-36"></a>        <span class="co"># Rearrange back to (batch_size, seq_len, d_model)</span></span>
<span id="cb7-37"><a href="#cb7-37"></a>        attn_output <span class="op">=</span> einops.rearrange(</span>
<span id="cb7-38"><a href="#cb7-38"></a>            attn_output,</span>
<span id="cb7-39"><a href="#cb7-39"></a>            <span class="st">"batch heads seq_len d_v -&gt; batch seq_len (heads d_v)"</span>,</span>
<span id="cb7-40"><a href="#cb7-40"></a>            heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb7-41"><a href="#cb7-41"></a>        )</span>
<span id="cb7-42"><a href="#cb7-42"></a></span>
<span id="cb7-43"><a href="#cb7-43"></a>        output <span class="op">=</span> <span class="va">self</span>.out_proj(attn_output)  <span class="co"># (batch_size, seq_len, d_model)</span></span>
<span id="cb7-44"><a href="#cb7-44"></a>        <span class="cf">return</span> output  <span class="co"># (batch_size, seq_len, d_model)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="encoder-block" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="encoder-block"><span class="header-section-number">3.6</span> Encoder Block</h3>
<p>Encoder Blockæ˜¯Transformerçš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒç”±Multi Head Attentionå’ŒFeed Forward Networkç»„æˆã€‚å®ƒçš„ä½œç”¨æ˜¯å¯¹è¾“å…¥çš„å‘é‡è¿›è¡Œç¼–ç ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">class</span> EncoderBlock(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb8-3"><a href="#cb8-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a>        <span class="va">self</span>.attention <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb8-6"><a href="#cb8-6"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FeedForwardNetwork(config)</span>
<span id="cb8-7"><a href="#cb8-7"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNormalization(config.d_model)</span>
<span id="cb8-8"><a href="#cb8-8"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNormalization(config.d_model)</span>
<span id="cb8-9"><a href="#cb8-9"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb8-10"><a href="#cb8-10"></a></span>
<span id="cb8-11"><a href="#cb8-11"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb8-12"><a href="#cb8-12"></a>        <span class="co"># Multi Head Attention</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>        attn_output <span class="op">=</span> <span class="va">self</span>.attention(x)</span>
<span id="cb8-14"><a href="#cb8-14"></a>        x <span class="op">=</span> <span class="va">self</span>.norm1(x <span class="op">+</span> <span class="va">self</span>.dropout(attn_output))   </span>
<span id="cb8-15"><a href="#cb8-15"></a>        <span class="co"># Feed Forward Network</span></span>
<span id="cb8-16"><a href="#cb8-16"></a>        ffn_output <span class="op">=</span> <span class="va">self</span>.ffn(x)</span>
<span id="cb8-17"><a href="#cb8-17"></a>        x <span class="op">=</span> <span class="va">self</span>.norm2(x <span class="op">+</span> <span class="va">self</span>.dropout(ffn_output))</span>
<span id="cb8-18"><a href="#cb8-18"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="decoder-block" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="decoder-block"><span class="header-section-number">3.7</span> Decoder Block</h3>
<p>Decoder Blockæ˜¯Transformerçš„å¦ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒä¸Encoder Blockç±»ä¼¼ï¼Œä½†å®ƒè¿˜éœ€è¦å¤„ç† Masked Multi Head Attentionã€‚Masked Multi Head Attentionçš„ä½œç”¨æ˜¯é˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒæ—¶çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ï¼Œä»è€Œä¿è¯æ¨¡å‹çš„è‡ªå›å½’ç‰¹æ€§ã€‚</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">class</span> DecoderBlock(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb9-3"><a href="#cb9-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()      </span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a>        <span class="va">self</span>.attention1 <span class="op">=</span> MultiHeadAttention(config)</span>
<span id="cb9-6"><a href="#cb9-6"></a>        <span class="va">self</span>.attention2 <span class="op">=</span> MultiHeadAttention(config, is_causal<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>æ­å–œä½ ï¼Œä»¥åŠæˆåŠŸçš„å®ç°äº†Transformerï¼Œè¿™ä¸ªæ˜¯å½“å‰æœ€é‡è¦çš„AIæ¨¡å‹æ¡†æ¶ã€‚ç†è§£äº†å®ƒï¼Œä½ å°±ç†è§£å¯ä»¥ç†è§£å¤§éƒ¨åˆ†çš„AIæ¨¡å‹äº†ã€‚ç°åœ¨å¤§ç«çš„ChatGPTï¼ŒDeepSeekç­‰æ¨¡å‹éƒ½æ˜¯åŸºäºTransformerçš„å˜å‹ï¼ˆåœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¼šé˜…è¯»åˆ°è¿™äº›æ¨¡å‹ï¼‰ã€‚å®Œæ•´çš„ä»£ç å¯ä»¥åœ¨<a href="">GitHub</a>ä¸ŠæŸ¥çœ‹ã€‚</p>
</section>
</section>
<section id="æ‰©å±•" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="æ‰©å±•"><span class="header-section-number">4</span> æ‰©å±•</h2>
<p>è‡ªä»Transformerè¢«æå‡ºä»¥æ¥ï¼Œå·²ç»æœ‰äº†å¾ˆå¤šçš„å˜å‹å’Œæ”¹è¿›ã€‚å…·ä½“çš„æ¥è¯´ï¼ŒAttentionåœ¨Transformerä¸­éœ€è¦ <span class="math inline">\(\mathcal{O}(n^2)\)</span> çš„è®¡ç®—å¤æ‚åº¦ï¼Œè¿™åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ä¼šå˜å¾—éå¸¸æ…¢ã€‚å› æ­¤ï¼Œå¾ˆå¤šç ”ç©¶è€…æå‡ºäº†å„ç§å„æ ·çš„æ”¹è¿›æ–¹æ³•æ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„æ”¹è¿›æ–¹æ³•ï¼š</p>
<ul>
<li><strong>Sparse Attention</strong>: é€šè¿‡ç¨€ç–åŒ–æ³¨æ„åŠ›çŸ©é˜µæ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚æ¯”å¦‚ï¼ŒReformeræ¨¡å‹ä½¿ç”¨äº†å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰æ¥å®ç°ç¨€ç–æ³¨æ„åŠ›ã€‚</li>
<li><strong>Linear Attention</strong>: é€šè¿‡å°†æ³¨æ„åŠ›è®¡ç®—è½¬æ¢ä¸ºçº¿æ€§æ—¶é—´å¤æ‚åº¦</li>
</ul>
</section>
<section id="qa" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="qa"><span class="header-section-number">5</span> Q&amp;A</h2>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
é—®é¢˜ï¼šä¸ºä»€ä¹ˆè¦ç”¨ $\sqrt{d_k}$ ç¼©æ”¾ç‚¹ç§¯ï¼Ÿ
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>å›ç­”ï¼šå¦‚æœä¸ç¼©æ”¾ï¼Œå½“ <span class="math inline">\(d_k\)</span> å¾ˆå¤§æ—¶ï¼ŒQK çš„æ–¹å·®ä¹Ÿä¼šå˜å¤§ï¼Œä½¿ softmax è½å…¥æ¢¯åº¦éå¸¸å°çš„åŒºåŸŸã€‚é™¤ä»¥ <span class="math inline">\(\sqrt{d_k}\)</span> æœ‰åŠ©äºå°†æ¿€æ´»å€¼ä¿æŒåœ¨é€‚åˆè®­ç»ƒçš„èŒƒå›´å†…ã€‚</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
é—®é¢˜ï¼šå¤šå¤´æ³¨æ„åŠ›è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>å›ç­”ï¼šå®ƒè®©æ¨¡å‹å¯ä»¥åŒæ—¶å…³æ³¨ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´å’Œä½ç½®çš„ä¿¡æ¯ï¼Œä»è€Œå…‹æœå•å¤´è‡ªæ³¨æ„åŠ›å®¹æ˜“â€œå¹³å‡åŒ–â€çš„é—®é¢˜ã€‚</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
é—®é¢˜ï¼šTransformer å¦‚ä½•å®ç°è‡ªå›å½’å¼è§£ç ï¼Ÿ
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>å›ç­”ï¼šè§£ç å™¨é€šè¿‡å°†æœªæ¥ä½ç½®çš„æ³¨æ„åŠ›å¾—åˆ†è®¾ä¸º <span class="math inline">\(-\infty\)</span> æ¥å±è”½å®ƒä»¬ï¼Œç¡®ä¿æ¯ä¸ªä½ç½®åªèƒ½å…³æ³¨åˆ°å‰é¢çš„è¾“å‡ºã€‚</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
é—®é¢˜ï¼šä¸ºä»€ä¹ˆä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç è€Œä¸æ˜¯å¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Ÿ
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>å›ç­”ï¼šæ­£å¼¦ä½ç½®ç¼–ç å…è®¸æ¨¡å‹æ¨å¹¿åˆ°æ›´é•¿çš„åºåˆ—ï¼Œå‚æ•°é‡å°‘ï¼Œå¹¶ä¸”åœ¨ BLEU åˆ†æ•°ä¸Šä¸å¯å­¦ä¹ çš„ä½ç½®ç¼–ç æ•ˆæœç›¸å½“ã€‚</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-QA no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
é—®é¢˜ï¼šä¸ RNN/CNN ç›¸æ¯”ï¼Œè·¯å¾„é•¿åº¦å’Œè®¡ç®—å¤æ‚åº¦æœ‰ä½•ä¸åŒï¼Ÿ
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>å›ç­”ï¼šè‡ªæ³¨æ„åŠ›çš„è·¯å¾„é•¿åº¦æ˜¯å¸¸æ•°ï¼Œæ¯å±‚çš„è®¡ç®—å¤æ‚åº¦æ˜¯ <span class="math inline">\(\mathcal{O}(nÂ²d)\)</span>ï¼›è€Œ RNN éœ€è¦ <span class="math inline">\(\mathcal{O}(n)\)</span> çš„ä¸²è¡Œæ­¥éª¤ï¼ŒCNN éœ€è¦å †å å¤šå±‚æ‰èƒ½è¦†ç›–é•¿è·ç¦»ä¾èµ–ã€‚</p>
</div>
</div>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-DeepSeekV2StrongEconomical2024deepseek-ai" class="csl-entry" role="listitem">
DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, et al. 2024. <span>â€œ<span>DeepSeek-V2</span>: <span>A Strong</span>, <span>Economical</span>, and <span class="nocase">Efficient Mixture-of-Experts Language Model</span>.â€</span> June 19, 2024. <a href="https://doi.org/10.48550/arXiv.2405.04434">https://doi.org/10.48550/arXiv.2405.04434</a>.
</div>
<div id="ref-UsingOutputEmbedding2017press" class="csl-entry" role="listitem">
Press, Ofir, and Lior Wolf. 2017. <span>â€œUsing the <span>Output Embedding</span> to <span>Improve Language Models</span>.â€</span> February 21, 2017. <a href="https://doi.org/10.48550/arXiv.1608.05859">https://doi.org/10.48550/arXiv.1608.05859</a>.
</div>
<div id="ref-LinformerSelfAttentionLinear2020wang" class="csl-entry" role="listitem">
Wang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. <span>â€œLinformer: <span>Self-Attention</span> with <span>Linear Complexity</span>.â€</span> June 14, 2020. <a href="https://doi.org/10.48550/arXiv.2006.04768">https://doi.org/10.48550/arXiv.2006.04768</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../00-how-to-read-paper.html" class="pagination-link" aria-label="00 Preparation for following">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">00 Preparation for following</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../posts/02-vision-transformer.html" class="pagination-link" aria-label="02: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale">
        <span class="nav-page-text">02: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© CC-By Yuyang, 2025</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>This page is built with â¤ï¸ and <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>