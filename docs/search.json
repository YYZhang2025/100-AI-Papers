[
  {
    "objectID": "notebooks/01-attention.html",
    "href": "notebooks/01-attention.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport math\nimport random\n\n\ndef green(text):\n    return f\"\\033[1;32m{text}\\033[0m\"\n\n\ndef red(text):\n    return f\"\\033[1;31m{text}\\033[0m\"\n\n\ndef yellow(text):\n    return f\"\\033[1;33m{text}\\033[0m\"\n\n\ndef bold(text):\n    return f\"\\033[1m{text}\\033[0m\"\n\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"Using device: {green(device)}\")\n\n\nUsing device: mps\n\n\n\n\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass TransformerConfig:\n    d_model: int = 512\n    d_ff: int = 2048\n\n    num_heads: int = 8\n\n    dropout: float = 0.1\n\n    vocab_size: int = 10000\n    max_seq_len: int = 512\n\n\nclass WordEmbedding(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n\n    def forward(self, x):\n        return self.embedding(x)\n\n\nPosition Encoding\n\nclass PositionEncoding(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.dropout = nn.Dropout(config.dropout)\n\n        # Create the positional encoding matrix\n        pe = torch.zeros(config.max_seq_len, config.d_model)\n        position = torch.arange(0, config.max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, config.d_model, 2).float()\n            * (-math.log(10000.0) / config.d_model)\n        )\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[: x.size(0), :]\n        return self.dropout(x)\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.weight * (x - mean) / (std + self.eps) + self.bias\n\n\nclass FFN(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\n\ndef scaled_dot_production_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    mask: torch.Tensor | None = None,\n):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n\n    attn_weights = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, value)\n\n    return output, attn_weights\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.num_heads = config.num_heads\n        self.d_model = config.d_model\n        self.d_k = config.d_model // config.num_heads\n\n        self.query = nn.Linear(config.d_model, config.d_model)\n        self.key = nn.Linear(config.d_model, config.d_model)\n        self.value = nn.Linear(config.d_model, config.d_model)\n\n        self.fc_out = nn.Linear(config.d_model, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        query = (\n            self.query(query)\n            .view(batch_size, -1, self.num_heads, self.d_k)\n            .transpose(1, 2)\n        )\n        key = (\n            self.key(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        )\n        value = (\n            self.value(value)\n            .view(batch_size, -1, self.num_heads, self.d_k)\n            .transpose(1, 2)\n        )\n\n        attn_output, attn_weights = scaled_dot_production_attention(\n            query, key, value, mask\n        )\n\n        attn_output = (\n            attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        )\n        output = self.fc_out(attn_output)\n\n        return output, attn_weights\n\n\nclass CausalAttention(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        mask = (\n            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n\n        query = query.view(batch_size, -1, 1, self.mask.size(-1))\n        key = key.view(batch_size, -1, 1, self.mask.size(-1))\n        value = value.view(batch_size, -1, 1, self.mask.size(-1))\n\n        attn_output, attn_weights = scaled_dot_production_attention(\n            query, key, value, self.mask\n        )\n\n        attn_output = attn_output.view(batch_size, -1, self.mask.size(-1))\n\n        return attn_output, attn_weights\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FFN(config)\n\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n\n        self.dropout1 = nn.Dropout(config.dropout)\n        self.dropout2 = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        attn_output, _ = self.attention(x, x, x)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n\n        ffn_output = self.ffn(x)\n        x = x + self.dropout2(ffn_output)\n        x = self.norm2(x)\n\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.attention1 = CausalAttention(config)\n        self.attention2 = MultiHeadAttention(config)\n        self.ffn = FFN(config)\n\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n        self.norm3 = LayerNorm(config.d_model)\n\n        self.dropout1 = nn.Dropout(config.dropout)\n        self.dropout2 = nn.Dropout(config.dropout)\n        self.dropout3 = nn.Dropout(config.dropout)\n\n    def forward(self, x, encoder_output):\n        attn_output, _ = self.attention1(x, x, x)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n\n        attn_output, _ = self.attention2(x, encoder_output, encoder_output)\n        x = x + self.dropout2(attn_output)\n        x = self.norm2(x)\n\n        ffn_output = self.ffn(x)\n        x = x + self.dropout3(ffn_output)\n        x = self.norm3(x)\n\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = WordEmbedding(config)\n        self.position_encoding = PositionEncoding(config)\n\n        self.layers = nn.ModuleList(\n            [EncoderBlock(config) for _ in range(config.num_heads)]\n        )\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        return x\n\n\nclass Decoder(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = WordEmbedding(config)\n        self.position_encoding = PositionEncoding(config)\n\n        self.layers = nn.ModuleList(\n            [DecoderBlock(config) for _ in range(config.num_heads)]\n        )\n\n    def forward(self, x, encoder_output):\n        x = self.embedding(x)\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x = layer(x, encoder_output)\n\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.encoder = Encoder(config)\n        self.decoder = Decoder(config)\n\n        self.fc_out = nn.Linear(config.d_model, config.vocab_size)\n\n    def tie_weights(self):\n        \"\"\"Tie the weights of the embedding and output layers.\"\"\"\n        self.fc_out.weight = self.encoder.embedding.embedding.weight\n\n    def forward(self, src, tgt):\n        encoder_output = self.encoder(src)\n        decoder_output = self.decoder(tgt, encoder_output)\n        output = self.fc_out(decoder_output)\n\n        return output"
  },
  {
    "objectID": "posts/02-bert.html",
    "href": "posts/02-bert.html",
    "title": "02: BERT",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see EquationÂ 1\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\n\nprint(\"Hello,m World!\")\n\\[\n\\mathcal{H} = \\mathcal{W} \\cdot \\mathcal{O}\n\\tag{1}\\]\nThis is the figure see, figure ?@fig-polar $$",
    "crumbs": [
      "100 Papers",
      "02: BERT"
    ]
  },
  {
    "objectID": "posts/01-attention.html",
    "href": "posts/01-attention.html",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "ç»™å®šä¸¤ä¸ªå‘é‡ \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), å®ƒä»¬çš„ç‚¹ç§¯ä¸º: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\næµ‹é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦, æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼\nåœ¨ Self-Attention ä¸­ï¼Œç”¨æ¥è¡¡é‡â€œå½“å‰è¯å¯¹å…¶ä»–è¯çš„å…³æ³¨ç¨‹åº¦â€\n\n\n\n\nç»™å®šå‘é‡ \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)ï¼ŒSoftmax å‡½æ•°å®šä¹‰ä¸º: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\nå°†æ³¨æ„åŠ›â€œå¾—åˆ†ï¼ˆdot productï¼‰â€è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ\næ¯ä¸ªè¯å¯¹å…¶ä»–è¯çš„æ³¨æ„åŠ›æƒé‡ \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#preliminary",
    "href": "posts/01-attention.html#preliminary",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "ç»™å®šä¸¤ä¸ªå‘é‡ \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), å®ƒä»¬çš„ç‚¹ç§¯ä¸º: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\næµ‹é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦, æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼\nåœ¨ Self-Attention ä¸­ï¼Œç”¨æ¥è¡¡é‡â€œå½“å‰è¯å¯¹å…¶ä»–è¯çš„å…³æ³¨ç¨‹åº¦â€\n\n\n\n\nç»™å®šå‘é‡ \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)ï¼ŒSoftmax å‡½æ•°å®šä¹‰ä¸º: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\nå°†æ³¨æ„åŠ›â€œå¾—åˆ†ï¼ˆdot productï¼‰â€è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ\næ¯ä¸ªè¯å¯¹å…¶ä»–è¯çš„æ³¨æ„åŠ›æƒé‡ \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#attention-is-all-you-need",
    "href": "posts/01-attention.html#attention-is-all-you-need",
    "title": "01: Attention is All You Need",
    "section": "Attention is All You Need",
    "text": "Attention is All You Need\n\n\n\n\n\n\nFigureÂ 1: The Transformer Family Version 2.0 | Lilâ€™Log\n\n\n\nå¦‚å›¾ FigureÂ 1 æ‰€ç¤ºï¼Œæ˜¯Transformerçš„æ•´ä½“æ¶æ„ã€‚ä»\n\nä½ç½®ç¼–ç \nTransformerçš„è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œåºåˆ—ä¸­çš„æ¯ä¸ªè¯éƒ½è¢«è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡ã€‚ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿç†è§£è¯è¯­åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ ä½ç½®ç¼–ç ï¼ˆPosition Encodingï¼‰ã€‚ä½ç½®ç¼–ç å¯ä»¥æ˜¯å›ºå®šçš„ï¼Œä¹Ÿå¯ä»¥æ˜¯å¯å­¦ä¹ çš„ã€‚ åœ¨Transformerä¸­ï¼Œä½ç½®ç¼–ç æ˜¯é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°æ¥å®ç°çš„ï¼šå…·ä½“æ¥è¯´ï¼Œå¯¹äºåºåˆ—ä¸­çš„ç¬¬ \\(i\\) ä¸ªä½ç½®ï¼Œä½ç½®ç¼–ç çš„ç¬¬ \\(j\\) ç»´å¯ä»¥è¡¨ç¤ºä¸ºï¼š \\[\n\\text{PE}(i, j) =\n\\begin{cases}\n\\sin\\left(\\frac{i}{10000^{j/d}}\\right) & \\text{if } j \\text{ is even} \\\\\n\\cos\\left(\\frac{i}{10000^{(j-1)/d}}\\right) & \\text{if } j \\     \\text{ is odd}\n\\end{cases}\n\\tag{3}\\] å…¶ä¸­ \\(d\\) æ˜¯ä½ç½®ç¼–ç çš„ç»´åº¦ã€‚ä½ç½®ç¼–ç çš„ä½œç”¨æ˜¯ä¸ºæ¯ä¸ªè¯æä¾›ä¸€ä¸ªå”¯ä¸€çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°è¯è¯­åœ¨åºåˆ—ä¸­çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#pytorch-å®ç°",
    "href": "posts/01-attention.html#pytorch-å®ç°",
    "title": "01: Attention is All You Need",
    "section": "PyTorch å®ç°",
    "text": "PyTorch å®ç°\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å«åˆ©ç”¨PyTorchæ¥å®ç°Transformerçš„æ¨¡å‹æ¶æ„ã€‚æˆ‘ä»¬é‡‡ç”¨Bottom-Upçš„æ–¹æ³•ï¼Œå…ˆå®ç°Word Embeddingï¼Œ æ¥ç€æ˜¯Position Embeddingï¼Œæ¥ç€å®ç°æˆ‘ä»¬çš„é‡ç‚¹ï¼Œå³Multi-Head-Attentionï¼Œå†æ¬¡ä¹‹åï¼Œæˆ‘ä»¬ä¼šå®ç° Point-Wise Feed Forward Networkã€‚æœ€åå°†è¿™å‡ ä¸ªæ¨¡å—ç»„åˆèµ·æ¥ï¼Œå®ç°Transformerçš„Encoder å’Œ Decoderã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\nWord Embedding\nWord Embeddingæ˜¯å°†è¯è¯­è½¬æ¢ä¸ºå‘é‡çš„è¿‡ç¨‹ã€‚åœ¨PyTorch ä¸­çš„å®ç°éå¸¸ç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨nn.Embeddingç±»æ¥å®ç°ã€‚è¿™ä¸ªç±»ä¼šå°†æ¯ä¸ªtokenæ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šç»´åº¦çš„å‘é‡ç©ºé—´ä¸­ã€‚\nclass WordEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n    \n    def _init_weight(self):\n        nn.init.trunc_normal_(self.embedding.weight, mean = 0, std=0.02, a=-3, b=3)\n    \n    def forward(self, x):\n        return self.embedding(x)    \n\n\nPosition Embedding\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥å®ç°Position Embedding EquationÂ 3ã€‚\nclass PositionEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.seq_len = config.seq_len\n        self.d_model = config.d_model\næœ‰äº†Word Embedding å’Œ Position Embeddingï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†è¾“å…¥çš„tokenè½¬æ¢ä¸ºå‘é‡äº†ã€‚æˆ‘ä»¬éœ€è¦æ¥ä¸‹æ¥éœ€è¦åšçš„å°±æ˜¯ï¼Œå°†è¿™ä¸¤ä¸ªå‘é‡ç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å…¥å‘é‡ã€‚\nclass InputEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.word_embedding = WordEmbedding(config)\n        self.position_embedding = PositionEmbedding(config)\n    \n    def forward(self, x):\n        word_emb = self.word_embedding(x)\n        pos_emb = self.position_embedding(x)\n        \n        return word_emb + pos_emb\n\n\nFeed Forward Network\næˆ‘ä»¬å…ˆè·³è¿‡Multi Head Attentionï¼Œå…ˆå®ç°Feed Forward Networkã€‚Feed Forward Networkæ˜¯Transformerä¸­çš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒçš„ä½œç”¨æ˜¯å¯¹æ¯ä¸ªä½ç½®çš„å‘é‡è¿›è¡Œéçº¿æ€§å˜æ¢ã€‚å…·ä½“æ¥è¯´ï¼ŒFeed Forward Networkç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°ç»„æˆã€‚\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n\n\nLayer Normalization\nè¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ¨¡å—æ˜¯Layer Normalizationï¼Œå®ƒå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›ã€‚\nclass LayerNormalization(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.weight * normalized_x + self.bias\n\n\nMulti Head Attention\nMulti Head Attentionæ˜¯Transformerçš„æ ¸å¿ƒæ¨¡å—ã€‚å®ƒçš„ä½œç”¨æ˜¯å°†è¾“å…¥çš„å‘é‡è¿›è¡Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚\n\n\n\n\n\n\nImportant\n\n\n\nè¿™éƒ¨åˆ†æ˜¯Transformerçš„æ ¸å¿ƒæ¨¡å—ï¼Œç†è§£å®ƒæ˜¯ç†è§£Transformerä»¥åŠä»–å˜å‹çš„å…³é”®ã€‚è®°å¾—å¤šçœ‹å‡ éï¼Œç›´åˆ°ä½ èƒ½ç†è§£ä¸ºæ­¢ã€‚\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        assert config.d_model % config.num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.head_dim = config.d_model // config.num_heads\n\n        self.qkv_proj = nn.Linear(config.d_model, config.d_model * 3)\n        self.out_proj = nn.Linear(config.d_model, config.d_model)\n\n\nEncoder Block\nEncoder Blockæ˜¯Transformerçš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒç”±Multi Head Attentionå’ŒFeed Forward Networkç»„æˆã€‚å®ƒçš„ä½œç”¨æ˜¯å¯¹è¾“å…¥çš„å‘é‡è¿›è¡Œç¼–ç ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚\nclass EncoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FeedForwardNetwork(config)\n        self.norm1 = LayerNormalization(config.d_model)\n        self.norm2 = LayerNormalization(config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        # Multi Head Attention\n        attn_output = self.attention(x)\n        x = self.norm1(x + self.dropout(attn_output))   \n        # Feed Forward Network\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\n\nDecoder Block\nDecoder Blockæ˜¯Transformerçš„å¦ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒä¸Encoder Blockç±»ä¼¼ï¼Œä½†å®ƒè¿˜éœ€è¦å¤„ç† Masked Multi Head Attentionã€‚Masked Multi Head Attentionçš„ä½œç”¨æ˜¯é˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒæ—¶çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ï¼Œä»è€Œä¿è¯æ¨¡å‹çš„è‡ªå›å½’ç‰¹æ€§ã€‚\nclass DecoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()      \n\n        self.attention1 = MultiHeadAttention(config)\n        self.attention2 = MultiHeadAttention(config, is_causal=True)\næ­å–œä½ ï¼Œä»¥åŠæˆåŠŸçš„å®ç°äº†Transformerï¼Œè¿™ä¸ªæ˜¯å½“å‰æœ€é‡è¦çš„AIæ¨¡å‹æ¡†æ¶ã€‚ç†è§£äº†å®ƒï¼Œä½ å°±ç†è§£å¯ä»¥ç†è§£å¤§éƒ¨åˆ†çš„AIæ¨¡å‹äº†ã€‚ç°åœ¨å¤§ç«çš„ChatGPTï¼ŒDeepSeekç­‰æ¨¡å‹éƒ½æ˜¯åŸºäºTransformerçš„å˜å‹ï¼ˆåœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¼šé˜…è¯»åˆ°è¿™äº›æ¨¡å‹ï¼‰ã€‚å®Œæ•´çš„ä»£ç å¯ä»¥åœ¨GitHubä¸ŠæŸ¥çœ‹ã€‚",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#æ‰©å±•",
    "href": "posts/01-attention.html#æ‰©å±•",
    "title": "01: Attention is All You Need",
    "section": "æ‰©å±•",
    "text": "æ‰©å±•\nè‡ªä»Transformerè¢«æå‡ºä»¥æ¥ï¼Œå·²ç»æœ‰äº†å¾ˆå¤šçš„å˜å‹å’Œæ”¹è¿›ã€‚å…·ä½“çš„æ¥è¯´ï¼ŒAttentionåœ¨Transformerä¸­éœ€è¦ \\(\\mathcal{O}(n^2)\\) çš„è®¡ç®—å¤æ‚åº¦ï¼Œè¿™åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ä¼šå˜å¾—éå¸¸æ…¢ã€‚å› æ­¤ï¼Œå¾ˆå¤šç ”ç©¶è€…æå‡ºäº†å„ç§å„æ ·çš„æ”¹è¿›æ–¹æ³•æ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„æ”¹è¿›æ–¹æ³•ï¼š\n\nSparse Attention: é€šè¿‡ç¨€ç–åŒ–æ³¨æ„åŠ›çŸ©é˜µæ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚æ¯”å¦‚ï¼ŒReformeræ¨¡å‹ä½¿ç”¨äº†å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰æ¥å®ç°ç¨€ç–æ³¨æ„åŠ›ã€‚\nLinear Attention: é€šè¿‡å°†æ³¨æ„åŠ›è®¡ç®—è½¬æ¢ä¸ºçº¿æ€§æ—¶é—´å¤æ‚åº¦",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nPaper Link\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need\nTransformer-based model for sequence modeling\nğŸ”—\n\nNLP / Transformer\n\n\n02\nBERT\nBidirectional encoder for pre-training NLP tasks\nğŸ”—\n\nNLP / Pretraining\n\n\n03\nGPT-3\nGenerative transformer for autoregressive tasks\nğŸ”—\n\nNLP / Generation\n\n\n04\nCLIP\nContrastive Language-Image Pretraining\nğŸ”—\n\nVision / Multimodal\n\n\n05\nViT\nVision Transformer for image classification\nğŸ”—\n\nVision / Transformer",
    "crumbs": [
      "About"
    ]
  }
]