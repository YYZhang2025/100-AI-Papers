[
  {
    "objectID": "notebooks/01-attention.html",
    "href": "notebooks/01-attention.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport math\nimport random\n\n\ndef green(text):\n    return f\"\\033[1;32m{text}\\033[0m\"\n\n\ndef red(text):\n    return f\"\\033[1;31m{text}\\033[0m\"\n\n\ndef yellow(text):\n    return f\"\\033[1;33m{text}\\033[0m\"\n\n\ndef bold(text):\n    return f\"\\033[1m{text}\\033[0m\"\n\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"Using device: {green(device)}\")\n\n\nUsing device: mps\n\n\n\n\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass TransformerConfig:\n    d_model: int = 512\n    d_ff: int = 2048\n\n    num_heads: int = 8\n\n    dropout: float = 0.1\n\n    vocab_size: int = 10000\n    max_seq_len: int = 512\n\n\nclass WordEmbedding(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n\n    def forward(self, x):\n        return self.embedding(x)\n\n\nPosition Encoding\n\nclass PositionEncoding(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.dropout = nn.Dropout(config.dropout)\n\n        # Create the positional encoding matrix\n        pe = torch.zeros(config.max_seq_len, config.d_model)\n        position = torch.arange(0, config.max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, config.d_model, 2).float()\n            * (-math.log(10000.0) / config.d_model)\n        )\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[: x.size(0), :]\n        return self.dropout(x)\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.weight * (x - mean) / (std + self.eps) + self.bias\n\n\nclass FFN(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\n\ndef scaled_dot_production_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    mask: torch.Tensor | None = None,\n):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n\n    attn_weights = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, value)\n\n    return output, attn_weights\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.num_heads = config.num_heads\n        self.d_model = config.d_model\n        self.d_k = config.d_model // config.num_heads\n\n        self.query = nn.Linear(config.d_model, config.d_model)\n        self.key = nn.Linear(config.d_model, config.d_model)\n        self.value = nn.Linear(config.d_model, config.d_model)\n\n        self.fc_out = nn.Linear(config.d_model, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        query = (\n            self.query(query)\n            .view(batch_size, -1, self.num_heads, self.d_k)\n            .transpose(1, 2)\n        )\n        key = (\n            self.key(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        )\n        value = (\n            self.value(value)\n            .view(batch_size, -1, self.num_heads, self.d_k)\n            .transpose(1, 2)\n        )\n\n        attn_output, attn_weights = scaled_dot_production_attention(\n            query, key, value, mask\n        )\n\n        attn_output = (\n            attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        )\n        output = self.fc_out(attn_output)\n\n        return output, attn_weights\n\n\nclass CausalAttention(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        mask = (\n            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n\n        query = query.view(batch_size, -1, 1, self.mask.size(-1))\n        key = key.view(batch_size, -1, 1, self.mask.size(-1))\n        value = value.view(batch_size, -1, 1, self.mask.size(-1))\n\n        attn_output, attn_weights = scaled_dot_production_attention(\n            query, key, value, self.mask\n        )\n\n        attn_output = attn_output.view(batch_size, -1, self.mask.size(-1))\n\n        return attn_output, attn_weights\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FFN(config)\n\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n\n        self.dropout1 = nn.Dropout(config.dropout)\n        self.dropout2 = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        attn_output, _ = self.attention(x, x, x)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n\n        ffn_output = self.ffn(x)\n        x = x + self.dropout2(ffn_output)\n        x = self.norm2(x)\n\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.attention1 = CausalAttention(config)\n        self.attention2 = MultiHeadAttention(config)\n        self.ffn = FFN(config)\n\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n        self.norm3 = LayerNorm(config.d_model)\n\n        self.dropout1 = nn.Dropout(config.dropout)\n        self.dropout2 = nn.Dropout(config.dropout)\n        self.dropout3 = nn.Dropout(config.dropout)\n\n    def forward(self, x, encoder_output):\n        attn_output, _ = self.attention1(x, x, x)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n\n        attn_output, _ = self.attention2(x, encoder_output, encoder_output)\n        x = x + self.dropout2(attn_output)\n        x = self.norm2(x)\n\n        ffn_output = self.ffn(x)\n        x = x + self.dropout3(ffn_output)\n        x = self.norm3(x)\n\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = WordEmbedding(config)\n        self.position_encoding = PositionEncoding(config)\n\n        self.layers = nn.ModuleList(\n            [EncoderBlock(config) for _ in range(config.num_heads)]\n        )\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        return x\n\n\nclass Decoder(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = WordEmbedding(config)\n        self.position_encoding = PositionEncoding(config)\n\n        self.layers = nn.ModuleList(\n            [DecoderBlock(config) for _ in range(config.num_heads)]\n        )\n\n    def forward(self, x, encoder_output):\n        x = self.embedding(x)\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x = layer(x, encoder_output)\n\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.encoder = Encoder(config)\n        self.decoder = Decoder(config)\n\n        self.fc_out = nn.Linear(config.d_model, config.vocab_size)\n\n    def tie_weights(self):\n        \"\"\"Tie the weights of the embedding and output layers.\"\"\"\n        self.fc_out.weight = self.encoder.embedding.embedding.weight\n\n    def forward(self, src, tgt):\n        encoder_output = self.encoder(src)\n        decoder_output = self.decoder(tgt, encoder_output)\n        output = self.fc_out(decoder_output)\n\n        return output"
  },
  {
    "objectID": "posts/02-bert.html",
    "href": "posts/02-bert.html",
    "title": "02: BERT",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Equation 1\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\n\nprint(\"Hello,m World!\")\n\\[\n\\mathcal{H} = \\mathcal{W} \\cdot \\mathcal{O}\n\\tag{1}\\]\nThis is the figure see, figure ?@fig-polar $$",
    "crumbs": [
      "100 Papers",
      "02: BERT"
    ]
  },
  {
    "objectID": "posts/01-attention.html",
    "href": "posts/01-attention.html",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "给定两个向量 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), 它们的点积为: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\n测量两个向量的相似度, 数值越大，说明两个向量越相似\n在 Self-Attention 中，用来衡量“当前词对其他词的关注程度”\n\n\n\n\n给定向量 \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)，Softmax 函数定义为: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\n将注意力“得分（dot product）”转换为概率分布\n每个词对其他词的注意力权重 \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#preliminary",
    "href": "posts/01-attention.html#preliminary",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "给定两个向量 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), 它们的点积为: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\n测量两个向量的相似度, 数值越大，说明两个向量越相似\n在 Self-Attention 中，用来衡量“当前词对其他词的关注程度”\n\n\n\n\n给定向量 \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)，Softmax 函数定义为: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\n将注意力“得分（dot product）”转换为概率分布\n每个词对其他词的注意力权重 \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#attention-is-all-you-need",
    "href": "posts/01-attention.html#attention-is-all-you-need",
    "title": "01: Attention is All You Need",
    "section": "Attention is All You Need",
    "text": "Attention is All You Need\n\n\n\n\n\n\nFigure 1: The Transformer Family Version 2.0 | Lil’Log\n\n\n\n如图 Figure 1 所示，是Transformer的整体架构。从\n\n位置编码\nTransformer的输入是一个序列，序列中的每个词都被转换为一个向量。为了让模型能够理解词语在序列中的位置，我们需要添加位置编码（Position Encoding）。位置编码可以是固定的，也可以是可学习的。 在Transformer中，位置编码是通过正弦和余弦函数来实现的：具体来说，对于序列中的第 \\(i\\) 个位置，位置编码的第 \\(j\\) 维可以表示为： \\[\n\\text{PE}(i, j) =\n\\begin{cases}\n\\sin\\left(\\frac{i}{10000^{j/d}}\\right) & \\text{if } j \\text{ is even} \\\\\n\\cos\\left(\\frac{i}{10000^{(j-1)/d}}\\right) & \\text{if } j \\     \\text{ is odd}\n\\end{cases}\n\\tag{3}\\] 其中 \\(d\\) 是位置编码的维度。位置编码的作用是为每个词提供一个唯一的位置信息，使得模型能够捕捉到词语在序列中的相对位置关系。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#pytorch-实现",
    "href": "posts/01-attention.html#pytorch-实现",
    "title": "01: Attention is All You Need",
    "section": "PyTorch 实现",
    "text": "PyTorch 实现\n接下来，我们叫利用PyTorch来实现Transformer的模型架构。我们采用Bottom-Up的方法，先实现Word Embedding， 接着是Position Embedding，接着实现我们的重点，即Multi-Head-Attention，再次之后，我们会实现 Point-Wise Feed Forward Network。最后将这几个模块组合起来，实现Transformer的Encoder 和 Decoder。准备好了吗？让我们开始吧！\n\nWord Embedding\nWord Embedding是将词语转换为向量的过程。在PyTorch 中的实现非常简单，我们可以使用nn.Embedding类来实现。这个类会将每个token映射到一个固定维度的向量空间中。\nclass WordEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n    \n    def _init_weight(self):\n        nn.init.trunc_normal_(self.embedding.weight, mean = 0, std=0.02, a=-3, b=3)\n    \n    def forward(self, x):\n        return self.embedding(x)    \n\n\nPosition Embedding\n接下来，我们来实现Position Embedding Equation 3。\nclass PositionEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.seq_len = config.seq_len\n        self.d_model = config.d_model\n有了Word Embedding 和 Position Embedding，我们就可以将输入的token转换为向量了。我们需要接下来需要做的就是，将这两个向量相加，得到最终的输入向量。\nclass InputEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.word_embedding = WordEmbedding(config)\n        self.position_embedding = PositionEmbedding(config)\n    \n    def forward(self, x):\n        word_emb = self.word_embedding(x)\n        pos_emb = self.position_embedding(x)\n        \n        return word_emb + pos_emb\n\n\nFeed Forward Network\n我们先跳过Multi Head Attention，先实现Feed Forward Network。Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Feed Forward Network由两个线性变换和一个ReLU激活函数组成。\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n\n\nLayer Normalization\n还有一个重要的模块是Layer Normalization，它可以帮助模型更快地收敛。\nclass LayerNormalization(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.weight * normalized_x + self.bias\n\n\nMulti Head Attention\nMulti Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。\n\n\n\n\n\n\nImportant\n\n\n\n这部分是Transformer的核心模块，理解它是理解Transformer以及他变型的关键。记得多看几遍，直到你能理解为止。\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        assert config.d_model % config.num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.head_dim = config.d_model // config.num_heads\n\n        self.qkv_proj = nn.Linear(config.d_model, config.d_model * 3)\n        self.out_proj = nn.Linear(config.d_model, config.d_model)\n\n\nEncoder Block\nEncoder Block是Transformer的一个重要模块，它由Multi Head Attention和Feed Forward Network组成。它的作用是对输入的向量进行编码，从而捕捉到不同的语义信息。\nclass EncoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FeedForwardNetwork(config)\n        self.norm1 = LayerNormalization(config.d_model)\n        self.norm2 = LayerNormalization(config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        # Multi Head Attention\n        attn_output = self.attention(x)\n        x = self.norm1(x + self.dropout(attn_output))   \n        # Feed Forward Network\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\n\nDecoder Block\nDecoder Block是Transformer的另一个重要模块，它与Encoder Block类似，但它还需要处理 Masked Multi Head Attention。Masked Multi Head Attention的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。\nclass DecoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()      \n\n        self.attention1 = MultiHeadAttention(config)\n        self.attention2 = MultiHeadAttention(config, is_causal=True)\n恭喜你，以及成功的实现了Transformer，这个是当前最重要的AI模型框架。理解了它，你就理解可以理解大部分的AI模型了。现在大火的ChatGPT，DeepSeek等模型都是基于Transformer的变型（在接下来的文章中，我们会阅读到这些模型）。完整的代码可以在GitHub上查看。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#扩展",
    "href": "posts/01-attention.html#扩展",
    "title": "01: Attention is All You Need",
    "section": "扩展",
    "text": "扩展\n自从Transformer被提出以来，已经有了很多的变型和改进。具体的来说，Attention在Transformer中需要 \\(\\mathcal{O}(n^2)\\) 的计算复杂度，这在处理长文本时会变得非常慢。因此，很多研究者提出了各种各样的改进方法来降低计算复杂度。以下是一些常见的改进方法：\n\nSparse Attention: 通过稀疏化注意力矩阵来降低计算复杂度。比如，Reformer模型使用了局部敏感哈希（LSH）来实现稀疏注意力。\nLinear Attention: 通过将注意力计算转换为线性时间复杂度",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nPaper Link\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need\nTransformer-based model for sequence modeling\n🔗\n\nNLP / Transformer\n\n\n02\nBERT\nBidirectional encoder for pre-training NLP tasks\n🔗\n\nNLP / Pretraining\n\n\n03\nGPT-3\nGenerative transformer for autoregressive tasks\n🔗\n\nNLP / Generation\n\n\n04\nCLIP\nContrastive Language-Image Pretraining\n🔗\n\nVision / Multimodal\n\n\n05\nViT\nVision Transformer for image classification\n🔗\n\nVision / Transformer",
    "crumbs": [
      "About"
    ]
  }
]