[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "这个网站旨在收集和展示 100 篇重要的 AI 论文及其代码实现。每篇论文都附有链接，方便读者深入了解。",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#关于这个网站",
    "href": "index.html#关于这个网站",
    "title": "100 Papers with Code",
    "section": "",
    "text": "这个网站旨在收集和展示 100 篇重要的 AI 论文及其代码实现。每篇论文都附有链接，方便读者深入了解。",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#为什么要搭建这个网站",
    "href": "index.html#为什么要搭建这个网站",
    "title": "100 Papers with Code",
    "section": "为什么要搭建这个网站？",
    "text": "为什么要搭建这个网站？\n在本人学习，阅读和实践 AI 领域的过程中，发现很多重要的论文和代码实现都散落在各个地方。为了方便自己和其他人查阅，我决定搭建这个网站，将这些重要的论文和以及自己的实现，集中在一起。",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#如何使用这个网站",
    "href": "index.html#如何使用这个网站",
    "title": "100 Papers with Code",
    "section": "如何使用这个网站？",
    "text": "如何使用这个网站？\n每个论文都有一个独立的页面，包含论文的基本信息、链接和代码实现。你可以通过点击论文名称来查看详细内容。一下是以及实现，或者是打算记录的论文列表：\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need\nIntroduced the Transformer architecture, which relies entirely on self-attention mechanisms for sequence modeling, enabling parallel computation and improving performance on NLP tasks.\n     \nNLP / Transformer",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/01-transformer.html",
    "href": "posts/01-transformer.html",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "在理解Transformer之前，我们需要先了解一些基本的概念。有基础的同学可以跳过这一节，直接看下一节的Transformer模型架构。\n\n\n给定两个向量 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), 它们的点积为: \\[\n\\text{score}(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\] 这个score用于测量两个向量的相似度, 数值越大，说明两个向量越相似\n\n\n\n给定向量 \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)，softmax 函数定义为: \\[\n\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\] softmax 函数将向量 \\(\\mathbf{x}\\) 转换为一个概率分布，所有元素的和为1。它常用于将模型的输出转换为概率分布。其中，计算softmax 的时间复杂度为 \\(\\mathcal{O}(n)\\)，其中 \\(n\\) 是向量 \\(\\mathbf{x}\\) 的维度。\n\n\n\n给定矩阵 \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) 和 \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times p}\\)，它们的矩阵乘法定义为: \\[\n\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B} \\in \\mathbb{R}^{m \\times p}\n\\tag{3}\\] 矩阵乘法是线性代数中的基本运算，用于将两个矩阵相乘，得到一个新的矩阵。它在神经网络中被广泛使用，尤其是在计算注意力分数时。其中，Matrix Multiplication的时间复杂度为 \\(\\mathcal{O}(mnp)\\)，其中 \\(m\\) 是 \\(\\mathbf{A}\\) 的行数，\\(n\\) 是 \\(\\mathbf{A}\\) 的列数，也是 \\(\\mathbf{B}\\) 的行数，\\(p\\) 是 \\(\\mathbf{B}\\) 的列数。 Wikipedia 上有一个关于 Matrix Multiplication Time Complexity 的详细介绍。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-transformer.html#preliminary",
    "href": "posts/01-transformer.html#preliminary",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "在理解Transformer之前，我们需要先了解一些基本的概念。有基础的同学可以跳过这一节，直接看下一节的Transformer模型架构。\n\n\n给定两个向量 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), 它们的点积为: \\[\n\\text{score}(\\mathbf{q}, \\mathbf{k}) = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\] 这个score用于测量两个向量的相似度, 数值越大，说明两个向量越相似\n\n\n\n给定向量 \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)，softmax 函数定义为: \\[\n\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\] softmax 函数将向量 \\(\\mathbf{x}\\) 转换为一个概率分布，所有元素的和为1。它常用于将模型的输出转换为概率分布。其中，计算softmax 的时间复杂度为 \\(\\mathcal{O}(n)\\)，其中 \\(n\\) 是向量 \\(\\mathbf{x}\\) 的维度。\n\n\n\n给定矩阵 \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) 和 \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times p}\\)，它们的矩阵乘法定义为: \\[\n\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B} \\in \\mathbb{R}^{m \\times p}\n\\tag{3}\\] 矩阵乘法是线性代数中的基本运算，用于将两个矩阵相乘，得到一个新的矩阵。它在神经网络中被广泛使用，尤其是在计算注意力分数时。其中，Matrix Multiplication的时间复杂度为 \\(\\mathcal{O}(mnp)\\)，其中 \\(m\\) 是 \\(\\mathbf{A}\\) 的行数，\\(n\\) 是 \\(\\mathbf{A}\\) 的列数，也是 \\(\\mathbf{B}\\) 的行数，\\(p\\) 是 \\(\\mathbf{B}\\) 的列数。 Wikipedia 上有一个关于 Matrix Multiplication Time Complexity 的详细介绍。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-transformer.html#attention-is-all-you-need",
    "href": "posts/01-transformer.html#attention-is-all-you-need",
    "title": "01: Attention is All You Need",
    "section": "2 Attention is All You Need",
    "text": "2 Attention is All You Need\n在这个篇章中，我们将深入阅读这篇论文，首先，我们先来了解一下这篇论文的背景和主要贡献\n\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks … We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n\n本篇论文提出：序列建模（Sequence Modeling）长期依赖 RNN/CNN，其计算受制于时间步骤的串行性，难以并行化，且长距离依赖捕获效率低。为了解决这个问题，作者提出了Transformer模型，完全基于注意力机制，去除了循环和卷积操作，从而实现了全局信息交互。利用多头注意力机制（Multi-Head Attention）+ 前馈网络 （Feed Forward Network）的编码器-解码器，去除了循环/卷积，仅靠注意力实现全局信息交互。 除此之外，他们还设计了Scaled Dot-Product Attention 和 Multi Head Attention，来降低数值尺度的同时并行学习多子空间表示。\n以下是Transformer的整体架构图：\n\n\n\n\n\n\nFigure 1: The Transformer Family Version 2.0 | Lil’Log\n\n\n\n如图 Figure 1 所示，Transformer是由：\n\nInput Embedding: Section 2.1\nPosition Embedding: Section 2.2\nMulti-Head Attention: Section 2.3\nLayer Normalization: Section 2.4\nPoint-Wise Feed Forward Network: Section 2.6\n\n这几个小模块来组成的，每个小模块组成一个Encoder Block，多个Encoder Block堆叠起来形成Encoder，Decoder Block也是类似的。接下来，我们会逐个介绍这些模块。\n\n2.1 Input Embedding\nInput Embedding是将输入的token转换为向量的过程。Transformer的输入是一个序列，序列中的每个词都被转换为一个向量。这个向量可以是预训练的词向量，也可以是随机初始化的向量。Transformer使用Word Embedding来将每个token转换为一个固定维度的向量。这在自然语言处理（NLP）中是一个常见的做法。\n\n\n\n2.2 Positional Encoding\nTransformer 的一个重要特点是它不使用循环神经网络（RNN）或卷积神经网络（CNN）来处理序列数据。这就导致了一个问题：Transformer无法捕捉到序列中词语的位置信息。为了解决这个问题，Transformer引入了位置编码（Position Encoding）。\n\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.\n\n位置编码是一个向量，它为每个词提供了一个唯一的位置信息。位置编码的维度与词向量的维度相同。Transformer使用位置编码来为每个词提供一个唯一的位置信息，从而使得模型能够捕捉到词语在序列中的相对位置关系。\n在Transformer中，位置编码是通过正弦和余弦函数来实现的：具体来说，对于序列中的第 \\(pos\\) 个位置，位置编码的第 \\(i\\) 维可以表示为： \\[\n\\begin{align}\n\\mathrm{PE}(pos,2i)=\\sin\\!\\bigl(pos \\times \\tfrac{1}{10000^{2i/d_\\mathrm{model}}}\\bigr) \\\\\n\\mathrm{PE}(pos,2i{+}1)=\\cos\\!\\bigl(pos \\times \\tfrac{1}{10000^{2i/d_\\mathrm{model}}}\\bigr)\n\\end{align}\n\\tag{4}\\]\n其中 \\(d_{\\text{model}}\\) 是位置编码的维度（与Word Embedding 的维度一样大）。位置编码的作用是为每个词提供一个唯一的位置信息，使得模型能够捕捉到词语在序列中的相对位置关系。\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Position Encoding with max sequence length 100\n\n\n\n\n\n\n\n\n\n\n\n(b) Position Encoding with max sequence length 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Position Encoding with max sequence length from 100 to 200\n\n\n\n\n\n\n\n\n\n\n\n(d) Position Encoding with log base from 10,000 to 1,000\n\n\n\n\n\n\n\nFigure 2: Illustration of Position Encoding with different max sequence lengths, the horizontal red line represent the encoding at position 50. The encoding is consistent across different max sequence lengths, which allows the model to generalize to longer sequences.\n\n\n\n\n从 Figure 2 中可以看到，随着max sequence length的增加，位置编码的变化是连续的 Figure 2 (c) 。并且在不同的max sequence length下，位置编码的变化是相同的， 图 Figure 2 (a), Figure 2 (b)， 显示的是位置50在不同的max sequence length下的位置编码变化。可以看到，位置编码在不同的max sequence length下是相同的，这使得模型可以更好地泛化到更长的序列上。\n\n\n\n\n\n\nFigure 3: Details of Position Encoding\n\n\n\n图 Figure 3 展示了位置编码不同dimension之间的细节。相对于diminution(4, 5), dimension(6,7) 的随着位置变化更大。从图中我们可以看出：\n\n低 \\(i\\)（靠前的维度）—— 波长短，随位置 pos 变化得快 → 易区别相邻 token；\n高 \\(i\\)（靠后的维度）—— 波长长，随位置 pos 变化得慢 → 捕获全局位置信息。\n\n除此之外，位置编码还可以通过改变正弦和余弦函数的基数来实现不同的效果。比如，可以将基数从10000改为1000，这样可以使得位置编码的波长变短，从而使得模型更容易捕捉到相邻位置的信息。图 Figure 2 (d) 展示了位置编码在不同的基数下的变化。\n\n\n\n2.3 Multi-Head Attention\nMulti-Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。Attention 其实就是一个加权求和的过程，它可以看作是对输入向量的加权平均。其核心公式为： \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\tag{5}\\]\n其中 \\(Q\\)、\\(K\\)、\\(V\\) 分别是查询（Query）、键（Key）和值（Value）矩阵，其中 \\(\\sqrt{d_k}\\) 是一个缩放因子，用来防止点积的值过大导致梯度消失。这个公式的含义是：首先计算查询和键的点积 (Section 1.1) ，然后通过Softmax函数 (Section 1.2) 将其转换为概率分布，最后用这个概率分布对值进行加权求和。\n\n\n\n\n\n\nNote\n\n\n\nEquation 5 是Attention的核心公式，而Attention是Transformer的核心模块。理解这个公式是理解Transformer的关键。后续很多的创新比如，Linear Attention (Wang et al. 2020)， Multi-head Latent Attention(DeepSeek-AI et al. 2024) 都是在这个公式的基础上进行改进。\n\n\n\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to \\(d_k\\), \\(d_k\\) and \\(d_v\\) dimensions, respectively.\n\nMulti-Head Attention 是Attention Equation 5 的一个扩展，它将输入的向量分成多个子空间（head），然后在每个子空间上独立地计算注意力。最后，将所有子空间的输出拼接起来，得到最终的输出。Multi-Head Attention 的公式为： \\[\n\\begin{align*}\n\\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O \\\\\n\\text{where}\\ \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n\\end{align*}    \n\\tag{6}\\]\n其中 \\(W_i^Q, W_i^K \\in \\mathbb{R}^{d_\\text{model} \\times d_k}, W_i^V \\in \\mathbb{R}^{d_\\text{model} \\times d_v}\\) 是用于将输入向量投影到子空间的权重矩阵，\\(W^O \\in \\mathbb{R}^{hd_v \\times d_\\text{model} }\\) 是用于将所有子空间的输出拼接起来的权重矩阵。Multi-Head Attention 的作用是通过多个子空间来捕捉不同的语义信息，从而提高模型的表达能力。\n\nDue to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n\n文章中提到，尽管增加了Heads会增加计算量，但由于每个Head的维度较小，并且可以并行计算，从而提高了模型的效率。\n\n2.3.1 Time Complexity of Multi-Head Attention\n在继续了解别的component之前，我们先来分析一下Multi-Head Attention的时间复杂度。假设input的长度为 \\(n\\)，每个head的维度为 \\(d_k\\)，那么计算 \\(QK^T\\) 的时间复杂度为 \\(\\mathcal{O}(n^2 d_k)\\) (Equation 3)。\n接下来是Softmax的计算，Softmax的时间复杂度为 \\(\\mathcal{O}(n)\\)(Section 1.2)。对于每个score matrix \\(QK^T \\in \\mathbb{R}^{n \\times n}\\) 的每一行，我们需要计算 softmax，这需要 \\(n\\) 次计算。因此，Softmax的时间复杂度为 \\(\\mathcal{O}(n^2)\\)。\n之后是对于value的加权，计算复杂度也是 \\(\\mathcal{O}(n^2d)\\). 所以总的时间复杂度为：\\(\\mathcal{O}(n^2d)\\), 随着 \\(n\\) 的增加，计算复杂度会Polynomial 增长。具体的计算复杂度如下表所示： \\[\n\\begin{array}{|l|l|}\n\\hline\n\\textbf{Step} & \\textbf{Time Complexity} \\\\\n\\hline\nQK^\\top & \\mathcal{O}(n^2 d) \\\\\n\\text{softmax}(QK^\\top) & \\mathcal{O}(n^2) \\\\\n\\text{attention} \\times V & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\textbf{Total} & \\mathcal{O}(n^2 d) \\\\\n\\hline\n\\end{array}\n\\tag{7}\\]\n\n\n2.3.2 Causal Attention\nCausal Attention（因果注意力）是Transformer中的一个重要概念，它的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。具体来说，Causal Attention会在计算注意力时，将未来的信息屏蔽掉，从而使得模型只能看到当前和过去的信息。这样可以保证模型在训练时不会看到未来的信息，从而保证模型的自回归特性。 Causal Attention通常作用在Decoder 模块中。\n\n\n\n\n\n\nFigure 4: Example of Causal Attention\n\n\n\nCausal Attention的公式为： \\[\n\\text{CausalAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V\n\\tag{8}\\]\n在这个公式中，\\(M\\) 是一个掩码矩阵（mask matrix），它的作用是将未来的信息屏蔽掉。\\(M\\) 是一个上三角矩阵，它的对角线以下的元素为0，对角线以上的元素为\\(-\\infty\\)。这样在计算Softmax时，对角线以上的元素会被屏蔽掉，从而保证模型只能看到当前和过去的信息。\n\n\n2.3.3 Cross Attention\nCross Attention（交叉注意力）作用是将Encoder的输出与Decoder的输入进行交叉注意力计算，从而使得Decoder能够利用Encoder的输出信息。具体来说，Cross Attention会将Encoder的输出作为键（Key）和值（Value），将Decoder的输入作为查询（Query），然后计算注意力。 Cross Attention的公式与Attention Equation 5 类似，只不过将Encoder的输出作为键和值，Decoder的输入作为查询\n\n\n\n\n\n\nNote\n\n\n\nCross Attention 除了可以应用在text中，可以应用在不同的modality之间，比如图像和文本之间的交叉注意力。比如在视觉问答（Visual Question Answering）任务中，Cross Attention可以将图像特征与问题文本进行交叉注意力计算，从而使得模型能够利用图像信息来回答问题。\n\n\n\n\n\n2.4 Layer Normalization\nLayer Normalization是Transformer中的一个重要模块，它可以帮助模型更快地收敛。Layer Normalization的作用是对每个位置的向量进行归一化处理，从而使得模型在训练时更加稳定。具体来说，Layer Normalization会对每个位置的向量(feature)进行均值和方差的归一化处理，使得每个位置的向量都具有相同的分布。这样可以使得模型在训练时更加稳定，从而加快模型的收敛速度。\n\n\n\n\n\n\nFigure 5: Illustration of Layer Normalization\n\n\n\nLayer Normalization的公式为： \\[\n\\text{LayerNorm}(\\mathrm{x}) = \\frac{\\mathrm{x} - \\mu}{\\sigma + \\epsilon} \\cdot \\gamma + \\beta\n\\tag{9}\\] 其中 \\(\\mu\\) 是向量 \\(\\mathrm{x}\\) 的均值，\\(\\sigma\\) 是向量 \\(\\mathrm{x}\\) 的标准差。\n对于一个batch的输入，Layer Normalization会对每个位置的向量进行归一化处理，而不是对整个batch进行归一化处理。这使得Layer Normalization可以更好地适应不同长度的序列，从而提高模型的性能。具体来说, \\(x \\in \\mathbb{R}^{B \\times H \\times S \\times d_v}\\)我们对每个位置的向量也就是 \\(d_v\\) 维度进行归一化处理，而不是对整个batch进行归一化处理。\n\n\n\n2.5 Residual Connection\nResidual Connection（残差连接）作用是将输入的向量与子层的输出相加，从而使得模型可以更容易地学习到Identity Mapping。Residual Connection的如图所示：\n\n\n\n\n\n\nFigure 6: Illustration of Residual Connection\n\n\n\nResidual Connection在Transformer的应用公式为： \\[\n\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x} + \\mathrm{Sublayer}(\\mathbf{x}))\n\\tag{10}\\]\n在训练时，残差连接可以为梯度提供一条“捷径”，即梯度可以不经过子层复杂的非线性变换，直接传回前面层的输入。这能有效缓解梯度消失问题。\n\\[\n\\begin{align}\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}}\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} \\\\\n&= \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot \\left( \\mathbf{I} + \\frac{\\partial \\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}} \\right) \\\\\n&= \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}}}_{\\text{straight path}} +\n\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{y}} \\cdot\n\\frac{\\partial\\,\\mathrm{Sublayer}(\\mathbf{x})}{\\partial \\mathbf{x}}}_{\\text{through the sub-layer}}\n\\end{align}\n\\tag{11}\\] 其中， \\(\\mathcal{L}\\) 是损失函数。我们可以看到由于存在第一项，哪怕子层梯度接近 0，Gradient 的信息也不会完全丢失。\n\n\n\n\n\n\nPre-Normalization vs Post-Normalization\n\n\n\nTransformer论文中, Normalization的位置是放在Residual Connection的后面（Post-Normalization）Equation 10 。但在后续的研究中，很多模型（比如BERT）将Normalization放在Residual Connection的前面（Pre-Normalization）。 \\[\n\\text{Output} = \\mathrm{Sublayer}(\\mathbf{x}) + \\mathrm{LayerNorm}(\\mathbf{x})\n\\tag{12}\\]\n\n\n\n\n2.6 Point-Wise Feed Forward Network\n\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n\nPoint-Wise Feed Forward Network作用是对每个位置的向量进行非线性变换, 其公式为： \\[\n\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n\\tag{13}\\]\n其中 \\(W_1\\) 和 \\(W_2\\) 是线性变换的权重矩阵，\\(b_1\\) 和 \\(b_2\\) 是偏置项。Point-Wise Feed Forward Network的作用是对每个位置的向量进行非线性变换，从而使得模型可以更好地捕捉到输入序列中的不同语义信息。\n\n\n2.7 Output Linear Projection & Softmax\n在Transformer的Decoder中，最后一步是将Decoder的输出通过一个线性变换和Softmax函数转换为词汇表中的概率分布。这个过程可以看作是将Decoder的输出映射到词汇表中的每个词的概率。具体来说， \\[\\text{Output} = \\text{Softmax}(xW + b)\n\\tag{14}\\] 其中 \\(W \\in \\mathbb{R} ^ {d_\\text{model} \\times vocab}\\) 是线性变换的权重矩阵，\\(b\\) 是偏置项。Softmax函数将线性变换的输出转换为概率分布，从而使得模型可以生成下一个词的概率分布。\n在这个过程中，Transformer 的作者采用了一种 weight tying(Press and Wolf 2017) 的方法，将输入的词嵌入矩阵和输出的线性变换矩阵共享同一个权重矩阵。这种方法可以减少模型的参数量，从而提高模型的效率，并且在实践中效果也很好。具体来说，Transformer 的作者将输入的词嵌入矩阵和输出的线性变换矩阵共享同一个权重矩阵，这样可以减少模型的参数量，从而提高模型的效率。\n\n\n2.8 Full Model\nTransformer的完整模型架构如图 Figure 1 所示。Transformer由多个Encoder Block和Decoder Block组成，每个Encoder Block和Decoder Block都包含了前面介绍的模块。Encoder Block和Decoder Block的结构是相似的，都是由Multi-Head Attention、Point-Wise Feed Forward Network、Layer Normalization和Residual Connection组成的。\n下图是整个Transformer的编码和解码过程的示意图：\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Transformer Encoding Process\n\n\n\n\n\n\n\n\n\n\n\n(b) Transformer Decoding Process\n\n\n\n\n\n\n\nFigure 7: Illustrate of Transformer Encoding and Decoding Process (Image Source: The Illustrated Transformer)\n\n\n\n\n如图 Figure 7 所示，Transformer的编码过程和解码过程是相似的。编码过程将输入的token转换为向量，然后通过多个Encoder Block进行编码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。解码过程则是将Encoder的输出与Decoder的输入进行交叉注意力计算，然后通过多个Decoder Block进行解码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。\n\n\n2.9 Training Process\n接下来，我们探讨一下Transformer的训练过程。Transformer的训练过程与其他神经网络模型类似，主要包括以下几个步骤：\n\n数据预处理：将输入的文本数据转换为token，并进行分词和编码。通常使用Word Embedding将每个token转换为一个固定维度的向量。\n模型初始化：初始化Transformer模型的参数，包括Word Embedding、Position Embedding、Multi-Head Attention、Point-Wise Feed Forward Network等模块的参数。\n前向传播：将输入的token通过Word Embedding和Position Embedding转换为向量，然后通过多个Encoder Block进行编码，最后通过一个线性变换和Softmax函数转换为词汇表中的概率分布。\n计算损失：使用交叉熵损失函数（Cross-Entropy Loss）计算模型的输出与真实标签之间的差异。交叉熵损失函数是一个常用的分类损失函数，它可以衡量模型的输出概率分布与真实标签之间的差异。\n反向传播：通过计算损失函数对模型参数的梯度，使用梯度下降算法（如Adam优化器）更新模型的参数。梯度下降算法是一种常用的优化算法，它可以通过计算损失函数对模型参数的梯度来更新模型的参数，从而使得模型的输出更接近真实标签\n\n具体的实现过程，我们将在接下来的PyTorch实现中详细介绍。包括Label Smoothing、Adam、Masking等细节处理。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-transformer.html#pytorch-实现",
    "href": "posts/01-transformer.html#pytorch-实现",
    "title": "01: Attention is All You Need",
    "section": "3 PyTorch 实现",
    "text": "3 PyTorch 实现\n接下来，我们利用PyTorch来实现Transformer的模型架构。我们采用Bottom-Up的方法，先实现Word Embedding， 是Position Embedding，然后实现我们的重点，即Multi-Head-Attention，再次之后，我们会实现 Point-Wise Feed Forward Network。最后将这几个模块组合起来，实现Transformer的Encoder 和 Decoder。准备好了吗？让我们开始吧！\n\n3.1 Word Embedding\nWord Embedding是将词语转换为向量的过程。在PyTorch 中的实现非常简单，我们可以使用nn.Embedding类来实现。这个类会将每个token映射到一个固定维度的向量空间中。\nclass WordEmbedding(nn.Module):\n    def __init__(self, config: ModelConfig, is_tgt: bool = False):\n        super().__init__()\n\n        if is_tgt:\n            self.embedding = nn.Embedding(config.tgt_vocab_size, config.d_model)\n        else:\n            self.embedding = nn.Embedding(config.src_vocab_size, config.d_model)\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, seq_len)\n        \"\"\"\n        return self.embedding(x)    \n\n\n3.2 Position Embedding\n接下来，我们来实现Position Embedding Equation 4。\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        pos_index = torch.arange(config.max_seq).unsqueeze(1)  # (max_seq, 1)\n\n        div_term = torch.exp(\n            torch.arange(0, config.d_model, 2) * -(math.log(10000.0) / config.d_model)\n        )\n\n        pe = torch.zeros(config.max_seq, config.d_model)  # (max_seq, d_model)\n        pe[:, 0::2] = torch.sin(pos_index * div_term)\n        pe[:, 1::2] = torch.cos(pos_index * div_term)\n\n        pe = pe.unsqueeze(0)  # (1, max_seq, d_model)\n\n        pe.requires_grad = False\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, seq_len, d_model)\n        \"\"\"\n        seq_len = x.size(1)\n        return self.pe[:, :seq_len, :]  # (1, seq_len, d_model)\n有了Word Embedding 和 Position Embedding，我们就可以将输入的token转换为向量了。我们需要接下来需要做的就是，将这两个向量相加，得到最终的输入向量。\nclass Embedding(nn.Module):\n    def __init__(self, config: ModelConfig, is_tgt: bool = False):\n        super().__init__()\n        self.word_embedding = WordEmbedding(config, is_tgt)\n        self.positional_embedding = PositionalEmbedding(config)\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, seq_len)\n        \"\"\"\n        word_emb = self.word_embedding(x)\n        pos_emb = self.positional_embedding(word_emb)\n        return word_emb + pos_emb  # (batch_size, seq_len, d_model)\n\n\n3.3 Feed Forward Network\n我们先跳过Multi Head Attention，先实现Feed Forward Network。Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Feed Forward Network由两个线性变换和一个ReLU激活函数组成。\nclass FFN(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.ln1 = nn.Linear(config.d_model, config.d_ff, bias=True)\n        self.ln2 = nn.Linear(config.d_ff, config.d_model, bias=True)\n\n    def forward(self, x):\n        x = F.relu(self.ln1(x))  # Apply ReLU activation\n        x = self.ln2(x)  # Linear transformation\n        return x  # (batch_size, seq_len, d_model)\n\n\n3.4 Layer Normalization\n还有一个重要的模块是Layer Normalization，它可以帮助模型更快地收敛。\nclass LayerNormalization(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        self.eps = config.eps\n\n        self.gamma = nn.Parameter(torch.ones(config.d_model))  # (d_model,)\n        self.beta = nn.Parameter(torch.zeros(config.d_model))  # (d_model,)\n\n    def _compute_mean_std(self, x):\n        \"\"\"\n        Compute mean and standard deviation for the input tensor x\n        On the last dimension (features)\n        x: (batch_size, seq_len, d_model)\n        Output:\n            mean: (batch_size, seq_len, 1)\n            std: (batch_size, seq_len, 1)\n        \"\"\"\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return mean, std\n\n    def forward(self, x):\n        mean, std = self._compute_mean_std(x)\n\n        # normalize x: (batch_size, seq_len, d_model)\n        normalized_x = (x - mean) / (std + self.eps)  # Avoid division by zero\n\n        return normalized_x * self.gamma + self.beta  # (batch_size, seq_len, d_model)\n\n\n3.5 Multi Head Attention\nMulti Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。\n\n\n\n\n\n\nImportant\n\n\n\n这部分是Transformer的核心模块，理解它是理解Transformer以及他变型的关键。记得多看几遍，直到你能理解为止。\n\n\ndef scaled_dot_product_attention(q, k, v, mask=None):\n    \"\"\"\n    Scaled Dot-Product Attention\n    q: (batch_size, num_heads, seq_len_q, d_k)\n    k: (batch_size, num_heads, seq_len_k, d_k)\n    v: (batch_size, num_heads, seq_len_v, d_v)\n    mask: (batch_size, 1, seq_len_q, seq_len_k) or None\n    \"\"\"\n    d_k = k.shape[-1]\n\n    scores = einops.einsum(\n        q,\n        k,\n        \"batch heads seq_len_q d_k, batch heads seq_len_k d_k -&gt; batch heads seq_len_q seq_len_k\",\n    )\n\n    scores = scores / math.sqrt(d_k)  # Scale the scores\n    scores = F.softmax(scores, dim=-1)  # Apply softmax to get attention weights\n\n    if mask is not None:\n        scores = scores.masked_fill(mask, float(\"-inf\"))  # Apply mask if provided\n\n    output = einops.einsum(\n        scores,\n        v,\n        \"batch heads seq_len_q seq_len_k, batch heads seq_len_k d_v -&gt; batch heads seq_len_q d_v\",\n    )\n\n    return output\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config: ModelConfig):\n        super().__init__()\n\n        assert (\n            config.d_model % config.num_heads == 0\n        ), \"d_model must be divisible by num_heads\"\n        self.d_k = config.d_model // config.num_heads  # Dimension of each head\n        self.num_heads = config.num_heads\n\n        self.qkv_proj = nn.Linear(\n            config.d_model, config.d_model * 3, bias=True\n        )  # (d_model, d_model * 3)\n\n        self.out_proj = nn.Linear(config.d_model, config.d_model, bias=True)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        x: (batch_size, seq_len, d_model)\n        mask: (batch_size, 1, seq_len_q, seq_len_k) or None\n        \"\"\"\n        batch_size, seq_len, _ = x.size()\n\n        q, k, v = map(\n            lambda t: einops.rearrange(\n                t,\n                \"batch seq_len (heads d_k) -&gt; batch heads seq_len d_k\",\n                heads=self.num_heads,\n            ),\n            self.qkv_proj(x).chunk(3, dim=-1),\n        )  # (batch, num_heads, seq_len, d_k)\n\n        # Compute attention\n        attn_output = scaled_dot_product_attention(q, k, v, mask)\n\n        # Rearrange back to (batch_size, seq_len, d_model)\n        attn_output = einops.rearrange(\n            attn_output,\n            \"batch heads seq_len d_v -&gt; batch seq_len (heads d_v)\",\n            heads=self.num_heads,\n        )\n\n        output = self.out_proj(attn_output)  # (batch_size, seq_len, d_model)\n        return output  # (batch_size, seq_len, d_model)\n\n\n3.6 Encoder Block\nEncoder Block是Transformer的一个重要模块，它由Multi Head Attention和Feed Forward Network组成。它的作用是对输入的向量进行编码，从而捕捉到不同的语义信息。\nclass EncoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FeedForwardNetwork(config)\n        self.norm1 = LayerNormalization(config.d_model)\n        self.norm2 = LayerNormalization(config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        # Multi Head Attention\n        attn_output = self.attention(x)\n        x = self.norm1(x + self.dropout(attn_output))   \n        # Feed Forward Network\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\n\n3.7 Decoder Block\nDecoder Block是Transformer的另一个重要模块，它与Encoder Block类似，但它还需要处理 Masked Multi Head Attention。Masked Multi Head Attention的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。\nclass DecoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()      \n\n        self.attention1 = MultiHeadAttention(config)\n        self.attention2 = MultiHeadAttention(config, is_causal=True)\n恭喜你，以及成功的实现了Transformer，这个是当前最重要的AI模型框架。理解了它，你就理解可以理解大部分的AI模型了。现在大火的ChatGPT，DeepSeek等模型都是基于Transformer的变型（在接下来的文章中，我们会阅读到这些模型）。完整的代码可以在GitHub上查看。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-transformer.html#扩展",
    "href": "posts/01-transformer.html#扩展",
    "title": "01: Attention is All You Need",
    "section": "4 扩展",
    "text": "4 扩展\n自从Transformer被提出以来，已经有了很多的变型和改进。具体的来说，Attention在Transformer中需要 \\(\\mathcal{O}(n^2)\\) 的计算复杂度，这在处理长文本时会变得非常慢。因此，很多研究者提出了各种各样的改进方法来降低计算复杂度。以下是一些常见的改进方法：\n\nSparse Attention: 通过稀疏化注意力矩阵来降低计算复杂度。比如，Reformer模型使用了局部敏感哈希（LSH）来实现稀疏注意力。\nLinear Attention: 通过将注意力计算转换为线性时间复杂度",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-transformer.html#qa",
    "href": "posts/01-transformer.html#qa",
    "title": "01: Attention is All You Need",
    "section": "5 Q&A",
    "text": "5 Q&A\n\n\n\n\n\n\n问题：为什么要用 $\\sqrt{d_k}$ 缩放点积？\n\n\n\n\n\n回答：如果不缩放，当 \\(d_k\\) 很大时，QK 的方差也会变大，使 softmax 落入梯度非常小的区域。除以 \\(\\sqrt{d_k}\\) 有助于将激活值保持在适合训练的范围内。\n\n\n\n\n\n\n\n\n\n问题：多头注意力解决了什么问题？\n\n\n\n\n\n回答：它让模型可以同时关注不同的表示子空间和位置的信息，从而克服单头自注意力容易“平均化”的问题。\n\n\n\n\n\n\n\n\n\n问题：Transformer 如何实现自回归式解码？\n\n\n\n\n\n回答：解码器通过将未来位置的注意力得分设为 \\(-\\infty\\) 来屏蔽它们，确保每个位置只能关注到前面的输出。\n\n\n\n\n\n\n\n\n\n问题：为什么使用正弦位置编码而不是可学习的位置编码？\n\n\n\n\n\n回答：正弦位置编码允许模型推广到更长的序列，参数量少，并且在 BLEU 分数上与可学习的位置编码效果相当。\n\n\n\n\n\n\n\n\n\n问题：与 RNN/CNN 相比，路径长度和计算复杂度有何不同？\n\n\n\n\n\n回答：自注意力的路径长度是常数，每层的计算复杂度是 \\(\\mathcal{O}(n²d)\\)；而 RNN 需要 \\(\\mathcal{O}(n)\\) 的串行步骤，CNN 需要堆叠多层才能覆盖长距离依赖。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer.html",
    "href": "posts/02-vision-transformer.html",
    "title": "02: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "section": "",
    "text": "在了解了什么是Transformer之后，我们来看看如何将Transformer应用于Computer Vision。Vision Transformer（ViT）是一个将Transformer架构应用于图像分类的模型。它的核心思想是将图像划分为小块（patches），然后将这些小块视为序列数据，类似于处理文本数据。",
    "crumbs": [
      "100 Papers",
      "02: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ]
  },
  {
    "objectID": "posts/02-vision-transformer.html#patchifying-images",
    "href": "posts/02-vision-transformer.html#patchifying-images",
    "title": "02: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
    "section": "1 Patchifying Images",
    "text": "1 Patchifying Images\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Image before patching\n\n\n\n\n\n\n\n\n\n\n\n(b) Image after patching\n\n\n\n\n\n\n\nFigure 2: Illustration of Patchifying Images with image size 256x256 and patch size 16x16\n\n\n\n\nOther content",
    "crumbs": [
      "100 Papers",
      "02: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html",
    "href": "00-how-to-read-paper.html",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "在深入学习人工智能相关论文之前，掌握高效且系统的阅读方法至关重要。论文阅读经典指南 How to Read a Paper 中提出了“三遍阅读法”，为我们提供了清晰的实践路径：\n\n\n\n\n第一遍：快速浏览，获取论文的整体结构和核心结论。聚焦于标题（title）、摘要（abstract）、引言（introduction）与结论（conclusion），从宏观上了解文章的研究方向。\n第二遍：细读论文，关注理论推导、实验设置与关键图表。重点理解论文所提出的方法、实验设计、结果分析及其支撑逻辑。\n第三遍：批判性阅读，系统分析论文的优劣，提出建设性问题，反思该方法是否具有通用性或是否能应用于自身研究。\n\n\n\nListing 1: 三遍阅读法\n\n\n\n需要强调的是，阅读论文是一个循环迭代、逐步深入的过程。在反复阅读与思考中，我们会不断修正理解、加深认知。与此同时，通过与他人交流、参与讨论，有助于拓宽视角、深化思考。（本项目网站也因此而生，旨在为研究者们提供一个共同交流学习的平台）\n为提升阅读效率，我们建议在精读阶段，使用颜色标记不同内容。例如在 Harvard CS197 AI Research Experiences 的 Lecture 3 中给出的策略：\n\n黄色：突出论文所试图解决的核心问题或挑战。\n绿色：标记论文中的提到的通用概念。\n粉色：对应提出的算法方法或技术路径。\n橙色：用于标记论文的创新点与贡献。\n\n例如如下示意图：\n\n\n\n\n\n\nFigure 1: Highlight Example of paper (Attention is all you need)\n\n\n\n这套方法并非唯一标准，关键在于构建一套适合自身认知方式的可视化标记体系。统一的标注风格，有助于在后期回顾或跨论文比较时高效定位关键信息。本项目所收录的每篇论文笔记也将采用这一结构化高亮标记方法。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#论文阅读指南",
    "href": "00-how-to-read-paper.html#论文阅读指南",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "在深入学习人工智能相关论文之前，掌握高效且系统的阅读方法至关重要。论文阅读经典指南 How to Read a Paper 中提出了“三遍阅读法”，为我们提供了清晰的实践路径：\n\n\n\n\n第一遍：快速浏览，获取论文的整体结构和核心结论。聚焦于标题（title）、摘要（abstract）、引言（introduction）与结论（conclusion），从宏观上了解文章的研究方向。\n第二遍：细读论文，关注理论推导、实验设置与关键图表。重点理解论文所提出的方法、实验设计、结果分析及其支撑逻辑。\n第三遍：批判性阅读，系统分析论文的优劣，提出建设性问题，反思该方法是否具有通用性或是否能应用于自身研究。\n\n\n\nListing 1: 三遍阅读法\n\n\n\n需要强调的是，阅读论文是一个循环迭代、逐步深入的过程。在反复阅读与思考中，我们会不断修正理解、加深认知。与此同时，通过与他人交流、参与讨论，有助于拓宽视角、深化思考。（本项目网站也因此而生，旨在为研究者们提供一个共同交流学习的平台）\n为提升阅读效率，我们建议在精读阶段，使用颜色标记不同内容。例如在 Harvard CS197 AI Research Experiences 的 Lecture 3 中给出的策略：\n\n黄色：突出论文所试图解决的核心问题或挑战。\n绿色：标记论文中的提到的通用概念。\n粉色：对应提出的算法方法或技术路径。\n橙色：用于标记论文的创新点与贡献。\n\n例如如下示意图：\n\n\n\n\n\n\nFigure 1: Highlight Example of paper (Attention is all you need)\n\n\n\n这套方法并非唯一标准，关键在于构建一套适合自身认知方式的可视化标记体系。统一的标注风格，有助于在后期回顾或跨论文比较时高效定位关键信息。本项目所收录的每篇论文笔记也将采用这一结构化高亮标记方法。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#工具准备",
    "href": "00-how-to-read-paper.html#工具准备",
    "title": "00: Preparation for Following",
    "section": "2 工具准备",
    "text": "2 工具准备\n科学阅读离不开合适的工具支撑。以下是推荐的工具体系，涵盖文献管理、笔记整理、代码执行等多个维度。\n\n2.1 文献管理：Zotero\n随着论文积累的增多，系统的文献管理工具不可或缺。Zotero 是一款免费且开源的文献管理平台，支持自动导入、分组管理与多格式引用（如 BibTeX）。其可扩展性极强，支持插件与主题定制。\n\n\n\n\n\n\nFigure 2: Example of Zotero\n\n\n\n推荐插件：\n\nBetter BibTex：增强 BibTeX 导出功能，便于与 LaTeX 无缝集成。\nEthereal Style：为 Zotero 提供美观的 UI 风格，提升使用体验。\n\n尽管 Zotero 存在一定学习曲线，但其长期价值远超初期投入。若仅希望临时阅读，PDF 阅读器亦可；但从科研视角出发，建议尽早投入学习与使用。\n此外，Zotero Chrome Connector 插件可实现一键导入网页文献，极大提升文献收集效率：\n\n\n\n\n\n\nFigure 3: Zotero Chrome Connector\n\n\n\n如 Figure 3 所示，只需点击插件按钮，即可将当前网页内容导入至文献库。\n\n\n2.2 笔记记录：Obsidian\nObsidian 是一款基于 Markdown 的笔记系统，支持双向链接与图谱视图，特别适合用于构建个人知识体系。\n\n\n\n\n\n\nFigure 4: Obsidian Example\n\n\n\n推荐插件：\n\nobsidian-latex-suite：提供 LaTeX 快捷输入与公式预览功能，显著提高数学表达效率。\nHighlightr Plugin：支持自定义高亮颜色，便于分类信息标注。\n\n \n需要注意的是，过度美化界面或插件堆叠可能反而分散注意力。建议以“结构清晰、内容为本”为首要原则。\n对于不使用 Obsidian 的用户，也可选择：\n\n\n\n\n\n\n\n\n\n\n\n(a) Notion Home Page\n\n\n\n\n\n\n\n\n\n\n\n(b) FeiShu Home Page\n\n\n\n\n\n\n\nFigure 5: Home Page of Notion and FeiShu\n\n\n\n\nNotion：如 Figure 5 (a) 所示，适合多人协作与可视化编辑。\n飞书：如 Figure 5 (b) 所示，功能全面，适合企业级文档管理。\n\n\n\n2.3 代码执行：Jupyter Notebook\n在“Paper with Code”理念下，每篇论文将配套 Jupyter Notebook 实现核心算法。其交互式文档特性，使其成为学习与验证代码的理想平台。\n\n\n\n\n\n\nNote\n\n\n\n若对 Jupyter Notebook 不熟悉，推荐参考 官方文档，以快速入门。\n\n\n相应的代码，我会放在GitHub的仓库中\n\n\n\n\n\n\nFigure 6: The preview of GitHub Page\n\n\n\n\n\n2.4 GPU 平台：云端执行环境\n深度学习模型常需 GPU 加速，若本地无 GPU 可使用以下平台：\n\nGoogle Colab：Google 提供的免费云端 Notebook 平台，支持 GPU 与 TPU。\nKaggle Kernels：支持 GPU 的数据科学平台，适合快速实验。\n\n国内可选平台：\n\nAutoDL：适合国内用户，配置简单，支持定制化部署。\n\n其他推荐：\n\nRunPod、Lambda Labs：提供稳定、低延迟的 GPU 训练环境，适合中大型实验任务。\n\n\n通过合理配置上述工具，可以构建出一个系统化、高效的论文学习与研究流程。在接下来的章节中，每篇论文将附带代码实现、结构解析与批判性思考，欢迎共同学习交流。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#总结",
    "href": "00-how-to-read-paper.html#总结",
    "title": "00: Preparation for Following",
    "section": "3 总结",
    "text": "3 总结\n在本节中，我们介绍了高效阅读论文的方法论与工具体系。通过“三遍阅读法” Listing 1， 我们可以系统地理解论文内容，并在此基础上进行批判性思考。同时，借助 Zotero Section 2.1、ObsidianSection 2.2 等工具，可以有效管理文献、记录笔记与执行代码。 在后续章节中，我们将应用这些方法与工具，深入分析每篇论文的核心思想、实验设计与创新贡献。希望通过本项目的学习，能够帮助大家更好地掌握人工智能领域的前沿研究动态，并在实践中不断提升自己的科研能力。",
    "crumbs": [
      "00 Preparation for following"
    ]
  }
]