[
  {
    "objectID": "notebooks/01-attention.html",
    "href": "notebooks/01-attention.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport math\nimport random\n\n\ndef green(text):\n    return f\"\\033[1;32m{text}\\033[0m\"\n\n\ndef red(text):\n    return f\"\\033[1;31m{text}\\033[0m\"\n\n\ndef yellow(text):\n    return f\"\\033[1;33m{text}\\033[0m\"\n\n\ndef bold(text):\n    return f\"\\033[1m{text}\\033[0m\"\n\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"Using device: {green(device)}\")\n\n\nUsing device: mps\n\n\n\n\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass TransformerConfig:\n    d_model: int = 512\n    d_ff: int = 2048\n\n    num_heads: int = 8\n\n    dropout: float = 0.1\n\n    vocab_size: int = 10000\n    max_seq_len: int = 512\n\n\nclass WordEmbedding(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n\n    def forward(self, x):\n        return self.embedding(x)\n\n\nPosition Encoding\n\nclass PositionEncoding(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.dropout = nn.Dropout(config.dropout)\n\n        # Create the positional encoding matrix\n        pe = torch.zeros(config.max_seq_len, config.d_model)\n        position = torch.arange(0, config.max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, config.d_model, 2).float()\n            * (-math.log(10000.0) / config.d_model)\n        )\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[: x.size(0), :]\n        return self.dropout(x)\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.weight * (x - mean) / (std + self.eps) + self.bias\n\n\nclass FFN(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\n\ndef scaled_dot_production_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    mask: torch.Tensor | None = None,\n):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n\n    attn_weights = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, value)\n\n    return output, attn_weights\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.num_heads = config.num_heads\n        self.d_model = config.d_model\n        self.d_k = config.d_model // config.num_heads\n\n        self.query = nn.Linear(config.d_model, config.d_model)\n        self.key = nn.Linear(config.d_model, config.d_model)\n        self.value = nn.Linear(config.d_model, config.d_model)\n\n        self.fc_out = nn.Linear(config.d_model, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        query = (\n            self.query(query)\n            .view(batch_size, -1, self.num_heads, self.d_k)\n            .transpose(1, 2)\n        )\n        key = (\n            self.key(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        )\n        value = (\n            self.value(value)\n            .view(batch_size, -1, self.num_heads, self.d_k)\n            .transpose(1, 2)\n        )\n\n        attn_output, attn_weights = scaled_dot_production_attention(\n            query, key, value, mask\n        )\n\n        attn_output = (\n            attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        )\n        output = self.fc_out(attn_output)\n\n        return output, attn_weights\n\n\nclass CausalAttention(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        mask = (\n            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n\n        query = query.view(batch_size, -1, 1, self.mask.size(-1))\n        key = key.view(batch_size, -1, 1, self.mask.size(-1))\n        value = value.view(batch_size, -1, 1, self.mask.size(-1))\n\n        attn_output, attn_weights = scaled_dot_production_attention(\n            query, key, value, self.mask\n        )\n\n        attn_output = attn_output.view(batch_size, -1, self.mask.size(-1))\n\n        return attn_output, attn_weights\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FFN(config)\n\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n\n        self.dropout1 = nn.Dropout(config.dropout)\n        self.dropout2 = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        attn_output, _ = self.attention(x, x, x)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n\n        ffn_output = self.ffn(x)\n        x = x + self.dropout2(ffn_output)\n        x = self.norm2(x)\n\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.attention1 = CausalAttention(config)\n        self.attention2 = MultiHeadAttention(config)\n        self.ffn = FFN(config)\n\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n        self.norm3 = LayerNorm(config.d_model)\n\n        self.dropout1 = nn.Dropout(config.dropout)\n        self.dropout2 = nn.Dropout(config.dropout)\n        self.dropout3 = nn.Dropout(config.dropout)\n\n    def forward(self, x, encoder_output):\n        attn_output, _ = self.attention1(x, x, x)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n\n        attn_output, _ = self.attention2(x, encoder_output, encoder_output)\n        x = x + self.dropout2(attn_output)\n        x = self.norm2(x)\n\n        ffn_output = self.ffn(x)\n        x = x + self.dropout3(ffn_output)\n        x = self.norm3(x)\n\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = WordEmbedding(config)\n        self.position_encoding = PositionEncoding(config)\n\n        self.layers = nn.ModuleList(\n            [EncoderBlock(config) for _ in range(config.num_heads)]\n        )\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        return x\n\n\nclass Decoder(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = WordEmbedding(config)\n        self.position_encoding = PositionEncoding(config)\n\n        self.layers = nn.ModuleList(\n            [DecoderBlock(config) for _ in range(config.num_heads)]\n        )\n\n    def forward(self, x, encoder_output):\n        x = self.embedding(x)\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x = layer(x, encoder_output)\n\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.encoder = Encoder(config)\n        self.decoder = Decoder(config)\n\n        self.fc_out = nn.Linear(config.d_model, config.vocab_size)\n\n    def tie_weights(self):\n        \"\"\"Tie the weights of the embedding and output layers.\"\"\"\n        self.fc_out.weight = self.encoder.embedding.embedding.weight\n\n    def forward(self, src, tgt):\n        encoder_output = self.encoder(src)\n        decoder_output = self.decoder(tgt, encoder_output)\n        output = self.fc_out(decoder_output)\n\n        return output"
  },
  {
    "objectID": "00-how-to-read-paper.html",
    "href": "00-how-to-read-paper.html",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "在深入学习人工智能相关论文之前，掌握高效且系统的阅读方法至关重要。论文阅读经典指南 How to Read a Paper 中提出了“三遍阅读法”，为我们提供了清晰的实践路径：\n\n\n\n\n第一遍：快速浏览，获取论文的整体结构和核心结论。聚焦于标题（title）、摘要（abstract）、引言（introduction）与结论（conclusion），从宏观上了解文章的研究方向。\n第二遍：细读论文，关注理论推导、实验设置与关键图表。重点理解论文所提出的方法、实验设计、结果分析及其支撑逻辑。\n第三遍：批判性阅读，系统分析论文的优劣，提出建设性问题，反思该方法是否具有通用性或是否能应用于自身研究。\n\n\n\nListing 1: 三遍阅读法\n\n\n\n需要强调的是，阅读论文是一个循环迭代、逐步深入的过程。在反复阅读与思考中，我们会不断修正理解、加深认知。与此同时，通过与他人交流、参与讨论，有助于拓宽视角、深化思考。（本项目网站也因此而生，旨在为研究者们提供一个共同交流学习的平台）\n为提升阅读效率，我们建议在精读阶段，使用颜色标记不同内容。例如在 Harvard CS197 AI Research Experiences 的 Lecture 3 中给出的策略：\n\n黄色：突出论文所试图解决的核心问题或挑战。\n粉色：对应提出的算法方法或技术路径。\n橙色：用于标记论文的创新点与贡献。\n\n例如如下示意图：\n\n\n\n\n\n\nFigure 1: Highlight Example of paper (Attention is all you need)\n\n\n\n这套方法并非唯一标准，关键在于构建一套适合自身认知方式的可视化标记体系。统一的标注风格，有助于在后期回顾或跨论文比较时高效定位关键信息。本项目所收录的每篇论文笔记也将采用这一结构化高亮标记方法。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#论文阅读指南",
    "href": "00-how-to-read-paper.html#论文阅读指南",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "在深入学习人工智能相关论文之前，掌握高效且系统的阅读方法至关重要。论文阅读经典指南 How to Read a Paper 中提出了“三遍阅读法”，为我们提供了清晰的实践路径：\n\n\n\n\n第一遍：快速浏览，获取论文的整体结构和核心结论。聚焦于标题（title）、摘要（abstract）、引言（introduction）与结论（conclusion），从宏观上了解文章的研究方向。\n第二遍：细读论文，关注理论推导、实验设置与关键图表。重点理解论文所提出的方法、实验设计、结果分析及其支撑逻辑。\n第三遍：批判性阅读，系统分析论文的优劣，提出建设性问题，反思该方法是否具有通用性或是否能应用于自身研究。\n\n\n\nListing 1: 三遍阅读法\n\n\n\n需要强调的是，阅读论文是一个循环迭代、逐步深入的过程。在反复阅读与思考中，我们会不断修正理解、加深认知。与此同时，通过与他人交流、参与讨论，有助于拓宽视角、深化思考。（本项目网站也因此而生，旨在为研究者们提供一个共同交流学习的平台）\n为提升阅读效率，我们建议在精读阶段，使用颜色标记不同内容。例如在 Harvard CS197 AI Research Experiences 的 Lecture 3 中给出的策略：\n\n黄色：突出论文所试图解决的核心问题或挑战。\n粉色：对应提出的算法方法或技术路径。\n橙色：用于标记论文的创新点与贡献。\n\n例如如下示意图：\n\n\n\n\n\n\nFigure 1: Highlight Example of paper (Attention is all you need)\n\n\n\n这套方法并非唯一标准，关键在于构建一套适合自身认知方式的可视化标记体系。统一的标注风格，有助于在后期回顾或跨论文比较时高效定位关键信息。本项目所收录的每篇论文笔记也将采用这一结构化高亮标记方法。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#工具准备",
    "href": "00-how-to-read-paper.html#工具准备",
    "title": "00: Preparation for Following",
    "section": "工具准备",
    "text": "工具准备\n科学阅读离不开合适的工具支撑。以下是推荐的工具体系，涵盖文献管理、笔记整理、代码执行等多个维度。\n\n文献管理：Zotero\n随着论文积累的增多，系统的文献管理工具不可或缺。Zotero 是一款免费且开源的文献管理平台，支持自动导入、分组管理与多格式引用（如 BibTeX）。其可扩展性极强，支持插件与主题定制。\n\n\n\n\n\n\nFigure 2: Example of Zotero\n\n\n\n推荐插件：\n\nBetter BibTex：增强 BibTeX 导出功能，便于与 LaTeX 无缝集成。\nEthereal Style：为 Zotero 提供美观的 UI 风格，提升使用体验。\n\n尽管 Zotero 存在一定学习曲线，但其长期价值远超初期投入。若仅希望临时阅读，PDF 阅读器亦可；但从科研视角出发，建议尽早投入学习与使用。\n此外，Zotero Chrome Connector 插件可实现一键导入网页文献，极大提升文献收集效率：\n\n\n\n\n\n\nFigure 3: Zotero Chrome Connector\n\n\n\n如 Figure 3 所示，只需点击插件按钮，即可将当前网页内容导入至文献库。\n\n\n笔记记录：Obsidian\nObsidian 是一款基于 Markdown 的笔记系统，支持双向链接与图谱视图，特别适合用于构建个人知识体系。\n\n\n\n\n\n\nFigure 4: Obsidian Example\n\n\n\n推荐插件：\n\nobsidian-latex-suite：提供 LaTeX 快捷输入与公式预览功能，显著提高数学表达效率。\nHighlightr Plugin：支持自定义高亮颜色，便于分类信息标注。\n\n \n需要注意的是，过度美化界面或插件堆叠可能反而分散注意力。建议以“结构清晰、内容为本”为首要原则。\n对于不使用 Obsidian 的用户，也可选择：\n\n\n\n\n\n\n\n\n\n\n\n(a) Notion Home Page\n\n\n\n\n\n\n\n\n\n\n\n(b) FeiShu Home Page\n\n\n\n\n\n\n\nFigure 5: Home Page of Notion and FeiShu\n\n\n\n\nNotion：如 Figure 5 (a) 所示，适合多人协作与可视化编辑。\n飞书：如 Figure 5 (b) 所示，功能全面，适合企业级文档管理。\n\n\n\n代码执行：Jupyter Notebook\n在“Paper with Code”理念下，每篇论文将配套 Jupyter Notebook 实现核心算法。其交互式文档特性，使其成为学习与验证代码的理想平台。\n\n\n\n\n\n\nNote\n\n\n\n若对 Jupyter Notebook 不熟悉，推荐参考 官方文档，以快速入门。\n\n\n相应的代码，我会放在GitHub的仓库中\n\n\n\n\n\n\nFigure 6: The preview of GitHub Page\n\n\n\n\n\nGPU 平台：云端执行环境\n深度学习模型常需 GPU 加速，若本地无 GPU 可使用以下平台：\n\nGoogle Colab：Google 提供的免费云端 Notebook 平台，支持 GPU 与 TPU。\nKaggle Kernels：支持 GPU 的数据科学平台，适合快速实验。\n\n国内可选平台：\n\nAutoDL：适合国内用户，配置简单，支持定制化部署。\n\n其他推荐：\n\nRunPod、Lambda Labs：提供稳定、低延迟的 GPU 训练环境，适合中大型实验任务。\n\n\n通过合理配置上述工具，可以构建出一个系统化、高效的论文学习与研究流程。在接下来的章节中，每篇论文将附带代码实现、结构解析与批判性思考，欢迎共同学习交流。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#总结",
    "href": "00-how-to-read-paper.html#总结",
    "title": "00: Preparation for Following",
    "section": "总结",
    "text": "总结\n在本节中，我们介绍了高效阅读论文的方法论与工具体系。通过“三遍阅读法” Listing 1， 我们可以系统地理解论文内容，并在此基础上进行批判性思考。同时，借助 Zotero Section 2.1、ObsidianSection 2.2 等工具，可以有效管理文献、记录笔记与执行代码。 在后续章节中，我们将应用这些方法与工具，深入分析每篇论文的核心思想、实验设计与创新贡献。希望通过本项目的学习，能够帮助大家更好地掌握人工智能领域的前沿研究动态，并在实践中不断提升自己的科研能力。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "posts/01-attention.html",
    "href": "posts/01-attention.html",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "给定两个向量 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), 它们的点积为: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\n测量两个向量的相似度, 数值越大，说明两个向量越相似\n在 Self-Attention 中，用来衡量“当前词对其他词的关注程度”\n\n\n\n\n给定向量 \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)，Softmax 函数定义为: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\n将注意力“得分（dot product）”转换为概率分布\n每个词对其他词的注意力权重 \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#preliminary",
    "href": "posts/01-attention.html#preliminary",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "给定两个向量 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), 它们的点积为: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\n测量两个向量的相似度, 数值越大，说明两个向量越相似\n在 Self-Attention 中，用来衡量“当前词对其他词的关注程度”\n\n\n\n\n给定向量 \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)，Softmax 函数定义为: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\n将注意力“得分（dot product）”转换为概率分布\n每个词对其他词的注意力权重 \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#attention-is-all-you-need",
    "href": "posts/01-attention.html#attention-is-all-you-need",
    "title": "01: Attention is All You Need",
    "section": "Attention is All You Need",
    "text": "Attention is All You Need\n\n\n\n\n\n\nFigure 1: The Transformer Family Version 2.0 | Lil’Log\n\n\n\n如图 Figure 1 所示，是Transformer的整体架构。从\n\n位置编码\nTransformer的输入是一个序列，序列中的每个词都被转换为一个向量。为了让模型能够理解词语在序列中的位置，我们需要添加位置编码（Position Encoding）。位置编码可以是固定的，也可以是可学习的。 在Transformer中，位置编码是通过正弦和余弦函数来实现的：具体来说，对于序列中的第 \\(i\\) 个位置，位置编码的第 \\(j\\) 维可以表示为： \\[\n\\text{PE}(i, j) =\n\\begin{cases}\n\\sin\\left(\\frac{i}{10000^{j/d}}\\right) & \\text{if } j \\text{ is even} \\\\\n\\cos\\left(\\frac{i}{10000^{(j-1)/d}}\\right) & \\text{if } j \\     \\text{ is odd}\n\\end{cases}\n\\tag{3}\\] 其中 \\(d\\) 是位置编码的维度。位置编码的作用是为每个词提供一个唯一的位置信息，使得模型能够捕捉到词语在序列中的相对位置关系。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#pytorch-实现",
    "href": "posts/01-attention.html#pytorch-实现",
    "title": "01: Attention is All You Need",
    "section": "PyTorch 实现",
    "text": "PyTorch 实现\n接下来，我们叫利用PyTorch来实现Transformer的模型架构。我们采用Bottom-Up的方法，先实现Word Embedding， 接着是Position Embedding，接着实现我们的重点，即Multi-Head-Attention，再次之后，我们会实现 Point-Wise Feed Forward Network。最后将这几个模块组合起来，实现Transformer的Encoder 和 Decoder。准备好了吗？让我们开始吧！\n\nWord Embedding\nWord Embedding是将词语转换为向量的过程。在PyTorch 中的实现非常简单，我们可以使用nn.Embedding类来实现。这个类会将每个token映射到一个固定维度的向量空间中。\nclass WordEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n    \n    def _init_weight(self):\n        nn.init.trunc_normal_(self.embedding.weight, mean = 0, std=0.02, a=-3, b=3)\n    \n    def forward(self, x):\n        return self.embedding(x)    \n\n\nPosition Embedding\n接下来，我们来实现Position Embedding Equation 3。\nclass PositionEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.seq_len = config.seq_len\n        self.d_model = config.d_model\n有了Word Embedding 和 Position Embedding，我们就可以将输入的token转换为向量了。我们需要接下来需要做的就是，将这两个向量相加，得到最终的输入向量。\nclass InputEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.word_embedding = WordEmbedding(config)\n        self.position_embedding = PositionEmbedding(config)\n    \n    def forward(self, x):\n        word_emb = self.word_embedding(x)\n        pos_emb = self.position_embedding(x)\n        \n        return word_emb + pos_emb\n\n\nFeed Forward Network\n我们先跳过Multi Head Attention，先实现Feed Forward Network。Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Feed Forward Network由两个线性变换和一个ReLU激活函数组成。\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n\n\nLayer Normalization\n还有一个重要的模块是Layer Normalization，它可以帮助模型更快地收敛。\nclass LayerNormalization(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.weight * normalized_x + self.bias\n\n\nMulti Head Attention\nMulti Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。\n\n\n\n\n\n\nImportant\n\n\n\n这部分是Transformer的核心模块，理解它是理解Transformer以及他变型的关键。记得多看几遍，直到你能理解为止。\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        assert config.d_model % config.num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.head_dim = config.d_model // config.num_heads\n\n        self.qkv_proj = nn.Linear(config.d_model, config.d_model * 3)\n        self.out_proj = nn.Linear(config.d_model, config.d_model)\n\n\nEncoder Block\nEncoder Block是Transformer的一个重要模块，它由Multi Head Attention和Feed Forward Network组成。它的作用是对输入的向量进行编码，从而捕捉到不同的语义信息。\nclass EncoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FeedForwardNetwork(config)\n        self.norm1 = LayerNormalization(config.d_model)\n        self.norm2 = LayerNormalization(config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        # Multi Head Attention\n        attn_output = self.attention(x)\n        x = self.norm1(x + self.dropout(attn_output))   \n        # Feed Forward Network\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\n\nDecoder Block\nDecoder Block是Transformer的另一个重要模块，它与Encoder Block类似，但它还需要处理 Masked Multi Head Attention。Masked Multi Head Attention的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。\nclass DecoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()      \n\n        self.attention1 = MultiHeadAttention(config)\n        self.attention2 = MultiHeadAttention(config, is_causal=True)\n恭喜你，以及成功的实现了Transformer，这个是当前最重要的AI模型框架。理解了它，你就理解可以理解大部分的AI模型了。现在大火的ChatGPT，DeepSeek等模型都是基于Transformer的变型（在接下来的文章中，我们会阅读到这些模型）。完整的代码可以在GitHub上查看。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#扩展",
    "href": "posts/01-attention.html#扩展",
    "title": "01: Attention is All You Need",
    "section": "扩展",
    "text": "扩展\n自从Transformer被提出以来，已经有了很多的变型和改进。具体的来说，Attention在Transformer中需要 \\(\\mathcal{O}(n^2)\\) 的计算复杂度，这在处理长文本时会变得非常慢。因此，很多研究者提出了各种各样的改进方法来降低计算复杂度。以下是一些常见的改进方法：\n\nSparse Attention: 通过稀疏化注意力矩阵来降低计算复杂度。比如，Reformer模型使用了局部敏感哈希（LSH）来实现稀疏注意力。\nLinear Attention: 通过将注意力计算转换为线性时间复杂度",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/02-bert.html",
    "href": "posts/02-bert.html",
    "title": "02: BERT",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Equation 1\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\n\nprint(\"Hello,m World!\")\n\\[\n\\mathcal{H} = \\mathcal{W} \\cdot \\mathcal{O}\n\\tag{1}\\]\nThis is the figure see, figure ?@fig-polar $$",
    "crumbs": [
      "100 Papers",
      "02: BERT"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nPaper Link\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need\nTransformer-based model for sequence modeling\n🔗\n\nNLP / Transformer\n\n\n02\nBERT\nBidirectional encoder for pre-training NLP tasks\n🔗\n\nNLP / Pretraining\n\n\n03\nGPT-3\nGenerative transformer for autoregressive tasks\n🔗\n\nNLP / Generation\n\n\n04\nCLIP\nContrastive Language-Image Pretraining\n🔗\n\nVision / Multimodal\n\n\n05\nViT\nVision Transformer for image classification\n🔗\n\nVision / Transformer",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#前瞻",
    "href": "00-how-to-read-paper.html#前瞻",
    "title": "00: Preparation for Following",
    "section": "前瞻",
    "text": "前瞻\n接下来，我们将进入第一篇论文的学习，开始我们的“100 Papers with Code”之旅。每篇论文都将配备详细的笔记、代码实现与批判性分析，期待与大家共同探索人工智能的奥秘。",
    "crumbs": [
      "00 Preparation for following"
    ]
  }
]