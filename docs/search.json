[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "这个网站旨在收集和展示 100 篇重要的 AI 论文及其代码实现。每篇论文都附有链接，方便读者深入了解。",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#关于这个网站",
    "href": "index.html#关于这个网站",
    "title": "100 Papers with Code",
    "section": "",
    "text": "这个网站旨在收集和展示 100 篇重要的 AI 论文及其代码实现。每篇论文都附有链接，方便读者深入了解。",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#为什么要搭建这个网站",
    "href": "index.html#为什么要搭建这个网站",
    "title": "100 Papers with Code",
    "section": "为什么要搭建这个网站？",
    "text": "为什么要搭建这个网站？\n在本人学习，阅读和实践 AI 领域的过程中，发现很多重要的论文和代码实现都散落在各个地方。为了方便自己和其他人查阅，我决定搭建这个网站，将这些重要的论文和以及自己的实现，集中在一起。",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html#如何使用这个网站",
    "href": "index.html#如何使用这个网站",
    "title": "100 Papers with Code",
    "section": "如何使用这个网站？",
    "text": "如何使用这个网站？\n每个论文都有一个独立的页面，包含论文的基本信息、链接和代码实现。你可以通过点击论文名称来查看详细内容。一下是以及实现，或者是打算记录的论文列表：\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need\nIntroduced the Transformer architecture, which relies entirely on self-attention mechanisms for sequence modeling, enabling parallel computation and improving performance on NLP tasks.\n     \nNLP / Transformer",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/01-attention.html",
    "href": "posts/01-attention.html",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "给定两个向量 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), 它们的点积为: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\n测量两个向量的相似度, 数值越大，说明两个向量越相似\n在 Self-Attention 中，用来衡量“当前词对其他词的关注程度”\n\n\n\n\n给定向量 \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)，Softmax 函数定义为: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\n将注意力“得分（dot product）”转换为概率分布\n每个词对其他词的注意力权重 \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#preliminary",
    "href": "posts/01-attention.html#preliminary",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "给定两个向量 \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), 它们的点积为: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\n测量两个向量的相似度, 数值越大，说明两个向量越相似\n在 Self-Attention 中，用来衡量“当前词对其他词的关注程度”\n\n\n\n\n给定向量 \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)，Softmax 函数定义为: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\n将注意力“得分（dot product）”转换为概率分布\n每个词对其他词的注意力权重 \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#attention-is-all-you-need",
    "href": "posts/01-attention.html#attention-is-all-you-need",
    "title": "01: Attention is All You Need",
    "section": "2 Attention is All You Need",
    "text": "2 Attention is All You Need\n\n\n\n\n\n\nFigure 1: The Transformer Family Version 2.0 | Lil’Log\n\n\n\n如图 Figure 1 所示，是Transformer的整体架构。从\n\n2.1 位置编码\nTransformer的输入是一个序列，序列中的每个词都被转换为一个向量。为了让模型能够理解词语在序列中的位置，我们需要添加位置编码（Position Encoding）。位置编码可以是固定的，也可以是可学习的。 在Transformer中，位置编码是通过正弦和余弦函数来实现的：具体来说，对于序列中的第 \\(i\\) 个位置，位置编码的第 \\(j\\) 维可以表示为： \\[\n\\text{PE}(i, j) =\n\\begin{cases}\n\\sin\\left(\\frac{i}{10000^{j/d}}\\right) & \\text{if } j \\text{ is even} \\\\\n\\cos\\left(\\frac{i}{10000^{(j-1)/d}}\\right) & \\text{if } j \\     \\text{ is odd}\n\\end{cases}\n\\tag{3}\\] 其中 \\(d\\) 是位置编码的维度。位置编码的作用是为每个词提供一个唯一的位置信息，使得模型能够捕捉到词语在序列中的相对位置关系。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#pytorch-实现",
    "href": "posts/01-attention.html#pytorch-实现",
    "title": "01: Attention is All You Need",
    "section": "3 PyTorch 实现",
    "text": "3 PyTorch 实现\n接下来，我们叫利用PyTorch来实现Transformer的模型架构。我们采用Bottom-Up的方法，先实现Word Embedding， 接着是Position Embedding，接着实现我们的重点，即Multi-Head-Attention，再次之后，我们会实现 Point-Wise Feed Forward Network。最后将这几个模块组合起来，实现Transformer的Encoder 和 Decoder。准备好了吗？让我们开始吧！\n\n3.1 Word Embedding\nWord Embedding是将词语转换为向量的过程。在PyTorch 中的实现非常简单，我们可以使用nn.Embedding类来实现。这个类会将每个token映射到一个固定维度的向量空间中。\nclass WordEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n    \n    def _init_weight(self):\n        nn.init.trunc_normal_(self.embedding.weight, mean = 0, std=0.02, a=-3, b=3)\n    \n    def forward(self, x):\n        return self.embedding(x)    \n\n\n3.2 Position Embedding\n接下来，我们来实现Position Embedding Equation 3。\nclass PositionEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.seq_len = config.seq_len\n        self.d_model = config.d_model\n有了Word Embedding 和 Position Embedding，我们就可以将输入的token转换为向量了。我们需要接下来需要做的就是，将这两个向量相加，得到最终的输入向量。\nclass InputEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.word_embedding = WordEmbedding(config)\n        self.position_embedding = PositionEmbedding(config)\n    \n    def forward(self, x):\n        word_emb = self.word_embedding(x)\n        pos_emb = self.position_embedding(x)\n        \n        return word_emb + pos_emb\n\n\n3.3 Feed Forward Network\n我们先跳过Multi Head Attention，先实现Feed Forward Network。Feed Forward Network是Transformer中的一个重要模块，它的作用是对每个位置的向量进行非线性变换。具体来说，Feed Forward Network由两个线性变换和一个ReLU激活函数组成。\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n\n\n3.4 Layer Normalization\n还有一个重要的模块是Layer Normalization，它可以帮助模型更快地收敛。\nclass LayerNormalization(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.weight * normalized_x + self.bias\n\n\n3.5 Multi Head Attention\nMulti Head Attention是Transformer的核心模块。它的作用是将输入的向量进行多头注意力计算，从而捕捉到不同的语义信息。\n\n\n\n\n\n\nImportant\n\n\n\n这部分是Transformer的核心模块，理解它是理解Transformer以及他变型的关键。记得多看几遍，直到你能理解为止。\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        assert config.d_model % config.num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.head_dim = config.d_model // config.num_heads\n\n        self.qkv_proj = nn.Linear(config.d_model, config.d_model * 3)\n        self.out_proj = nn.Linear(config.d_model, config.d_model)\n\n\n3.6 Encoder Block\nEncoder Block是Transformer的一个重要模块，它由Multi Head Attention和Feed Forward Network组成。它的作用是对输入的向量进行编码，从而捕捉到不同的语义信息。\nclass EncoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FeedForwardNetwork(config)\n        self.norm1 = LayerNormalization(config.d_model)\n        self.norm2 = LayerNormalization(config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        # Multi Head Attention\n        attn_output = self.attention(x)\n        x = self.norm1(x + self.dropout(attn_output))   \n        # Feed Forward Network\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\n\n3.7 Decoder Block\nDecoder Block是Transformer的另一个重要模块，它与Encoder Block类似，但它还需要处理 Masked Multi Head Attention。Masked Multi Head Attention的作用是防止模型在训练时看到未来的信息，从而保证模型的自回归特性。\nclass DecoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()      \n\n        self.attention1 = MultiHeadAttention(config)\n        self.attention2 = MultiHeadAttention(config, is_causal=True)\n恭喜你，以及成功的实现了Transformer，这个是当前最重要的AI模型框架。理解了它，你就理解可以理解大部分的AI模型了。现在大火的ChatGPT，DeepSeek等模型都是基于Transformer的变型（在接下来的文章中，我们会阅读到这些模型）。完整的代码可以在GitHub上查看。",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#扩展",
    "href": "posts/01-attention.html#扩展",
    "title": "01: Attention is All You Need",
    "section": "4 扩展",
    "text": "4 扩展\n自从Transformer被提出以来，已经有了很多的变型和改进。具体的来说，Attention在Transformer中需要 \\(\\mathcal{O}(n^2)\\) 的计算复杂度，这在处理长文本时会变得非常慢。因此，很多研究者提出了各种各样的改进方法来降低计算复杂度。以下是一些常见的改进方法：\n\nSparse Attention: 通过稀疏化注意力矩阵来降低计算复杂度。比如，Reformer模型使用了局部敏感哈希（LSH）来实现稀疏注意力。\nLinear Attention: 通过将注意力计算转换为线性时间复杂度",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html",
    "href": "00-how-to-read-paper.html",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "在深入学习人工智能相关论文之前，掌握高效且系统的阅读方法至关重要。论文阅读经典指南 How to Read a Paper 中提出了“三遍阅读法”，为我们提供了清晰的实践路径：\n\n\n\n\n第一遍：快速浏览，获取论文的整体结构和核心结论。聚焦于标题（title）、摘要（abstract）、引言（introduction）与结论（conclusion），从宏观上了解文章的研究方向。\n第二遍：细读论文，关注理论推导、实验设置与关键图表。重点理解论文所提出的方法、实验设计、结果分析及其支撑逻辑。\n第三遍：批判性阅读，系统分析论文的优劣，提出建设性问题，反思该方法是否具有通用性或是否能应用于自身研究。\n\n\n\nListing 1: 三遍阅读法\n\n\n\n需要强调的是，阅读论文是一个循环迭代、逐步深入的过程。在反复阅读与思考中，我们会不断修正理解、加深认知。与此同时，通过与他人交流、参与讨论，有助于拓宽视角、深化思考。（本项目网站也因此而生，旨在为研究者们提供一个共同交流学习的平台）\n为提升阅读效率，我们建议在精读阶段，使用颜色标记不同内容。例如在 Harvard CS197 AI Research Experiences 的 Lecture 3 中给出的策略：\n\n黄色：突出论文所试图解决的核心问题或挑战。\n绿色：标记论文中的提到的通用概念。\n粉色：对应提出的算法方法或技术路径。\n橙色：用于标记论文的创新点与贡献。\n\n例如如下示意图：\n\n\n\n\n\n\nFigure 1: Highlight Example of paper (Attention is all you need)\n\n\n\n这套方法并非唯一标准，关键在于构建一套适合自身认知方式的可视化标记体系。统一的标注风格，有助于在后期回顾或跨论文比较时高效定位关键信息。本项目所收录的每篇论文笔记也将采用这一结构化高亮标记方法。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#论文阅读指南",
    "href": "00-how-to-read-paper.html#论文阅读指南",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "在深入学习人工智能相关论文之前，掌握高效且系统的阅读方法至关重要。论文阅读经典指南 How to Read a Paper 中提出了“三遍阅读法”，为我们提供了清晰的实践路径：\n\n\n\n\n第一遍：快速浏览，获取论文的整体结构和核心结论。聚焦于标题（title）、摘要（abstract）、引言（introduction）与结论（conclusion），从宏观上了解文章的研究方向。\n第二遍：细读论文，关注理论推导、实验设置与关键图表。重点理解论文所提出的方法、实验设计、结果分析及其支撑逻辑。\n第三遍：批判性阅读，系统分析论文的优劣，提出建设性问题，反思该方法是否具有通用性或是否能应用于自身研究。\n\n\n\nListing 1: 三遍阅读法\n\n\n\n需要强调的是，阅读论文是一个循环迭代、逐步深入的过程。在反复阅读与思考中，我们会不断修正理解、加深认知。与此同时，通过与他人交流、参与讨论，有助于拓宽视角、深化思考。（本项目网站也因此而生，旨在为研究者们提供一个共同交流学习的平台）\n为提升阅读效率，我们建议在精读阶段，使用颜色标记不同内容。例如在 Harvard CS197 AI Research Experiences 的 Lecture 3 中给出的策略：\n\n黄色：突出论文所试图解决的核心问题或挑战。\n绿色：标记论文中的提到的通用概念。\n粉色：对应提出的算法方法或技术路径。\n橙色：用于标记论文的创新点与贡献。\n\n例如如下示意图：\n\n\n\n\n\n\nFigure 1: Highlight Example of paper (Attention is all you need)\n\n\n\n这套方法并非唯一标准，关键在于构建一套适合自身认知方式的可视化标记体系。统一的标注风格，有助于在后期回顾或跨论文比较时高效定位关键信息。本项目所收录的每篇论文笔记也将采用这一结构化高亮标记方法。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#工具准备",
    "href": "00-how-to-read-paper.html#工具准备",
    "title": "00: Preparation for Following",
    "section": "2 工具准备",
    "text": "2 工具准备\n科学阅读离不开合适的工具支撑。以下是推荐的工具体系，涵盖文献管理、笔记整理、代码执行等多个维度。\n\n2.1 文献管理：Zotero\n随着论文积累的增多，系统的文献管理工具不可或缺。Zotero 是一款免费且开源的文献管理平台，支持自动导入、分组管理与多格式引用（如 BibTeX）。其可扩展性极强，支持插件与主题定制。\n\n\n\n\n\n\nFigure 2: Example of Zotero\n\n\n\n推荐插件：\n\nBetter BibTex：增强 BibTeX 导出功能，便于与 LaTeX 无缝集成。\nEthereal Style：为 Zotero 提供美观的 UI 风格，提升使用体验。\n\n尽管 Zotero 存在一定学习曲线，但其长期价值远超初期投入。若仅希望临时阅读，PDF 阅读器亦可；但从科研视角出发，建议尽早投入学习与使用。\n此外，Zotero Chrome Connector 插件可实现一键导入网页文献，极大提升文献收集效率：\n\n\n\n\n\n\nFigure 3: Zotero Chrome Connector\n\n\n\n如 Figure 3 所示，只需点击插件按钮，即可将当前网页内容导入至文献库。\n\n\n2.2 笔记记录：Obsidian\nObsidian 是一款基于 Markdown 的笔记系统，支持双向链接与图谱视图，特别适合用于构建个人知识体系。\n\n\n\n\n\n\nFigure 4: Obsidian Example\n\n\n\n推荐插件：\n\nobsidian-latex-suite：提供 LaTeX 快捷输入与公式预览功能，显著提高数学表达效率。\nHighlightr Plugin：支持自定义高亮颜色，便于分类信息标注。\n\n \n需要注意的是，过度美化界面或插件堆叠可能反而分散注意力。建议以“结构清晰、内容为本”为首要原则。\n对于不使用 Obsidian 的用户，也可选择：\n\n\n\n\n\n\n\n\n\n\n\n(a) Notion Home Page\n\n\n\n\n\n\n\n\n\n\n\n(b) FeiShu Home Page\n\n\n\n\n\n\n\nFigure 5: Home Page of Notion and FeiShu\n\n\n\n\nNotion：如 Figure 5 (a) 所示，适合多人协作与可视化编辑。\n飞书：如 Figure 5 (b) 所示，功能全面，适合企业级文档管理。\n\n\n\n2.3 代码执行：Jupyter Notebook\n在“Paper with Code”理念下，每篇论文将配套 Jupyter Notebook 实现核心算法。其交互式文档特性，使其成为学习与验证代码的理想平台。\n\n\n\n\n\n\nNote\n\n\n\n若对 Jupyter Notebook 不熟悉，推荐参考 官方文档，以快速入门。\n\n\n相应的代码，我会放在GitHub的仓库中\n\n\n\n\n\n\nFigure 6: The preview of GitHub Page\n\n\n\n\n\n2.4 GPU 平台：云端执行环境\n深度学习模型常需 GPU 加速，若本地无 GPU 可使用以下平台：\n\nGoogle Colab：Google 提供的免费云端 Notebook 平台，支持 GPU 与 TPU。\nKaggle Kernels：支持 GPU 的数据科学平台，适合快速实验。\n\n国内可选平台：\n\nAutoDL：适合国内用户，配置简单，支持定制化部署。\n\n其他推荐：\n\nRunPod、Lambda Labs：提供稳定、低延迟的 GPU 训练环境，适合中大型实验任务。\n\n\n通过合理配置上述工具，可以构建出一个系统化、高效的论文学习与研究流程。在接下来的章节中，每篇论文将附带代码实现、结构解析与批判性思考，欢迎共同学习交流。",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#总结",
    "href": "00-how-to-read-paper.html#总结",
    "title": "00: Preparation for Following",
    "section": "3 总结",
    "text": "3 总结\n在本节中，我们介绍了高效阅读论文的方法论与工具体系。通过“三遍阅读法” Listing 1， 我们可以系统地理解论文内容，并在此基础上进行批判性思考。同时，借助 Zotero Section 2.1、ObsidianSection 2.2 等工具，可以有效管理文献、记录笔记与执行代码。 在后续章节中，我们将应用这些方法与工具，深入分析每篇论文的核心思想、实验设计与创新贡献。希望通过本项目的学习，能够帮助大家更好地掌握人工智能领域的前沿研究动态，并在实践中不断提升自己的科研能力。",
    "crumbs": [
      "00 Preparation for following"
    ]
  }
]