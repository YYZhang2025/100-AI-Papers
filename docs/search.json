[
  {
    "objectID": "notebooks/01-attention.html",
    "href": "notebooks/01-attention.html",
    "title": "Word Embeddings",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport math\nimport random\n\n\ndef green(text):\n    return f\"\\033[1;32m{text}\\033[0m\"\n\n\ndef red(text):\n    return f\"\\033[1;31m{text}\\033[0m\"\n\n\ndef yellow(text):\n    return f\"\\033[1;33m{text}\\033[0m\"\n\n\ndef bold(text):\n    return f\"\\033[1m{text}\\033[0m\"\n\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"Using device: {green(device)}\")\n\n\nUsing device: mps\n\n\n\n\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass TransformerConfig:\n    d_model: int = 512\n    d_ff: int = 2048\n\n    num_heads: int = 8\n\n    dropout: float = 0.1\n\n    vocab_size: int = 10000\n    max_seq_len: int = 512\n\n\nclass WordEmbedding(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n\n    def forward(self, x):\n        return self.embedding(x)\n\n\nPosition Encoding\n\nclass PositionEncoding(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.dropout = nn.Dropout(config.dropout)\n\n        # Create the positional encoding matrix\n        pe = torch.zeros(config.max_seq_len, config.d_model)\n        position = torch.arange(0, config.max_seq_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, config.d_model, 2).float()\n            * (-math.log(10000.0) / config.d_model)\n        )\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[: x.size(0), :]\n        return self.dropout(x)\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        std = x.std(dim=-1, keepdim=True)\n        return self.weight * (x - mean) / (std + self.eps) + self.bias\n\n\nclass FFN(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x\n\n\ndef scaled_dot_production_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    mask: torch.Tensor | None = None,\n):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n\n    attn_weights = F.softmax(scores, dim=-1)\n    output = torch.matmul(attn_weights, value)\n\n    return output, attn_weights\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.num_heads = config.num_heads\n        self.d_model = config.d_model\n        self.d_k = config.d_model // config.num_heads\n\n        self.query = nn.Linear(config.d_model, config.d_model)\n        self.key = nn.Linear(config.d_model, config.d_model)\n        self.value = nn.Linear(config.d_model, config.d_model)\n\n        self.fc_out = nn.Linear(config.d_model, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n\n        query = (\n            self.query(query)\n            .view(batch_size, -1, self.num_heads, self.d_k)\n            .transpose(1, 2)\n        )\n        key = (\n            self.key(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        )\n        value = (\n            self.value(value)\n            .view(batch_size, -1, self.num_heads, self.d_k)\n            .transpose(1, 2)\n        )\n\n        attn_output, attn_weights = scaled_dot_production_attention(\n            query, key, value, mask\n        )\n\n        attn_output = (\n            attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        )\n        output = self.fc_out(attn_output)\n\n        return output, attn_weights\n\n\nclass CausalAttention(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        mask = (\n            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n\n        self.register_buffer(\"mask\", mask)\n\n    def forward(self, query, key, value):\n        batch_size = query.size(0)\n\n        query = query.view(batch_size, -1, 1, self.mask.size(-1))\n        key = key.view(batch_size, -1, 1, self.mask.size(-1))\n        value = value.view(batch_size, -1, 1, self.mask.size(-1))\n\n        attn_output, attn_weights = scaled_dot_production_attention(\n            query, key, value, self.mask\n        )\n\n        attn_output = attn_output.view(batch_size, -1, self.mask.size(-1))\n\n        return attn_output, attn_weights\n\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FFN(config)\n\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n\n        self.dropout1 = nn.Dropout(config.dropout)\n        self.dropout2 = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        attn_output, _ = self.attention(x, x, x)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n\n        ffn_output = self.ffn(x)\n        x = x + self.dropout2(ffn_output)\n        x = self.norm2(x)\n\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.attention1 = CausalAttention(config)\n        self.attention2 = MultiHeadAttention(config)\n        self.ffn = FFN(config)\n\n        self.norm1 = LayerNorm(config.d_model)\n        self.norm2 = LayerNorm(config.d_model)\n        self.norm3 = LayerNorm(config.d_model)\n\n        self.dropout1 = nn.Dropout(config.dropout)\n        self.dropout2 = nn.Dropout(config.dropout)\n        self.dropout3 = nn.Dropout(config.dropout)\n\n    def forward(self, x, encoder_output):\n        attn_output, _ = self.attention1(x, x, x)\n        x = x + self.dropout1(attn_output)\n        x = self.norm1(x)\n\n        attn_output, _ = self.attention2(x, encoder_output, encoder_output)\n        x = x + self.dropout2(attn_output)\n        x = self.norm2(x)\n\n        ffn_output = self.ffn(x)\n        x = x + self.dropout3(ffn_output)\n        x = self.norm3(x)\n\n        return x\n\n\nclass Encoder(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = WordEmbedding(config)\n        self.position_encoding = PositionEncoding(config)\n\n        self.layers = nn.ModuleList(\n            [EncoderBlock(config) for _ in range(config.num_heads)]\n        )\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        return x\n\n\nclass Decoder(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.embedding = WordEmbedding(config)\n        self.position_encoding = PositionEncoding(config)\n\n        self.layers = nn.ModuleList(\n            [DecoderBlock(config) for _ in range(config.num_heads)]\n        )\n\n    def forward(self, x, encoder_output):\n        x = self.embedding(x)\n        x = self.position_encoding(x)\n\n        for layer in self.layers:\n            x = layer(x, encoder_output)\n\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n\n        self.encoder = Encoder(config)\n        self.decoder = Decoder(config)\n\n        self.fc_out = nn.Linear(config.d_model, config.vocab_size)\n\n    def tie_weights(self):\n        \"\"\"Tie the weights of the embedding and output layers.\"\"\"\n        self.fc_out.weight = self.encoder.embedding.embedding.weight\n\n    def forward(self, src, tgt):\n        encoder_output = self.encoder(src)\n        decoder_output = self.decoder(tgt, encoder_output)\n        output = self.fc_out(decoder_output)\n\n        return output"
  },
  {
    "objectID": "00-how-to-read-paper.html",
    "href": "00-how-to-read-paper.html",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "åœ¨æ·±å…¥å­¦ä¹ äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ä¹‹å‰ï¼ŒæŒæ¡é«˜æ•ˆä¸”ç³»ç»Ÿçš„é˜…è¯»æ–¹æ³•è‡³å…³é‡è¦ã€‚è®ºæ–‡é˜…è¯»ç»å…¸æŒ‡å— How to Read a Paper ä¸­æå‡ºäº†â€œä¸‰éé˜…è¯»æ³•â€ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„å®è·µè·¯å¾„ï¼š\n\n\n\n\nç¬¬ä¸€éï¼šå¿«é€Ÿæµè§ˆï¼Œè·å–è®ºæ–‡çš„æ•´ä½“ç»“æ„å’Œæ ¸å¿ƒç»“è®ºã€‚èšç„¦äºæ ‡é¢˜ï¼ˆtitleï¼‰ã€æ‘˜è¦ï¼ˆabstractï¼‰ã€å¼•è¨€ï¼ˆintroductionï¼‰ä¸ç»“è®ºï¼ˆconclusionï¼‰ï¼Œä»å®è§‚ä¸Šäº†è§£æ–‡ç« çš„ç ”ç©¶æ–¹å‘ã€‚\nç¬¬äºŒéï¼šç»†è¯»è®ºæ–‡ï¼Œå…³æ³¨ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®ä¸å…³é”®å›¾è¡¨ã€‚é‡ç‚¹ç†è§£è®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æåŠå…¶æ”¯æ’‘é€»è¾‘ã€‚\nç¬¬ä¸‰éï¼šæ‰¹åˆ¤æ€§é˜…è¯»ï¼Œç³»ç»Ÿåˆ†æè®ºæ–‡çš„ä¼˜åŠ£ï¼Œæå‡ºå»ºè®¾æ€§é—®é¢˜ï¼Œåæ€è¯¥æ–¹æ³•æ˜¯å¦å…·æœ‰é€šç”¨æ€§æˆ–æ˜¯å¦èƒ½åº”ç”¨äºè‡ªèº«ç ”ç©¶ã€‚\n\n\n\nListingÂ 1: ä¸‰éé˜…è¯»æ³•\n\n\n\néœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œé˜…è¯»è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£ã€é€æ­¥æ·±å…¥çš„è¿‡ç¨‹ã€‚åœ¨åå¤é˜…è¯»ä¸æ€è€ƒä¸­ï¼Œæˆ‘ä»¬ä¼šä¸æ–­ä¿®æ­£ç†è§£ã€åŠ æ·±è®¤çŸ¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œé€šè¿‡ä¸ä»–äººäº¤æµã€å‚ä¸è®¨è®ºï¼Œæœ‰åŠ©äºæ‹“å®½è§†è§’ã€æ·±åŒ–æ€è€ƒã€‚ï¼ˆæœ¬é¡¹ç›®ç½‘ç«™ä¹Ÿå› æ­¤è€Œç”Ÿï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä»¬æä¾›ä¸€ä¸ªå…±åŒäº¤æµå­¦ä¹ çš„å¹³å°ï¼‰\nä¸ºæå‡é˜…è¯»æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç²¾è¯»é˜¶æ®µï¼Œä½¿ç”¨é¢œè‰²æ ‡è®°ä¸åŒå†…å®¹ã€‚ä¾‹å¦‚åœ¨ Harvard CS197 AI Research Experiences çš„ Lecture 3 ä¸­ç»™å‡ºçš„ç­–ç•¥ï¼š\n\né»„è‰²ï¼šçªå‡ºè®ºæ–‡æ‰€è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚\nç²‰è‰²ï¼šå¯¹åº”æå‡ºçš„ç®—æ³•æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ã€‚\næ©™è‰²ï¼šç”¨äºæ ‡è®°è®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸è´¡çŒ®ã€‚\n\nä¾‹å¦‚å¦‚ä¸‹ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\nFigureÂ 1: Highlight Example of paper (Attention is all you need)\n\n\n\nè¿™å¥—æ–¹æ³•å¹¶éå”¯ä¸€æ ‡å‡†ï¼Œå…³é”®åœ¨äºæ„å»ºä¸€å¥—é€‚åˆè‡ªèº«è®¤çŸ¥æ–¹å¼çš„å¯è§†åŒ–æ ‡è®°ä½“ç³»ã€‚ç»Ÿä¸€çš„æ ‡æ³¨é£æ ¼ï¼Œæœ‰åŠ©äºåœ¨åæœŸå›é¡¾æˆ–è·¨è®ºæ–‡æ¯”è¾ƒæ—¶é«˜æ•ˆå®šä½å…³é”®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®æ‰€æ”¶å½•çš„æ¯ç¯‡è®ºæ–‡ç¬”è®°ä¹Ÿå°†é‡‡ç”¨è¿™ä¸€ç»“æ„åŒ–é«˜äº®æ ‡è®°æ–¹æ³•ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#è®ºæ–‡é˜…è¯»æŒ‡å—",
    "href": "00-how-to-read-paper.html#è®ºæ–‡é˜…è¯»æŒ‡å—",
    "title": "00: Preparation for Following",
    "section": "",
    "text": "åœ¨æ·±å…¥å­¦ä¹ äººå·¥æ™ºèƒ½ç›¸å…³è®ºæ–‡ä¹‹å‰ï¼ŒæŒæ¡é«˜æ•ˆä¸”ç³»ç»Ÿçš„é˜…è¯»æ–¹æ³•è‡³å…³é‡è¦ã€‚è®ºæ–‡é˜…è¯»ç»å…¸æŒ‡å— How to Read a Paper ä¸­æå‡ºäº†â€œä¸‰éé˜…è¯»æ³•â€ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†æ¸…æ™°çš„å®è·µè·¯å¾„ï¼š\n\n\n\n\nç¬¬ä¸€éï¼šå¿«é€Ÿæµè§ˆï¼Œè·å–è®ºæ–‡çš„æ•´ä½“ç»“æ„å’Œæ ¸å¿ƒç»“è®ºã€‚èšç„¦äºæ ‡é¢˜ï¼ˆtitleï¼‰ã€æ‘˜è¦ï¼ˆabstractï¼‰ã€å¼•è¨€ï¼ˆintroductionï¼‰ä¸ç»“è®ºï¼ˆconclusionï¼‰ï¼Œä»å®è§‚ä¸Šäº†è§£æ–‡ç« çš„ç ”ç©¶æ–¹å‘ã€‚\nç¬¬äºŒéï¼šç»†è¯»è®ºæ–‡ï¼Œå…³æ³¨ç†è®ºæ¨å¯¼ã€å®éªŒè®¾ç½®ä¸å…³é”®å›¾è¡¨ã€‚é‡ç‚¹ç†è§£è®ºæ–‡æ‰€æå‡ºçš„æ–¹æ³•ã€å®éªŒè®¾è®¡ã€ç»“æœåˆ†æåŠå…¶æ”¯æ’‘é€»è¾‘ã€‚\nç¬¬ä¸‰éï¼šæ‰¹åˆ¤æ€§é˜…è¯»ï¼Œç³»ç»Ÿåˆ†æè®ºæ–‡çš„ä¼˜åŠ£ï¼Œæå‡ºå»ºè®¾æ€§é—®é¢˜ï¼Œåæ€è¯¥æ–¹æ³•æ˜¯å¦å…·æœ‰é€šç”¨æ€§æˆ–æ˜¯å¦èƒ½åº”ç”¨äºè‡ªèº«ç ”ç©¶ã€‚\n\n\n\nListingÂ 1: ä¸‰éé˜…è¯»æ³•\n\n\n\néœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œé˜…è¯»è®ºæ–‡æ˜¯ä¸€ä¸ªå¾ªç¯è¿­ä»£ã€é€æ­¥æ·±å…¥çš„è¿‡ç¨‹ã€‚åœ¨åå¤é˜…è¯»ä¸æ€è€ƒä¸­ï¼Œæˆ‘ä»¬ä¼šä¸æ–­ä¿®æ­£ç†è§£ã€åŠ æ·±è®¤çŸ¥ã€‚ä¸æ­¤åŒæ—¶ï¼Œé€šè¿‡ä¸ä»–äººäº¤æµã€å‚ä¸è®¨è®ºï¼Œæœ‰åŠ©äºæ‹“å®½è§†è§’ã€æ·±åŒ–æ€è€ƒã€‚ï¼ˆæœ¬é¡¹ç›®ç½‘ç«™ä¹Ÿå› æ­¤è€Œç”Ÿï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä»¬æä¾›ä¸€ä¸ªå…±åŒäº¤æµå­¦ä¹ çš„å¹³å°ï¼‰\nä¸ºæå‡é˜…è¯»æ•ˆç‡ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ç²¾è¯»é˜¶æ®µï¼Œä½¿ç”¨é¢œè‰²æ ‡è®°ä¸åŒå†…å®¹ã€‚ä¾‹å¦‚åœ¨ Harvard CS197 AI Research Experiences çš„ Lecture 3 ä¸­ç»™å‡ºçš„ç­–ç•¥ï¼š\n\né»„è‰²ï¼šçªå‡ºè®ºæ–‡æ‰€è¯•å›¾è§£å†³çš„æ ¸å¿ƒé—®é¢˜æˆ–æŒ‘æˆ˜ã€‚\nç²‰è‰²ï¼šå¯¹åº”æå‡ºçš„ç®—æ³•æ–¹æ³•æˆ–æŠ€æœ¯è·¯å¾„ã€‚\næ©™è‰²ï¼šç”¨äºæ ‡è®°è®ºæ–‡çš„åˆ›æ–°ç‚¹ä¸è´¡çŒ®ã€‚\n\nä¾‹å¦‚å¦‚ä¸‹ç¤ºæ„å›¾ï¼š\n\n\n\n\n\n\nFigureÂ 1: Highlight Example of paper (Attention is all you need)\n\n\n\nè¿™å¥—æ–¹æ³•å¹¶éå”¯ä¸€æ ‡å‡†ï¼Œå…³é”®åœ¨äºæ„å»ºä¸€å¥—é€‚åˆè‡ªèº«è®¤çŸ¥æ–¹å¼çš„å¯è§†åŒ–æ ‡è®°ä½“ç³»ã€‚ç»Ÿä¸€çš„æ ‡æ³¨é£æ ¼ï¼Œæœ‰åŠ©äºåœ¨åæœŸå›é¡¾æˆ–è·¨è®ºæ–‡æ¯”è¾ƒæ—¶é«˜æ•ˆå®šä½å…³é”®ä¿¡æ¯ã€‚æœ¬é¡¹ç›®æ‰€æ”¶å½•çš„æ¯ç¯‡è®ºæ–‡ç¬”è®°ä¹Ÿå°†é‡‡ç”¨è¿™ä¸€ç»“æ„åŒ–é«˜äº®æ ‡è®°æ–¹æ³•ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#å·¥å…·å‡†å¤‡",
    "href": "00-how-to-read-paper.html#å·¥å…·å‡†å¤‡",
    "title": "00: Preparation for Following",
    "section": "å·¥å…·å‡†å¤‡",
    "text": "å·¥å…·å‡†å¤‡\nç§‘å­¦é˜…è¯»ç¦»ä¸å¼€åˆé€‚çš„å·¥å…·æ”¯æ’‘ã€‚ä»¥ä¸‹æ˜¯æ¨èçš„å·¥å…·ä½“ç³»ï¼Œæ¶µç›–æ–‡çŒ®ç®¡ç†ã€ç¬”è®°æ•´ç†ã€ä»£ç æ‰§è¡Œç­‰å¤šä¸ªç»´åº¦ã€‚\n\næ–‡çŒ®ç®¡ç†ï¼šZotero\néšç€è®ºæ–‡ç§¯ç´¯çš„å¢å¤šï¼Œç³»ç»Ÿçš„æ–‡çŒ®ç®¡ç†å·¥å…·ä¸å¯æˆ–ç¼ºã€‚Zotero æ˜¯ä¸€æ¬¾å…è´¹ä¸”å¼€æºçš„æ–‡çŒ®ç®¡ç†å¹³å°ï¼Œæ”¯æŒè‡ªåŠ¨å¯¼å…¥ã€åˆ†ç»„ç®¡ç†ä¸å¤šæ ¼å¼å¼•ç”¨ï¼ˆå¦‚ BibTeXï¼‰ã€‚å…¶å¯æ‰©å±•æ€§æå¼ºï¼Œæ”¯æŒæ’ä»¶ä¸ä¸»é¢˜å®šåˆ¶ã€‚\n\n\n\n\n\n\nFigureÂ 2: Example of Zotero\n\n\n\næ¨èæ’ä»¶ï¼š\n\nBetter BibTexï¼šå¢å¼º BibTeX å¯¼å‡ºåŠŸèƒ½ï¼Œä¾¿äºä¸ LaTeX æ— ç¼é›†æˆã€‚\nEthereal Styleï¼šä¸º Zotero æä¾›ç¾è§‚çš„ UI é£æ ¼ï¼Œæå‡ä½¿ç”¨ä½“éªŒã€‚\n\nå°½ç®¡ Zotero å­˜åœ¨ä¸€å®šå­¦ä¹ æ›²çº¿ï¼Œä½†å…¶é•¿æœŸä»·å€¼è¿œè¶…åˆæœŸæŠ•å…¥ã€‚è‹¥ä»…å¸Œæœ›ä¸´æ—¶é˜…è¯»ï¼ŒPDF é˜…è¯»å™¨äº¦å¯ï¼›ä½†ä»ç§‘ç ”è§†è§’å‡ºå‘ï¼Œå»ºè®®å°½æ—©æŠ•å…¥å­¦ä¹ ä¸ä½¿ç”¨ã€‚\næ­¤å¤–ï¼ŒZotero Chrome Connector æ’ä»¶å¯å®ç°ä¸€é”®å¯¼å…¥ç½‘é¡µæ–‡çŒ®ï¼Œæå¤§æå‡æ–‡çŒ®æ”¶é›†æ•ˆç‡ï¼š\n\n\n\n\n\n\nFigureÂ 3: Zotero Chrome Connector\n\n\n\nå¦‚ FigureÂ 3 æ‰€ç¤ºï¼Œåªéœ€ç‚¹å‡»æ’ä»¶æŒ‰é’®ï¼Œå³å¯å°†å½“å‰ç½‘é¡µå†…å®¹å¯¼å…¥è‡³æ–‡çŒ®åº“ã€‚\n\n\nç¬”è®°è®°å½•ï¼šObsidian\nObsidian æ˜¯ä¸€æ¬¾åŸºäº Markdown çš„ç¬”è®°ç³»ç»Ÿï¼Œæ”¯æŒåŒå‘é“¾æ¥ä¸å›¾è°±è§†å›¾ï¼Œç‰¹åˆ«é€‚åˆç”¨äºæ„å»ºä¸ªäººçŸ¥è¯†ä½“ç³»ã€‚\n\n\n\n\n\n\nFigureÂ 4: Obsidian Example\n\n\n\næ¨èæ’ä»¶ï¼š\n\nobsidian-latex-suiteï¼šæä¾› LaTeX å¿«æ·è¾“å…¥ä¸å…¬å¼é¢„è§ˆåŠŸèƒ½ï¼Œæ˜¾è‘—æé«˜æ•°å­¦è¡¨è¾¾æ•ˆç‡ã€‚\nHighlightr Pluginï¼šæ”¯æŒè‡ªå®šä¹‰é«˜äº®é¢œè‰²ï¼Œä¾¿äºåˆ†ç±»ä¿¡æ¯æ ‡æ³¨ã€‚\n\n \néœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿‡åº¦ç¾åŒ–ç•Œé¢æˆ–æ’ä»¶å †å å¯èƒ½åè€Œåˆ†æ•£æ³¨æ„åŠ›ã€‚å»ºè®®ä»¥â€œç»“æ„æ¸…æ™°ã€å†…å®¹ä¸ºæœ¬â€ä¸ºé¦–è¦åŸåˆ™ã€‚\nå¯¹äºä¸ä½¿ç”¨ Obsidian çš„ç”¨æˆ·ï¼Œä¹Ÿå¯é€‰æ‹©ï¼š\n\n\n\n\n\n\n\n\n\n\n\n(a) Notion Home Page\n\n\n\n\n\n\n\n\n\n\n\n(b) FeiShu Home Page\n\n\n\n\n\n\n\nFigureÂ 5: Home Page of Notion and FeiShu\n\n\n\n\nNotionï¼šå¦‚ FigureÂ 5 (a) æ‰€ç¤ºï¼Œé€‚åˆå¤šäººåä½œä¸å¯è§†åŒ–ç¼–è¾‘ã€‚\né£ä¹¦ï¼šå¦‚ FigureÂ 5 (b) æ‰€ç¤ºï¼ŒåŠŸèƒ½å…¨é¢ï¼Œé€‚åˆä¼ä¸šçº§æ–‡æ¡£ç®¡ç†ã€‚\n\n\n\nä»£ç æ‰§è¡Œï¼šJupyter Notebook\nåœ¨â€œPaper with Codeâ€ç†å¿µä¸‹ï¼Œæ¯ç¯‡è®ºæ–‡å°†é…å¥— Jupyter Notebook å®ç°æ ¸å¿ƒç®—æ³•ã€‚å…¶äº¤äº’å¼æ–‡æ¡£ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºå­¦ä¹ ä¸éªŒè¯ä»£ç çš„ç†æƒ³å¹³å°ã€‚\n\n\n\n\n\n\nNote\n\n\n\nè‹¥å¯¹ Jupyter Notebook ä¸ç†Ÿæ‚‰ï¼Œæ¨èå‚è€ƒ å®˜æ–¹æ–‡æ¡£ï¼Œä»¥å¿«é€Ÿå…¥é—¨ã€‚\n\n\nç›¸åº”çš„ä»£ç ï¼Œæˆ‘ä¼šæ”¾åœ¨GitHubçš„ä»“åº“ä¸­\n\n\n\n\n\n\nFigureÂ 6: The preview of GitHub Page\n\n\n\n\n\nGPU å¹³å°ï¼šäº‘ç«¯æ‰§è¡Œç¯å¢ƒ\næ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸éœ€ GPU åŠ é€Ÿï¼Œè‹¥æœ¬åœ°æ—  GPU å¯ä½¿ç”¨ä»¥ä¸‹å¹³å°ï¼š\n\nGoogle Colabï¼šGoogle æä¾›çš„å…è´¹äº‘ç«¯ Notebook å¹³å°ï¼Œæ”¯æŒ GPU ä¸ TPUã€‚\nKaggle Kernelsï¼šæ”¯æŒ GPU çš„æ•°æ®ç§‘å­¦å¹³å°ï¼Œé€‚åˆå¿«é€Ÿå®éªŒã€‚\n\nå›½å†…å¯é€‰å¹³å°ï¼š\n\nAutoDLï¼šé€‚åˆå›½å†…ç”¨æˆ·ï¼Œé…ç½®ç®€å•ï¼Œæ”¯æŒå®šåˆ¶åŒ–éƒ¨ç½²ã€‚\n\nå…¶ä»–æ¨èï¼š\n\nRunPodã€Lambda Labsï¼šæä¾›ç¨³å®šã€ä½å»¶è¿Ÿçš„ GPU è®­ç»ƒç¯å¢ƒï¼Œé€‚åˆä¸­å¤§å‹å®éªŒä»»åŠ¡ã€‚\n\n\né€šè¿‡åˆç†é…ç½®ä¸Šè¿°å·¥å…·ï¼Œå¯ä»¥æ„å»ºå‡ºä¸€ä¸ªç³»ç»ŸåŒ–ã€é«˜æ•ˆçš„è®ºæ–‡å­¦ä¹ ä¸ç ”ç©¶æµç¨‹ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæ¯ç¯‡è®ºæ–‡å°†é™„å¸¦ä»£ç å®ç°ã€ç»“æ„è§£æä¸æ‰¹åˆ¤æ€§æ€è€ƒï¼Œæ¬¢è¿å…±åŒå­¦ä¹ äº¤æµã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#æ€»ç»“",
    "href": "00-how-to-read-paper.html#æ€»ç»“",
    "title": "00: Preparation for Following",
    "section": "æ€»ç»“",
    "text": "æ€»ç»“\nåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†é«˜æ•ˆé˜…è¯»è®ºæ–‡çš„æ–¹æ³•è®ºä¸å·¥å…·ä½“ç³»ã€‚é€šè¿‡â€œä¸‰éé˜…è¯»æ³•â€ ListingÂ 1ï¼Œ æˆ‘ä»¬å¯ä»¥ç³»ç»Ÿåœ°ç†è§£è®ºæ–‡å†…å®¹ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ‰¹åˆ¤æ€§æ€è€ƒã€‚åŒæ—¶ï¼Œå€ŸåŠ© Zotero SectionÂ 2.1ã€ObsidianSectionÂ 2.2 ç­‰å·¥å…·ï¼Œå¯ä»¥æœ‰æ•ˆç®¡ç†æ–‡çŒ®ã€è®°å½•ç¬”è®°ä¸æ‰§è¡Œä»£ç ã€‚ åœ¨åç»­ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨è¿™äº›æ–¹æ³•ä¸å·¥å…·ï¼Œæ·±å…¥åˆ†ææ¯ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ã€å®éªŒè®¾è®¡ä¸åˆ›æ–°è´¡çŒ®ã€‚å¸Œæœ›é€šè¿‡æœ¬é¡¹ç›®çš„å­¦ä¹ ï¼Œèƒ½å¤Ÿå¸®åŠ©å¤§å®¶æ›´å¥½åœ°æŒæ¡äººå·¥æ™ºèƒ½é¢†åŸŸçš„å‰æ²¿ç ”ç©¶åŠ¨æ€ï¼Œå¹¶åœ¨å®è·µä¸­ä¸æ–­æå‡è‡ªå·±çš„ç§‘ç ”èƒ½åŠ›ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  },
  {
    "objectID": "posts/01-attention.html",
    "href": "posts/01-attention.html",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "ç»™å®šä¸¤ä¸ªå‘é‡ \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), å®ƒä»¬çš„ç‚¹ç§¯ä¸º: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\næµ‹é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦, æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼\nåœ¨ Self-Attention ä¸­ï¼Œç”¨æ¥è¡¡é‡â€œå½“å‰è¯å¯¹å…¶ä»–è¯çš„å…³æ³¨ç¨‹åº¦â€\n\n\n\n\nç»™å®šå‘é‡ \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)ï¼ŒSoftmax å‡½æ•°å®šä¹‰ä¸º: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\nå°†æ³¨æ„åŠ›â€œå¾—åˆ†ï¼ˆdot productï¼‰â€è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ\næ¯ä¸ªè¯å¯¹å…¶ä»–è¯çš„æ³¨æ„åŠ›æƒé‡ \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#preliminary",
    "href": "posts/01-attention.html#preliminary",
    "title": "01: Attention is All You Need",
    "section": "",
    "text": "ç»™å®šä¸¤ä¸ªå‘é‡ \\(\\mathbf{q}, \\mathbf{k} \\in \\mathbb{R}^d\\), å®ƒä»¬çš„ç‚¹ç§¯ä¸º: \\[\n\\text{score} = \\mathbf{q} \\cdot \\mathbf{k} = \\sum_{i=1}^{d} q_i k_i\n\\tag{1}\\]\n\næµ‹é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦, æ•°å€¼è¶Šå¤§ï¼Œè¯´æ˜ä¸¤ä¸ªå‘é‡è¶Šç›¸ä¼¼\nåœ¨ Self-Attention ä¸­ï¼Œç”¨æ¥è¡¡é‡â€œå½“å‰è¯å¯¹å…¶ä»–è¯çš„å…³æ³¨ç¨‹åº¦â€\n\n\n\n\nç»™å®šå‘é‡ \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\)ï¼ŒSoftmax å‡½æ•°å®šä¹‰ä¸º: \\[\n\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n\\tag{2}\\]\n\nå°†æ³¨æ„åŠ›â€œå¾—åˆ†ï¼ˆdot productï¼‰â€è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ\næ¯ä¸ªè¯å¯¹å…¶ä»–è¯çš„æ³¨æ„åŠ›æƒé‡ \\(\\alpha_{ij} \\in [0, 1]\\)",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#attention-is-all-you-need",
    "href": "posts/01-attention.html#attention-is-all-you-need",
    "title": "01: Attention is All You Need",
    "section": "Attention is All You Need",
    "text": "Attention is All You Need\n\n\n\n\n\n\nFigureÂ 1: The Transformer Family Version 2.0 | Lilâ€™Log\n\n\n\nå¦‚å›¾ FigureÂ 1 æ‰€ç¤ºï¼Œæ˜¯Transformerçš„æ•´ä½“æ¶æ„ã€‚ä»\n\nä½ç½®ç¼–ç \nTransformerçš„è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œåºåˆ—ä¸­çš„æ¯ä¸ªè¯éƒ½è¢«è½¬æ¢ä¸ºä¸€ä¸ªå‘é‡ã€‚ä¸ºäº†è®©æ¨¡å‹èƒ½å¤Ÿç†è§£è¯è¯­åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ ä½ç½®ç¼–ç ï¼ˆPosition Encodingï¼‰ã€‚ä½ç½®ç¼–ç å¯ä»¥æ˜¯å›ºå®šçš„ï¼Œä¹Ÿå¯ä»¥æ˜¯å¯å­¦ä¹ çš„ã€‚ åœ¨Transformerä¸­ï¼Œä½ç½®ç¼–ç æ˜¯é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°æ¥å®ç°çš„ï¼šå…·ä½“æ¥è¯´ï¼Œå¯¹äºåºåˆ—ä¸­çš„ç¬¬ \\(i\\) ä¸ªä½ç½®ï¼Œä½ç½®ç¼–ç çš„ç¬¬ \\(j\\) ç»´å¯ä»¥è¡¨ç¤ºä¸ºï¼š \\[\n\\text{PE}(i, j) =\n\\begin{cases}\n\\sin\\left(\\frac{i}{10000^{j/d}}\\right) & \\text{if } j \\text{ is even} \\\\\n\\cos\\left(\\frac{i}{10000^{(j-1)/d}}\\right) & \\text{if } j \\     \\text{ is odd}\n\\end{cases}\n\\tag{3}\\] å…¶ä¸­ \\(d\\) æ˜¯ä½ç½®ç¼–ç çš„ç»´åº¦ã€‚ä½ç½®ç¼–ç çš„ä½œç”¨æ˜¯ä¸ºæ¯ä¸ªè¯æä¾›ä¸€ä¸ªå”¯ä¸€çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ•æ‰åˆ°è¯è¯­åœ¨åºåˆ—ä¸­çš„ç›¸å¯¹ä½ç½®å…³ç³»ã€‚",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#pytorch-å®ç°",
    "href": "posts/01-attention.html#pytorch-å®ç°",
    "title": "01: Attention is All You Need",
    "section": "PyTorch å®ç°",
    "text": "PyTorch å®ç°\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å«åˆ©ç”¨PyTorchæ¥å®ç°Transformerçš„æ¨¡å‹æ¶æ„ã€‚æˆ‘ä»¬é‡‡ç”¨Bottom-Upçš„æ–¹æ³•ï¼Œå…ˆå®ç°Word Embeddingï¼Œ æ¥ç€æ˜¯Position Embeddingï¼Œæ¥ç€å®ç°æˆ‘ä»¬çš„é‡ç‚¹ï¼Œå³Multi-Head-Attentionï¼Œå†æ¬¡ä¹‹åï¼Œæˆ‘ä»¬ä¼šå®ç° Point-Wise Feed Forward Networkã€‚æœ€åå°†è¿™å‡ ä¸ªæ¨¡å—ç»„åˆèµ·æ¥ï¼Œå®ç°Transformerçš„Encoder å’Œ Decoderã€‚å‡†å¤‡å¥½äº†å—ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹å§ï¼\n\nWord Embedding\nWord Embeddingæ˜¯å°†è¯è¯­è½¬æ¢ä¸ºå‘é‡çš„è¿‡ç¨‹ã€‚åœ¨PyTorch ä¸­çš„å®ç°éå¸¸ç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨nn.Embeddingç±»æ¥å®ç°ã€‚è¿™ä¸ªç±»ä¼šå°†æ¯ä¸ªtokenæ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šç»´åº¦çš„å‘é‡ç©ºé—´ä¸­ã€‚\nclass WordEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n    \n    def _init_weight(self):\n        nn.init.trunc_normal_(self.embedding.weight, mean = 0, std=0.02, a=-3, b=3)\n    \n    def forward(self, x):\n        return self.embedding(x)    \n\n\nPosition Embedding\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥å®ç°Position Embedding EquationÂ 3ã€‚\nclass PositionEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.seq_len = config.seq_len\n        self.d_model = config.d_model\næœ‰äº†Word Embedding å’Œ Position Embeddingï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†è¾“å…¥çš„tokenè½¬æ¢ä¸ºå‘é‡äº†ã€‚æˆ‘ä»¬éœ€è¦æ¥ä¸‹æ¥éœ€è¦åšçš„å°±æ˜¯ï¼Œå°†è¿™ä¸¤ä¸ªå‘é‡ç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å…¥å‘é‡ã€‚\nclass InputEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.word_embedding = WordEmbedding(config)\n        self.position_embedding = PositionEmbedding(config)\n    \n    def forward(self, x):\n        word_emb = self.word_embedding(x)\n        pos_emb = self.position_embedding(x)\n        \n        return word_emb + pos_emb\n\n\nFeed Forward Network\næˆ‘ä»¬å…ˆè·³è¿‡Multi Head Attentionï¼Œå…ˆå®ç°Feed Forward Networkã€‚Feed Forward Networkæ˜¯Transformerä¸­çš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒçš„ä½œç”¨æ˜¯å¯¹æ¯ä¸ªä½ç½®çš„å‘é‡è¿›è¡Œéçº¿æ€§å˜æ¢ã€‚å…·ä½“æ¥è¯´ï¼ŒFeed Forward Networkç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªReLUæ¿€æ´»å‡½æ•°ç»„æˆã€‚\nclass FeedForwardNetwork(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n        self.relu = nn.ReLU()\n        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n    def forward(self, x):\n        return self.linear2(self.relu(self.linear1(x)))\n\n\nLayer Normalization\nè¿˜æœ‰ä¸€ä¸ªé‡è¦çš„æ¨¡å—æ˜¯Layer Normalizationï¼Œå®ƒå¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›ã€‚\nclass LayerNormalization(nn.Module):\n    def __init__(self, in_features, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(in_features))\n        self.bias = nn.Parameter(torch.zeros(in_features))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.weight * normalized_x + self.bias\n\n\nMulti Head Attention\nMulti Head Attentionæ˜¯Transformerçš„æ ¸å¿ƒæ¨¡å—ã€‚å®ƒçš„ä½œç”¨æ˜¯å°†è¾“å…¥çš„å‘é‡è¿›è¡Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚\n\n\n\n\n\n\nImportant\n\n\n\nè¿™éƒ¨åˆ†æ˜¯Transformerçš„æ ¸å¿ƒæ¨¡å—ï¼Œç†è§£å®ƒæ˜¯ç†è§£Transformerä»¥åŠä»–å˜å‹çš„å…³é”®ã€‚è®°å¾—å¤šçœ‹å‡ éï¼Œç›´åˆ°ä½ èƒ½ç†è§£ä¸ºæ­¢ã€‚\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        assert config.d_model % config.num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.head_dim = config.d_model // config.num_heads\n\n        self.qkv_proj = nn.Linear(config.d_model, config.d_model * 3)\n        self.out_proj = nn.Linear(config.d_model, config.d_model)\n\n\nEncoder Block\nEncoder Blockæ˜¯Transformerçš„ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒç”±Multi Head Attentionå’ŒFeed Forward Networkç»„æˆã€‚å®ƒçš„ä½œç”¨æ˜¯å¯¹è¾“å…¥çš„å‘é‡è¿›è¡Œç¼–ç ï¼Œä»è€Œæ•æ‰åˆ°ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯ã€‚\nclass EncoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.attention = MultiHeadAttention(config)\n        self.ffn = FeedForwardNetwork(config)\n        self.norm1 = LayerNormalization(config.d_model)\n        self.norm2 = LayerNormalization(config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        # Multi Head Attention\n        attn_output = self.attention(x)\n        x = self.norm1(x + self.dropout(attn_output))   \n        # Feed Forward Network\n        ffn_output = self.ffn(x)\n        x = self.norm2(x + self.dropout(ffn_output))\n        return x\n\n\nDecoder Block\nDecoder Blockæ˜¯Transformerçš„å¦ä¸€ä¸ªé‡è¦æ¨¡å—ï¼Œå®ƒä¸Encoder Blockç±»ä¼¼ï¼Œä½†å®ƒè¿˜éœ€è¦å¤„ç† Masked Multi Head Attentionã€‚Masked Multi Head Attentionçš„ä½œç”¨æ˜¯é˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒæ—¶çœ‹åˆ°æœªæ¥çš„ä¿¡æ¯ï¼Œä»è€Œä¿è¯æ¨¡å‹çš„è‡ªå›å½’ç‰¹æ€§ã€‚\nclass DecoderBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()      \n\n        self.attention1 = MultiHeadAttention(config)\n        self.attention2 = MultiHeadAttention(config, is_causal=True)\næ­å–œä½ ï¼Œä»¥åŠæˆåŠŸçš„å®ç°äº†Transformerï¼Œè¿™ä¸ªæ˜¯å½“å‰æœ€é‡è¦çš„AIæ¨¡å‹æ¡†æ¶ã€‚ç†è§£äº†å®ƒï¼Œä½ å°±ç†è§£å¯ä»¥ç†è§£å¤§éƒ¨åˆ†çš„AIæ¨¡å‹äº†ã€‚ç°åœ¨å¤§ç«çš„ChatGPTï¼ŒDeepSeekç­‰æ¨¡å‹éƒ½æ˜¯åŸºäºTransformerçš„å˜å‹ï¼ˆåœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä¼šé˜…è¯»åˆ°è¿™äº›æ¨¡å‹ï¼‰ã€‚å®Œæ•´çš„ä»£ç å¯ä»¥åœ¨GitHubä¸ŠæŸ¥çœ‹ã€‚",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/01-attention.html#æ‰©å±•",
    "href": "posts/01-attention.html#æ‰©å±•",
    "title": "01: Attention is All You Need",
    "section": "æ‰©å±•",
    "text": "æ‰©å±•\nè‡ªä»Transformerè¢«æå‡ºä»¥æ¥ï¼Œå·²ç»æœ‰äº†å¾ˆå¤šçš„å˜å‹å’Œæ”¹è¿›ã€‚å…·ä½“çš„æ¥è¯´ï¼ŒAttentionåœ¨Transformerä¸­éœ€è¦ \\(\\mathcal{O}(n^2)\\) çš„è®¡ç®—å¤æ‚åº¦ï¼Œè¿™åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ä¼šå˜å¾—éå¸¸æ…¢ã€‚å› æ­¤ï¼Œå¾ˆå¤šç ”ç©¶è€…æå‡ºäº†å„ç§å„æ ·çš„æ”¹è¿›æ–¹æ³•æ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„æ”¹è¿›æ–¹æ³•ï¼š\n\nSparse Attention: é€šè¿‡ç¨€ç–åŒ–æ³¨æ„åŠ›çŸ©é˜µæ¥é™ä½è®¡ç®—å¤æ‚åº¦ã€‚æ¯”å¦‚ï¼ŒReformeræ¨¡å‹ä½¿ç”¨äº†å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰æ¥å®ç°ç¨€ç–æ³¨æ„åŠ›ã€‚\nLinear Attention: é€šè¿‡å°†æ³¨æ„åŠ›è®¡ç®—è½¬æ¢ä¸ºçº¿æ€§æ—¶é—´å¤æ‚åº¦",
    "crumbs": [
      "100 Papers",
      "01: Attention is All You Need"
    ]
  },
  {
    "objectID": "posts/02-bert.html",
    "href": "posts/02-bert.html",
    "title": "02: BERT",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see EquationÂ 1\n#| label: fig-polar\n#| fig-cap: \"A line plot on a polar axis\"\n\nprint(\"Hello,m World!\")\n\\[\n\\mathcal{H} = \\mathcal{W} \\cdot \\mathcal{O}\n\\tag{1}\\]\nThis is the figure see, figure ?@fig-polar $$",
    "crumbs": [
      "100 Papers",
      "02: BERT"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "100 Papers with Code",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\nNumber\nPaper Name\nDescription\nPaper Link\nCode\nCategory\n\n\n\n\n01\nAttention is All You Need\nTransformer-based model for sequence modeling\nğŸ”—\n\nNLP / Transformer\n\n\n02\nBERT\nBidirectional encoder for pre-training NLP tasks\nğŸ”—\n\nNLP / Pretraining\n\n\n03\nGPT-3\nGenerative transformer for autoregressive tasks\nğŸ”—\n\nNLP / Generation\n\n\n04\nCLIP\nContrastive Language-Image Pretraining\nğŸ”—\n\nVision / Multimodal\n\n\n05\nViT\nVision Transformer for image classification\nğŸ”—\n\nVision / Transformer",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "00-how-to-read-paper.html#å‰ç»",
    "href": "00-how-to-read-paper.html#å‰ç»",
    "title": "00: Preparation for Following",
    "section": "å‰ç»",
    "text": "å‰ç»\næ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿›å…¥ç¬¬ä¸€ç¯‡è®ºæ–‡çš„å­¦ä¹ ï¼Œå¼€å§‹æˆ‘ä»¬çš„â€œ100 Papers with Codeâ€ä¹‹æ—…ã€‚æ¯ç¯‡è®ºæ–‡éƒ½å°†é…å¤‡è¯¦ç»†çš„ç¬”è®°ã€ä»£ç å®ç°ä¸æ‰¹åˆ¤æ€§åˆ†æï¼ŒæœŸå¾…ä¸å¤§å®¶å…±åŒæ¢ç´¢äººå·¥æ™ºèƒ½çš„å¥¥ç§˜ã€‚",
    "crumbs": [
      "00 Preparation for following"
    ]
  }
]